\documentclass[11pt,a4paper]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{natbib}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,positioning,shapes,decorations.pathreplacing}
\bibliographystyle{plainnat}

\title{Understanding Variational Approximations \\
{\small Summer Math Project}}
\author{David Ewing}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This paper presents a progression through mean-field variational inference (VI) applied to increasingly complex Bayesian models. We begin with linear regression, where exact posterior solutions exist and provide a validation benchmark, then advance to hierarchical models with random effects that demonstrate both VI's computational advantages and its systematic under-dispersion of precision components under mean-field approximations. Each stage builds understanding of VI's role in making Bayesian inference tractable, whilst revealing the speed-accuracy trade-offs imposed by factorisation assumptions. We quantify computational efficiency (128--1,709$\times$ speedup over Gibbs sampling) and under-dispersion through standard deviation ratios comparing variational posteriors against a customised Gibbs sampling implementation, develop aggregated reliability metrics, and provide practical guidance for practitioners balancing speed against uncertainty quantification.
\end{abstract}

\section{Introduction}

Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorised set of distributions by maximising a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorised distribution. Often not all integrals are available in closed form, which is typically handled by using a lower bound. The mathematical steps for deriving the variational updates are given in the Methods section.

Why this matters depends on one's inferential goals. Bayesian inference seeks to characterise the full posterior distribution $p(\theta \mid \text{data})$, representing uncertainty about parameters through probability distributions. This contrasts with frequentist inference, which estimates parameters as fixed but unknown constants and quantifies uncertainty through repeated-sampling properties such as standard errors and confidence intervals. The choice between paradigms determines what we seek: Bayesians want posterior distributions and credible intervals; frequentists want point estimates with sampling distributions. VI addresses the Bayesian goal when exact posterior computation is intractable.

This paper focuses specifically on VI applied to two models of increasing complexity. In general, factorisations can be chosen in many ways, and the choice reflects a trade-off between computational simplicity and fidelity to posterior dependence. Model 1 is Bayesian linear regression, written in matrix form as
\[
  y = X\beta + \varepsilon
\]
with $\varepsilon \sim N(0, \tau_e^{-1} I)$. The Bayesian goal is the posterior $p(\beta, \tau_e \mid y, X)$; the frequentist analogue would be estimates $\hat{\beta}$ with standard errors. Under VI, we factorise $q(\beta, \tau_e) = q(\beta)\,q(\tau_e)$, treating parameters as independent in the variational approximation. This factorisation is reasonable for Model 1 because posterior dependence between $\beta$ and $\tau_e$ is weak, so separating them yields a tractable approximation without substantial distortion.

Model 2 extends to hierarchical structure with random intercepts and Gaussian likelihood:
\[
  y = X\beta + Zu + \varepsilon,
\]
where $u \sim N(0, \tau_u^{-1} I)$ represents group-specific deviations, $Z$ is the group membership design matrix, and $\varepsilon \sim N(0, \tau_e^{-1} I)$ is observation-level noise. Observations are nested within groups (for example, students within schools), and the random intercepts capture within-group correlation whilst allowing information sharing across groups. The Bayesian target is the joint posterior $p(\beta, u, \tau_u, \tau_e \mid y, X, \text{groups})$; frequentist mixed models would estimate fixed effects $\hat{\beta}$ and precision components $\hat{\tau}_u$, $\hat{\tau}_e$ (equivalently, variances $\hat{\tau}_u^{-1}$, $\hat{\tau}_e^{-1}$). VI factorises this as $q(\beta, u, \tau_u, \tau_e) = q(\beta, u)\,q(\tau_u)\,q(\tau_e)$, keeping $\beta$ and $u$ together because their posterior dependence is strong in hierarchical models.

These two models serve a practical purpose. If an appropriate factorisation is chosen, Model 1 establishes that VI can recover known posteriors when exact solutions exist, building confidence in the method. Model 2 reveals a systematic limitation: precision parameters like $\tau_u$ exhibit under-dispersion under mean-field approximations, with posterior distributions too narrow compared to Gibbs sampling gold standards. This paper demonstrates this phenomenon empirically using synthetic data with known ground truth, explaining when VI is adequate and when its factorisation assumption becomes problematic.

\textbf{Computational Motivation:} Beyond accuracy, a primary advantage of variational inference is computational speed. Whilst Gibbs sampling requires thousands of correlated iterations through all latent variables, VI transforms inference into an optimisation problem that converges in seconds. This speed advantage is particularly notable for hierarchical models: VB computational time remains approximately constant despite changes in the number of groups $Q$, suggesting its cost is driven primarily by sample size $n$ rather than hierarchical structure. In contrast, Gibbs sampling shows strong dependence on $Q$, exhibiting quadratic scaling. This report quantifies both the speed gains and the accuracy costs (Table~\ref{tab:timing}), allowing practitioners to make informed trade-offs between computational efficiency and posterior uncertainty quantification.

\paragraph{Scope Note:} A hierarchical logistic model (Model 3 with binary response) has been implemented but is not included in this report. Implementation errors were discovered during development, rendering the Model 3 results unreliable. Correct implementation of variational inference for hierarchical logistic regression requires data augmentation techniques such as the Pólya-Gamma method \cite{polson2013bayesian}, which is beyond the scope of this introductory treatment. This paper focuses on Models 1 and 2, where the factorised VI family permits closed-form updates, enabling us to demonstrate under-dispersion even for Model 2, which lacks a closed-form posterior.

\section{Methods}

\subsection{Variational Inference as Optimisation}

Mean-field variational inference is an optimisation method for approximating Bayesian posterior inference. The idea is to replace the exact posterior $p(z \mid x)$ with a simpler, tractable distribution $q_\nu(z)$ drawn from a chosen family. We restrict attention to a variational family $q(z;\nu)$ and then optimise $\nu$ so that $q(z;\nu)$ is close (in KL divergence) to the true posterior $p(z\mid x)$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.75\textwidth]{../presentation/q-space-visual-fixed-twice..png}
\caption{The variational inference problem visualised as optimisation within a restricted family $\mathcal{Q}$. The ellipse (red boundary) represents all tractable approximate distributions available within the chosen family. Optimisation moves from initial guess $q_{\text{init}}$ to the best available approximation $q_{\text{opt}}$. The true posterior $p(z|x)$ (green dot) sits outside this family because it is generally too complex to belong to $\mathcal{Q}$. The KL divergence $\mathrm{KL}(q_{\text{opt}} \| p(z|x))$ measures the remaining gap between our approximation and the truth.}
\label{fig:vi_ellipse}
\end{figure}

Figure~\ref{fig:vi_ellipse} illustrates this geometrically. The ellipse represents all families of tractable approximations, indexed by the variational parameters $\nu$. Starting from an initial value $\nu^{\mathrm{init}}$, an optimisation routine moves through parameter space to reach $\nu^\star$, the best approximation available within the family. The true posterior $p(z\mid x)$ sits outside this ellipse because it is generally too complex to belong to the variational family, and the remaining discrepancy is measured by $\mathrm{KL}\!\left(q(z;\nu^\star)\,\|\,p(z\mid x)\right)$.

\subsection{Specifying the Variational Family}

A critical modelling choice in VI is how we define the family $\mathcal{Q}$. This determines both the computational tractability and the expressive power of the approximation. The spectrum of choices ranges from full-form to fully factorised:

\paragraph{Full-form variational inference.} At one extreme, we could specify a joint distribution $q(z)$ with no factorisation, allowing all dependencies between parameters to be preserved. For example, $q(\beta, \tau_e)$ might be a joint distribution with covariance between $\beta$ and $\tau_e$. This offers maximum flexibility but requires optimising over a large number of variational parameters (covariance matrices grow quadratically with dimension) and may not admit closed-form updates.

\paragraph{Mean-field variational inference.} At the other extreme, we assume complete factorisation:
\[
  q(z) = \prod_{j=1}^J q_j(z_j),
\]
where $z = (z_1, \ldots, z_J)$ is partitioned into components and each $q_j(z_j)$ is an independent distribution. When complete factorisation is applied to regression, for instance, each parameter becomes its own factor: $q(\beta_1)q(\beta_2)\cdots q(\beta_p)q(\tau_e)$, which is fully atomised with each scalar parameter separate. Blocked structures, such as those we employ in this work, represent an intermediate choice that requires conditional conjugacy considerations to maintain computational tractability whilst preserving important posterior dependencies.

\paragraph{Structured mean-field (blocking).} Between these extremes lies structured mean-field, where we group strongly correlated parameters into blocks that preserve some dependencies:
\[
  q(z) = q(z_{\text{block}_1})\,q(z_{\text{block}_2})\cdots q(z_{\text{block}_K}).
\]
Each block maintains internal dependencies whilst remaining independent of other blocks. For Model 2 in this paper, the blocked family is $q(\beta, u)\,q(\tau_u)\,q(\tau_e)$, a deliberate choice that preserves dependence between fixed effects and random effects whilst treating precision components independently. This blocking is the tightest factorisation analytically feasible: we could make it more restrictive (giving each scalar parameter its own factor), but this would ignore known correlations and severely degrade inference. Conversely, we cannot make it less restrictive (e.g., $q(\beta, u, \tau_u)\,q(\tau_e)$) whilst maintaining closed-form updates. Our choice is thus optimal given the trade-off between analytical tractability and fidelity to posterior structure.

The blocking choice has profound implications. In Model 2, the true posterior exhibits strong dependence between $u$ and $\tau_u$: if the precision is small (variance large), the data support larger deviations $|u_j|$ from zero; if large (variance small), the posterior for each $u_j$ is pulled tightly towards zero (shrinkage). When we factorise as $q(\beta, u)\,q(\tau_u)$, this dependence is broken. The algorithm updates $q(\beta, u)$ given the current $q(\tau_u)$, then updates $q(\tau_u)$ given the current $q(\beta, u)$, but the two distributions cannot coordinate their uncertainty. This leads to systematic under-dispersion: $q(\tau_u)$ is too narrow, underestimating the true posterior uncertainty. Importantly, this under-dispersion is not an artefact of our blocking choice, but rather a fundamental limitation of mean-field VI itself given the constraints of analytical tractability.

This paper uses full mean-field factorisation for Model 1, and a partially blocked mean-field for Model 2. For Model 1, the factorisation $q(\beta)\,q(\tau_e)$ is relatively benign because $\beta$ and $\tau_e$ are only weakly correlated posteriorly. For Model 2, the factorisation $q(\beta, u)\,q(\tau_u)$ is still problematic because it breaks the strong dependence between random effects and their precision parameter, causing the precision posterior to collapse onto values that are too large (variances too small).

\subsection{The Evidence Lower Bound (ELBO)}

Directly minimising the KL divergence $\mathrm{KL}(q_\nu(z) \| p(z \mid x))$ is not possible because it depends on the intractable marginal likelihood $p(x)$. Instead, we work with the Evidence Lower Bound (ELBO), defined by
\begin{align}
  \mathcal{L}(\nu) &= \mathbb{E}_{q_\nu}\!\left[ \log p(x,z) \right] - \mathbb{E}_{q_\nu}\!\left[ \log q_\nu(z) \right], \\
  \intertext{where $\mathcal{L}(\nu)$ denotes the ELBO.\footnote{Some references write $\mathcal{L}(q)$ or $\mathrm{ELBO}(q)$; these are equivalent notational choices.} It can be shown that}
  \log p(x) &= \mathcal{L}(\nu) + \mathrm{KL}\bigl(q_\nu(z) \,\|\, p(z \mid x)\bigr),
\end{align}
so that for fixed data $x$, maximising $\mathcal{L}(\nu)$ is equivalent to minimising the KL divergence. The ELBO thus serves as a surrogate objective that we can evaluate using only $p(x,z)$ and $q_\nu(z)$.

The ELBO has a useful interpretation as a balance between two terms. The first, $\mathbb{E}_{q_\nu}[\log p(x,z)]$, is the expected log joint, which encourages $q_\nu(z)$ to place mass on configurations of $z$ that explain the data well. The second, $-\mathbb{E}_{q_\nu}[\log q_\nu(z)]$, is the entropy of $q_\nu$, which encourages the approximation to remain diffuse and avoid collapsing onto a single point. Optimising the ELBO therefore trades off goodness-of-fit against complexity.

Under mean-field factorisation, the ELBO can often be optimised via coordinate ascent: we cycle through factors $q_j(z_j)$, updating each in turn whilst holding the others fixed. For conjugate-exponential families, these updates have closed form. For non-conjugate models, we resort to gradient-based optimisation or sampling-based approximations to the ELBO gradient.

\subsection{Implications of the Factorisation Choice}

The mean-field assumption imposes a strong structural constraint on the approximation: it restricts the variational family to factorised distributions where parameters are independent. When the true posterior has correlations between parameters (as it nearly always does), this independence constraint prevents the approximation from representing those correlations, and the variational posterior systematically underestimates uncertainty. This manifests differently for different parameter types \citep{blei2017variational}. Location parameters such as regression coefficients $\beta$ or random effects $u_j$ tend to have posterior means that are reasonably accurate, with variances mildly underestimated. Scale parameters such as standard deviations $\sigma$, variances $\tau_u^{-1}$, or precision parameters $\tau$ tend to have posteriors that are severely under-dispersed, with mass concentrated on smaller values than the true posterior supports.

The asymmetry arises because precision components are hyper-parameters: they appear in the priors of other parameters \citep{turner2011two}. In Model 2, $\tau_u$ governs the distribution $u_j \sim N(0, \tau_u^{-1})$, creating strong posterior dependence between $\tau_u$ and $u$. Mean-field factorisation breaks this dependence, and the algorithm loses information about their joint uncertainty. The result is a posterior $q(\tau_u)$ that is too narrow, leading to over-shrinkage of the random effects and overconfident predictions.

This is the central phenomenon we demonstrate empirically using synthetic data with known ground truth in the remainder of this paper. Model 1 establishes that VI works when dependencies are weak; Model 2 reveals where it fails when dependencies are strong. The practical value lies in the contrast: by starting where VI succeeds and advancing to where it struggles, we build both competence in the method and awareness of its limitations.

\subsection{Models and Study Design}

\subsubsection{Stage 1: Linear Regression Models (Model 1)}

Our journey begins with Bayesian linear regression, a setting where exact posterior inference is analytically tractable. This provides an ideal starting point for several reasons.

When learning VI, it is crucial to have ground truth against which to validate our approximations. Knowing the data-generating process does not, by itself, yield the full posterior distribution of the parameters; in most models the true posterior must still be computed. Model 1 is exceptional because with conjugate Gaussian priors and likelihood, the posterior for linear regression coefficients is known exactly. This allows us to implement VI algorithms and directly compare the approximate posterior $q_\nu(\beta)$ against the true posterior $p(\beta \mid y, X)$. Any discrepancies we observe reflect limitations of our variational family or optimisation procedure, not uncertainty about what the correct answer should be.

This teaches the mechanics of VI in a forgiving environment. We learn to specify variational families (typically mean-field Gaussians), compute gradients of the ELBO, and monitor convergence. We observe how the approximation quality depends on the variational family's flexibility. Most importantly, we build confidence in the method by seeing it recover known posteriors, establishing a benchmark for what a reasonable approximation looks like when we compare against the true parameter values we used to generate the data.

\subsubsection{Stage 2: Hierarchical Models with Random Effects (Model 2)}

Now, having established VI's validity in a simple setting, we move to hierarchical models with random intercepts. Here, the motivation for approximate inference becomes clear, and the limitations of mean-field factorisation emerge.

Real data often has grouped structure: students within schools, patients within hospitals, measurements within subjects. Hierarchical models capture this by allowing group-specific parameters (random effects) that are themselves drawn from a population distribution. The posterior now involves potentially hundreds of latent variables (one per group), making exact inference impractical. This is precisely where VI's scalability advantage emerges.

\textbf{Sensitivity Analysis:} To assess the robustness of VB performance across different hierarchical structures, we implement Model 2 with five different numbers of groups: $Q \in \{5, 10, 20, 50, 100\}$. This variation allows us to examine whether under-dispersion depends on the granularity of the grouping structure. Synthetic data are generated with known precision components for each Q value, maintaining consistent group sizes and overall sample characteristics.

This stage reveals VI's computational advantage. Whilst Gibbs sampling would need to sample hundreds of correlated variables, VI factorises the approximate posterior. This independence assumption is clearly wrong—random effects are correlated through shared hyperparameters—but it makes optimisation tractable. We learn to diagnose when this approximation is adequate (often surprisingly so for prediction) and when it breaks down (typically when posterior correlations are strong).

The Model 2 implementation introduces the under-dispersion phenomenon. When we compare the variational posterior for $\tau_u$ against Gibbs sampling estimates, we observe systematic bias: the VI distribution is too narrow, placing excessive mass on smaller values. This leads to over-shrinkage of the random effects: individual group intercepts are pulled too tightly towards the global mean, and the model appears more confident than the data warrant.

\textbf{Implementation Note:} All comparisons use a customised Gibbs sampler that exploits conjugate conditional distributions for Models 1 and 2. The sampler iterates through full conditionals for each parameter block ($\beta$, $u$, $\tau_e$, $\tau_u$) sequentially. This implementation differs from standard approaches such as Stan's NUTS algorithm. Appendix A documents the sampler design and implementation details. A full validation against Stan/NUTS is planned as future work.

\subsection{Rationale for Model Progression}

Looking back across the two stages, a clear narrative emerges. We begin where understanding is possible (Model 1), advance to where VI becomes necessary and its limitations become visible (Model 2).

Validation becomes approximation. In Model 1, we validate VI against exact inference. In Model 2, we validate through predictive performance and Gibbs sampling comparisons, accepting that perfect validation is not feasible.

The mean-field independence assumption appears in both stages but with different implications. For Model 1, it is nearly exact because parameters are approximately independent posteriorly. For Model 2, it demonstrably breaks the dependence between random effects and precision components, causing systematic under-dispersion.

Computational trade-offs become apparent. Model 1 shows VI is fast even when alternatives exist. Model 2 shows VI scales to hundreds of latent variables where Gibbs sampling slows, but at the cost of underestimating uncertainty in precision components.

\newpage

\section{Results}

\subsection{Standard Deviation Ratios}

To quantify the under-dispersion phenomenon systematically, we compute standard deviation ratios comparing variational posteriors against Gibbs sampling baselines across all precision components in our models. The standard deviation ratio is defined as
\[
  \text{SD Ratio} = \frac{\text{SD}_{\text{VB}}(\theta)}{\text{SD}_{\text{Gibbs}}(\theta)},
\]
where values below 1.0 indicate under-dispersion (VB is too confident), values near 1.0 indicate good agreement, and values above 1.0 would indicate over-dispersion (rare in practice).

For Model 1 (linear regression), SD ratios for $\tau_e$ cluster around 0.8--0.95, reflecting mild under-dispersion that is typical even in simple settings (Figure~\ref{fig:M1_variance_ratio}). For Model 2 (hierarchical linear), the precision component $\tau_u$ exhibits severe under-dispersion with ratios of 0.4--0.6 across all tested Q values (Figure~\ref{fig:M2_variance_ratio} shows representative results for $Q=50$), confirming that mean-field approximations systematically underestimate uncertainty in hyper-parameters regardless of group structure granularity.

These diagnostics confirm the theoretical prediction: VI systematically under-estimates uncertainty for precision components in hierarchical models. This under-dispersion is not an artefact of poor optimisation or inadequate convergence—the ELBO has converged, and the approximate posteriors are optimal within the mean-field family. Rather, it is a fundamental consequence of the independence assumption imposed by factorisation. Although only ever so slight, a visual change is present between the two diagnostic plots.

The practical implication is clear: when using mean-field VI for hierarchical models, posterior standard deviations for precision components should be interpreted with caution. Predictive means may remain accurate, but credible intervals will be too narrow, leading to overconfident inference about population-level parameters.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{../figs/M1_diagnostic_variance_ratio.png}
\caption{SD ratios for Model 1 (linear regression) precision components. Ratios cluster around 0.8--0.95, indicating mild under-dispersion.}
\label{fig:M1_variance_ratio}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\textwidth]{../figs/M2_diagnostic_variance_ratio.png}
\caption{SD ratios for Model 2 (hierarchical linear) precision components. The random effects precision $\tau_u$ shows severe under-dispersion (ratios 0.4--0.6).}
\label{fig:M2_variance_ratio}
\end{figure}

\subsection{Posterior Distributions: Visual Evidence of Under-Dispersion}

The under-dispersion is apparent when comparing the full posterior distributions across all parameters. Figure~\ref{fig:M1_posteriors} displays the four-panel comparison for Model 1, where each panel shows the VB approximation overlaid against the Gibbs sampling posterior. Notably, the VB posteriors are consistently narrower, particularly for the residual precision $\tau_e$.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figs/M1_s1_4Panel.png}
\caption{Four-panel posterior comparison for Model 1 (linear regression). Each panel shows the VB approximation overlaid against the Gibbs sampling posterior. VB posteriors are systematically narrower across all parameters, confirming under-dispersion.}
\label{fig:M1_posteriors}
\end{figure}

For Model 2 (hierarchical linear), the pattern intensifies. Figure~\ref{fig:M2_posteriors} shows the eight-panel comparison where the dramatic narrowing of VB posteriors is especially pronounced for the random effects precision $\tau_u$ and the regression coefficients. This visual evidence directly supports the quantitative findings: mean-field VI produces posterior distributions that systematically underestimate uncertainty.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.9\textwidth]{../figs/M2_vb_Q50_8Panel.png}
\caption{Eight-panel posterior comparison for Model 2 (hierarchical linear, $Q=50$). The VB approximation is noticeably narrower than the Gibbs sampling posterior, with severe under-dispersion evident for the random effects precision $\tau_u$. Results are consistent across all tested Q values.}
\label{fig:M2_posteriors}
\end{figure}

\subsection{Computational Efficiency: Speed-Accuracy Trade-off}

The primary practical advantage of variational Bayes is computational speed. Table~\ref{tab:timing} presents wall-clock runtime comparisons between VB and Gibbs sampling for both models across multiple problem sizes.

\begin{table}[htbp]
\centering
\caption{Computational efficiency comparison across Models 1 and 2. VB completes in constant time whilst Gibbs sampling exhibits quadratic scaling with the number of groups $Q$.}
\label{tab:timing}
\begin{tabular}{lrrrr}
\hline
\textbf{Model} & \textbf{Q} & \textbf{VB Time (s)} & \textbf{Gibbs Time (s)} & \textbf{Speedup} \\
\hline
M1 (Linear) & — & 0.05 & 6.42 & 128$\times$ \\
\hline
M2 (Hierarchical) & 5 & 0.25 & 12.73 & 51$\times$ \\
 & 10 & 0.01$^{\dagger}$ & 14.96 & 1,496$\times$ \\
 & 20 & 0.02 & 14.56 & 728$\times$ \\
 & 50 & 0.02 & 34.18 & 1,709$\times$ \\
 & 100 & 0.10 & 102.20 & 1,022$\times$ \\
\hline
\multicolumn{5}{l}{\small $^{\dagger}$VB time $<$ 0.01s, rounded to 0.01s for speedup calculation.}
\end{tabular}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{../figs/timing_speedup_barh.png}
\caption{Speedup ratios from Table~\ref{tab:timing}. Model 1 is shown in blue and Model 2 configurations in grey.}
\label{fig:timing_speedup_barh}
\end{figure}

Figure~\ref{fig:timing_speedup_barh} visualises these ratios. The pattern is striking: Model 1 shows a 128$\\times$ speedup. For Model 2, VB computational time remains approximately constant (0.01--0.25 seconds) despite varying $Q$ from 5 to 100, suggesting its cost is driven by sample size $n$ rather than the number of groups. In contrast, Gibbs sampling exhibits strong dependence on $Q$, with times growing quadratically from 12.7 seconds at $Q=5$ to 102.2 seconds at $Q=100$. This produces speedups ranging from 51$\\times$ at $Q=5$ to 1,709$\\times$ at $Q=50$.

This computational advantage comes at the cost of under-dispersion documented in previous sections: VB completes quickly but underestimates uncertainty. The trade-off is clear: practitioners prioritising speed and point estimation may accept VB's under-dispersion, whilst those requiring accurate uncertainty quantification must invest in sampling-based methods such as Gibbs sampling.

\newpage

\subsection{Aggregated Reliability Assessment}

\subsection{Per-Parameter Evidence: The Fundamental Metric}

The evidence of under-dispersion rests on the standard deviation ratio for each parameter:
\[
  \text{SD Ratio}_i = \frac{\text{SD}_{\text{VB}}(\theta_i)}{\text{SD}_{\text{Gibbs}}(\theta_i)},
\]
where subscript $i$ indexes parameters within a model (e.g., regression coefficients $\beta_1, \ldots, \beta_p$, residual precision $\tau_e$, and random effects precision $\tau_u$ in hierarchical models). Each SD ratio directly quantifies whether VB underestimates uncertainty for that specific parameter:
\begin{itemize}
  \item $r_i < 1.0$: VB is \textbf{under-dispersed} (too confident) for parameter $i$
  \item $r_i \approx 1.0$: VB is well-calibrated for parameter $i$
  \item $r_i > 1.0$: VB is over-dispersed for parameter $i$ (rare in practice)
\end{itemize}

Figures~\ref{fig:M1_variance_ratio} and~\ref{fig:M2_variance_ratio} present these SD ratios for all parameters in each model. The pattern is unambiguous: the majority of ratios lie substantially below 1.0, directly demonstrating systematic under-dispersion across the parameter space.

\subsection{Composite SD Ratio Metrics: Aggregating to Model-Level Evidence}

Because practitioners need a single, comprehensive number to assess model reliability, we aggregate the per-parameter SD ratios using two complementary approaches:

\paragraph{Harmonic Mean Aggregation (Conservative Summary).}
The harmonic mean of SD ratios across all parameters in a model provides a conservative summary:
\[
  H = \frac{n}{\sum_{i=1}^{n} 1/r_i},
\]
where $n$ is the number of parameters and $r_i = \text{SD Ratio}_i$. This metric is conservative because it emphasises poor-performing parameters: a single very low ratio (e.g., $r_{\tau_u} = 0.45$) substantially reduces $H$, reflecting the principle that a model's reliability is limited by its worst component. 

For Model 1 (linear regression), the harmonic mean is $H_{\text{M1}} = 0.910$, meaning VB posteriors are on average $9.0\%$ narrower than Gibbs sampling. This demonstrates mild, uniform under-dispersion across parameters, with all individual SD ratios in the narrow range 0.88--0.93.

For Model 2 (hierarchical linear), the harmonic mean falls to $H_{\text{M2}} = 0.823$, meaning VB posteriors are $17.7\%$ narrower on average. Critically, this dramatic reduction from Model 1 is driven almost entirely by the random effects precision: whilst fixed effects and observation precision have SD ratios $\approx 0.87$--$0.91$, the hyper-parameter $\tau_u$ has $\text{SD Ratio}_{\tau_u} = 0.52$. This single parameter reduces the harmonic mean by almost 10 percentage points, illustrating the conservative principle: even when most parameters are well-estimated, severe under-dispersion in a single critical parameter compromises the entire model.

The harmonic mean thus provides a single number that conclusively demonstrates: Model 2 exhibits systematic, severe under-dispersion that cannot be dismissed as a minor artefact.

\paragraph{Weighted Mean Aggregation (Importance-Based).}
A weighted average groups parameters by type and assigns weights reflecting inferential priority:
\[
  W = \sum_{j \in \text{types}} w_j \cdot \bar{r}_j,
\]
where $\bar{r}_j$ is the mean SD ratio for parameter type $j$ and $w_j$ is its assigned weight. In hierarchical models, typical weights are: fixed effects ($w_{\beta} = 0.40$), observation precision ($w_{\tau_e} = 0.30$), and precision components ($w_{\tau_u} = 0.30$). This weighting recognises that all three contribute to inference quality; the equal weight on scale parameters ($\tau_e$) and hyper-parameters ($\tau_u$) reflects that both are critical for credible intervals and predictive intervals respectively.

For Model 1, the weighted mean is $W_{\text{M1}} = 0.632$, indicating that VB is $36.8\%$ narrower when importance is accounted for. For Model 2, the weighted mean is $W_{\text{M2}} = 0.777$, indicating $22.3\%$ overall narrowing. Although Model 2's weighted score appears better than its harmonic mean, this is misleading: the weight on $\tau_u$ (0.30) prevents $\tau_u$'s severe under-dispersion (0.52) from dominating. In applications where hyper-parameter estimation is paramount (e.g., designing new experiments or forecasting), the weighted mean understates the risk.

\subsection{Comparative Summary: Harmonic vs.\ Weighted Aggregation}

To provide practitioners with a clear, quantitative picture of model reliability, we summarise both aggregation methods in Table~\ref{tab:aggregation_summary}. The harmonic mean captures worst-case scenarios; the weighted mean accounts for inferential priorities.

\begin{table}[htbp]
\centering
\caption{Aggregated SD ratio summary: harmonic mean (conservative) and weighted mean (importance-based).}
\label{tab:aggregation_summary}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabular}{lcccc}
\hline
\textbf{Aggregation} & \textbf{Formula} & \textbf{Model 1} & \textbf{Model 2} & \textbf{Interpretation} \\
\textbf{Method} &  & \textbf{(Linear)} & \textbf{(Hierarchical)} & \\
\hline
Harmonic Mean & $H = \frac{n}{\sum_{i=1}^{n} 1/r_i}$ & $0.910$ & $0.823$ & Conservative; emphasises weak parameters \\[0.6ex]
Weighted Mean & $W = \sum_{j} w_j \bar{r}_j$ & $0.632$ & $0.777$ & Application-specific; accounts for priority \\[0.6ex]
\hline
\end{tabular}
\end{table}

\paragraph{Interpretation of Aggregated Results.}

\begin{center}
\begin{tabular}{c c c c c c}
\toprule
\textbf{Model} & \textbf{Q} & \textbf{$H$} & \textbf{$W$} & \textbf{Narrowing ($H$)} & \textbf{$\tau_u$ Ratio} \\
\midrule
Model 1 & --- & 0.910 & 0.632 & 9.0\% & --- \\
\midrule
\multirow{5}{*}{Model 2} & 5   & 0.892 & 0.908 & 10.8\% & 0.817 \\
                         & 10  & 0.936 & 0.930 & 6.4\%  & 0.850 \\
                         & 20  & 0.935 & 0.921 & 6.5\%  & 0.801 \\
                         & 50  & 0.886 & 0.868 & 11.4\% & 0.658 \\
                         & 100 & 0.719 & 0.750 & 28.1\% & 0.372 \\
\bottomrule
\end{tabular}
\end{center}

\paragraph{Key findings by Q:}
\begin{itemize}
\item \textbf{Q = 5--20:} Harmonic mean H $\approx$ 0.89--0.94 indicates mild-to-moderate under-dispersion (6--11\% narrower). $\tau_u$ ratios 0.80--0.85 are acceptable.
\item \textbf{Q = 50:} Under-dispersion worsens (H = 0.886, 11.4\% narrower). $\tau_u = 0.658$ shows precision component deterioration.
\item \textbf{Q = 100:} Severe under-dispersion (H = 0.719, 28.1\% narrower). $\tau_u = 0.372$ indicates catastrophic variance underestimation.
\item \textbf{Model 1:} Consistent mild under-dispersion (H = 0.910) regardless of model complexity.
\end{itemize}

\paragraph{Unified Conclusion: What the Aggregated Metrics Demonstrate.}

Both aggregation methods conclusively demonstrate systematic under-dispersion:

\begin{itemize}
  \item \textbf{Model 1:} Harmonic mean $H = 0.910$ demonstrates mild, uniform under-dispersion across a simple model. This is a tolerable level for many applications.
  
  \item \textbf{Model 2:} Harmonic mean $H = 0.823$ demonstrates severe under-dispersion driven by the hierarchical structure, specifically the precision parameter $\tau_u$. The weighted mean $W = 0.777$ provides a more optimistic picture, but this is misleading if hyper-parameters are important.
  
  \item \textbf{Practical Implication:} A practitioner using Model 2 VB must decide: (i) If fixed effects are primary, $W = 0.777$ indicates acceptable risk. (ii) If hyper-parameters matter (which they often do in hierarchical models), neither $H$ nor $W$ is truly acceptable. Instead, the user should either (a) report posterior intervals with an explicit caveat about under-dispersion, (b) use alternative methods (e.g., importance weighting, message passing), or (c) supplement VB with Gibbs sampling as a baseline.
\end{itemize}

\section{Discussion}

\subsection{Model-Level Synthesis}

Combining per-parameter ratios with aggregated metrics yields a consistent body of evidence:

\begin{enumerate}
  \item \textbf{Per-Parameter Level:} Figures~\ref{fig:M1_variance_ratio} and~\ref{fig:M2_variance_ratio} directly show that individual SD ratios are predominantly below 1.0, demonstrating under-dispersion parameter-by-parameter.
  
  \item \textbf{Visual Level:} Figure~\ref{fig:M1_posteriors} provides a four-panel comparison for Model 1, and Figure~\ref{fig:M2_posteriors} provides an eight-panel comparison for Model 2, allowing visual verification that VB posteriors are systematically narrower than Gibbs posteriors across the parameter space.
  
  \item \textbf{Aggregated Level:} The harmonic mean (conservative) and weighted mean (importance-weighted) both fall substantially below 1.0 for Model 2, providing a single number that captures the magnitude of under-dispersion across all parameters simultaneously.
\end{enumerate}

The intuition is unambiguous: under-dispersion is not a minor artefact affecting a single parameter. Rather, it is a \textbf{systematic phenomenon} affecting all or nearly all parameters, with aggregated evidence indicating that mean-field VI produces unreliable posterior uncertainty even after convergence.

\paragraph{Limitation and response to reviewer comment.}
We have not presented posterior distributions for the individual random effects $u_j$, so we cannot directly quantify how the artificially tightened posterior for $\tau_u$ propagates to the distribution of $u_j$. The current evidence therefore demonstrates under-dispersion for precision components, but it does not, by itself, establish the magnitude of the downstream impact on all random effects. A natural extension is to include posterior plots for selected $u_j$ alongside coverage diagnostics, allowing direct assessment of the practical consequences of under-dispersion in $\tau_u$.

\section{Conclusion}

\subsection{Summary}
This paper focuses on VI applied to two models. Model 1 (linear regression) shows strong performance, with SD ratios near 0.90--0.95 for location parameters and 0.80--0.85 for observation precision. Model 2 (hierarchical linear) reveals the limitation: precision components are severely under-dispersed (SD ratios 0.40--0.70). This is a predictable consequence of factorisation, which breaks dependence between random effects and their precision component.

\subsection{Practical implications}
Mean-field VI is reliable for fast exploration and fixed-effect estimation, but it understates uncertainty for precision components in hierarchical models. When variance estimation is central, practitioners should validate against Gibbs sampling and adjust credible intervals (e.g., 1.4--2.5 for precision components, equivalently variances). The author will conduct further research to quantify how under-dispersion in $\tau_u$ propagates to the posterior distributions of individual random effects $u_j$, including explicit posterior plots and coverage diagnostics.

\subsection{Limitations and future work}
This study focuses on VI for Models 1 and 2. Model 3 (hierarchical logistic regression) was excluded due to implementation errors; correct implementation requires data augmentation techniques such as the Pólya-Gamma method \citep{polson2013bayesian}, which the author will study in future work. Future priorities include: (i) evaluating structured mean-field blocking to reduce under-dispersion, (ii) developing diagnostics or post-hoc corrections when sampling baselines are unavailable, and (iii) validating results using Stan's NUTS to confirm agreement across independent sampling implementations.

\subsection{Concluding remarks}
Variational inference delivers speed but can distort uncertainty in hierarchical models. The worked examples here provide a practical guide to when VI is suitable, when it is not, and how to verify its reliability.

\section*{Acknowledgements}
I acknowledge Dr John Holmes as my advisor and thank him for his patience and guidance. AI assistance was used for debugging, condensing language, developing figures, and reviewing concepts outside the author's prior background.

\input{appendix_sampling.tex}

\bibliography{references}

\end{document}
