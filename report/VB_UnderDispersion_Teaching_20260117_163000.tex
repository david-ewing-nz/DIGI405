\documentclass[11pt]{article}

% ---- Packages ----
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, bm}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage[british]{babel}

\geometry{margin=1in}

% ---- Document ----
\begin{document}

% ---- Abstract ----
\begin{abstract}
Variational Bayes (VB) offers a fast, scalable alternative to Markov chain Monte Carlo (MCMC) for approximate Bayesian inference, but this computational efficiency comes at a cost: systematic under-estimation of posterior uncertainty, particularly for variance components in hierarchical models. This paper provides a pedagogical demonstration of the under-dispersion phenomenon in mean-field variational Bayes (MFVB), designed specifically for researchers new to variational methods. We present two working examples of increasing complexity---a linear regression model with conjugate structure (Model 1) and a random-intercept hierarchical model (Model 3)---to illustrate how the mean-field independence assumption q($\bm{\beta}$, u, $\tau_e$, $\tau_u$) = q($\bm{\beta}$, u)q($\tau_e$)q($\tau_u$) forces the algorithm to ignore critical posterior correlations between random effects u and their variance component $\sigma^2_u = 1/\tau_u$. Through direct comparison with Gibbs sampling (our MCMC gold standard), we quantify under-dispersion using standard deviation ratios: SD$_{\text{VB}}$/SD$_{\text{MCMC}}$. Our results confirm that whilst location parameters ($\bm{\beta}$) exhibit mild under-dispersion (SD ratios $\approx$ 0.85--0.95), variance components show severe under-dispersion (SD ratios $\approx$ 0.20--0.70), with the effect most pronounced for hyper-parameters that govern the distribution of other parameters. All analyses are fully reproducible, with complete R code, Stan models, and explicit derivations of coordinate ascent updates provided. This work emphasises the practical importance of understanding when and why VB fails, enabling practitioners to make informed decisions about when to use variational methods and when to invest in more expensive MCMC alternatives.
\end{abstract}

\noindent\textbf{Keywords:} variational Bayes, mean-field approximation, under-dispersion, variance components, hierarchical models, coordinate ascent, Gibbs sampling, reproducible research

% ---- Main sections ----
\section{Introduction}

\subsection{Motivation: The Speed-Accuracy Trade-off in Approximate Inference}

Modern Bayesian analysis often requires inference in models where the posterior distribution p($\bm{\theta}$ $|$ $\mathbf{y}$) cannot be computed analytically. The central challenge is evaluating the marginal likelihood (evidence):
\begin{equation}
p(\mathbf{y}) = \int p(\mathbf{y}, \bm{\theta}) \, d\bm{\theta},
\end{equation}
which appears in the denominator of Bayes' rule. For most realistic models---particularly those with latent variables, hierarchical structure, or non-conjugate likelihoods---this integral is intractable.

The two dominant paradigms for approximate Bayesian inference represent fundamentally different computational strategies:

\textbf{Markov chain Monte Carlo (MCMC)} constructs a Markov chain whose stationary distribution is the target posterior. Methods such as Gibbs sampling and Hamiltonian Monte Carlo (HMC) provide samples $\bm{\theta}^{(1)}, \ldots, \bm{\theta}^{(S)} \sim p(\bm{\theta} | \mathbf{y})$ that converge to the true posterior as $S \to \infty$. These methods are widely regarded as the gold standard for accuracy, producing well-calibrated uncertainty estimates when properly tuned. However, MCMC can be computationally expensive, particularly for high-dimensional models or large datasets, and diagnosing convergence requires care.

\textbf{Variational Bayes (VB)} reframes posterior inference as an optimisation problem. Rather than sampling from p($\bm{\theta}$ $|$ $\mathbf{y}$), VB selects a tractable family of distributions $\mathcal{Q}$ and finds the member q$^*$($\bm{\theta}$) $\in$ $\mathcal{Q}$ closest to the true posterior, typically measured by Kullback--Leibler (KL) divergence. This optimisation is equivalent to maximising the Evidence Lower Bound (ELBO), which provides a lower bound on the log marginal likelihood. VB is often orders of magnitude faster than MCMC, making it attractive for large-scale applications and iterative model development.

However, this speed comes at a cost. The choice of variational family $\mathcal{Q}$ introduces approximation error: if the true posterior lies outside $\mathcal{Q}$, even optimal VB will produce biased estimates. The most common choice---mean-field variational Bayes (MFVB)---assumes posterior independence between parameter groups, factorising q($\bm{\theta}$) = $\prod_j$ q$_j$($\theta_j$). Whilst this factorisation enables efficient coordinate ascent optimisation, it systematically underestimates posterior uncertainty, particularly for parameters that exhibit strong posterior correlation in the true posterior.

\subsection{The Under-dispersion Problem: Variance Components at Risk}

Under-dispersion refers to the phenomenon where a variational posterior has smaller variance than the true posterior. For a parameter $\theta$, we quantify this using the standard deviation ratio:
\begin{equation}
\text{SD Ratio}(\theta) = \frac{\text{SD}_{\text{VB}}(\theta)}{\text{SD}_{\text{MCMC}}(\theta)}.
\end{equation}
Values substantially below 1.0 indicate that VB is overconfident, underestimating the true posterior uncertainty.

Empirical studies and theoretical analyses have consistently shown that under-dispersion is not uniform across parameter types. A robust pattern emerges:

\begin{itemize}
\item \textbf{Location parameters} (e.g., regression coefficients $\bm{\beta}$): Mild under-dispersion, with SD ratios typically 0.80--0.95.
\item \textbf{Scale parameters} (e.g., residual variance $\sigma^2$): Moderate under-dispersion, with SD ratios typically 0.60--0.80.
\item \textbf{Variance components} (e.g., random effect variance $\sigma^2_u$ in hierarchical models): Severe under-dispersion, with SD ratios as low as 0.20--0.70.
\end{itemize}

The third category is particularly concerning because variance components are \emph{hyper-parameters}---parameters that appear in the prior distributions of other parameters. In a random-intercept model, for instance, the variance component $\sigma^2_u$ governs the distribution of random intercepts:
\begin{equation}
u_j \sim \mathcal{N}(0, \sigma^2_u), \quad j = 1, \ldots, J.
\end{equation}
The true posterior p(u, $\sigma^2_u$ $|$ $\mathbf{y}$) exhibits strong correlation: if $\sigma^2_u$ is large, the data can support larger deviations $|u_j|$ from zero; if $\sigma^2_u$ is small, each $u_j$ is pulled tightly towards zero (shrinkage). Mean-field VB breaks this dependence by factorising q(u, $\sigma^2_u$) = q(u) q($\sigma^2_u$), forcing the algorithm to optimise over each factor independently. During coordinate ascent:
\begin{enumerate}
\item The algorithm updates q(u) given the current q($\sigma^2_u$), averaging over the variational uncertainty in $\sigma^2_u$.
\item It then updates q($\sigma^2_u$) given the current q(u), averaging over the variational distribution of u.
\end{enumerate}
Because the factors cannot communicate their joint uncertainty, q($\sigma^2_u$) systematically underestimates the true posterior variance. The under-dispersed estimate of $\sigma^2_u$ has cascading effects: it leads to over-shrinkage of random effects towards zero and overconfident predictions.

\subsection{The Need for Pedagogical Demonstrations}

Whilst the theoretical properties of variational inference are well-established in the machine learning literature, practitioners---particularly those trained in traditional statistics or applied fields---often lack intuition about when and why VB fails. Textbook treatments typically emphasise derivations of the ELBO and coordinate ascent updates, but rarely provide concrete, reproducible examples demonstrating the under-dispersion phenomenon with real code and data.

This paper addresses that gap by providing two fully worked examples designed for a teaching audience. Our goals are:
\begin{enumerate}
\item \textbf{Demonstrate} under-dispersion through direct posterior comparisons between MFVB and Gibbs sampling.
\item \textbf{Quantify} the effect using SD ratios, showing how the severity depends on parameter type.
\item \textbf{Explain} the mechanism: why mean-field factorisation causes under-dispersion, with particular focus on variance components.
\item \textbf{Implement} all methods from scratch in R, with explicit derivations and step-by-step walkthroughs.
\item \textbf{Ensure} complete reproducibility: all code, data generation procedures, prior specifications, and hyperparameters are documented.
\end{enumerate}

\subsection{The Two-Model Hierarchy: From Simple to Complex}

We structure our investigation as a progression from simple to complex:

\textbf{Model 1: Linear Regression with Conjugate Structure}

Our first model is a standard linear regression with normal likelihood and conjugate Normal-Inverse-Gamma prior:
\begin{align}
\mathbf{y} &\sim \mathcal{N}(\mathbf{X}\bm{\beta}, \sigma^2 \mathbf{I}), \\
\bm{\beta} &\sim \mathcal{N}(\bm{\mu}_0, \sigma^2 \bm{\Lambda}_0^{-1}), \\
\sigma^2 &\sim \text{Inverse-Gamma}(a_0, b_0).
\end{align}
This model serves as a calibration case: the exact posterior is available in closed form, allowing us to compare MFVB not only against MCMC but also against the analytical solution. We use synthetic data with known ground truth ($n = 500$ observations, $K = 4$ predictors including intercept, correlated design matrix) to validate that all methods recover the correct parameter values. The under-dispersion in this model is relatively mild, establishing a baseline.

\textbf{Model 3: Random-Intercept Hierarchical Model}

Our flagship example is a hierarchical model with group-specific random intercepts:
\begin{align}
y_i &= \mathbf{x}_i^\top \bm{\beta} + u_{j[i]} + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \tau_e^{-1}), \\
u_j &\sim \mathcal{N}(0, \tau_u^{-1}), \quad j = 1, \ldots, J,
\end{align}
where $\tau_e$ is the residual precision and $\tau_u$ is the random effect precision (the reciprocal of the variance component $\sigma^2_u = 1/\tau_u$). This model demonstrates severe under-dispersion for $\tau_u$ because:
\begin{itemize}
\item $\tau_u$ is a hyper-parameter governing the distribution of random effects u.
\item The true posterior p(u, $\tau_u$ $|$ $\mathbf{y}$) has strong correlation.
\item MFVB factorises q(u)q($\tau_u$), breaking this dependence.
\end{itemize}
We generate synthetic data with $n = 300$ observations distributed across two scenarios: (1) 30 sparse groups (10 observations each), and (2) 6 rich groups (50 observations each). This allows us to examine how the severity of under-dispersion depends on the informativeness of the data at the group level.

\subsection{Methodological Contributions and Emphasis on Reproducibility}

This paper makes the following contributions:

\begin{enumerate}
\item \textbf{Explicit algorithm derivations}: We derive mean-field coordinate ascent updates from first principles, highlighting the critical role of \emph{trace terms} that account for parameter uncertainty (e.g., $\text{tr}(\mathbf{X}^\top\mathbf{X} \bm{\Sigma}_\beta)$ in the update for $\tau_e$). These terms are often omitted in informal presentations but are essential for correct VB implementation.

\item \textbf{Working R implementations}: All algorithms (MFVB, Gibbs sampling, exact posterior for Model 1) are implemented from scratch in R with full documentation. We avoid black-box software (e.g., Stan's \texttt{vb()} function, Python's \texttt{pyro}) to ensure readers understand every computational step.

\item \textbf{SD ratio diagnostic}: We systematically compute SD ratios for all parameters, creating a quantitative summary of under-dispersion that is easily interpretable and comparable across models.

\item \textbf{Pedagogical code style}: Code is written for readability over efficiency, with extensive comments, step-by-step single-iteration walkthroughs, and consistent variable naming aligned with mathematical notation.

\item \textbf{Reproducibility guarantees}: We document all random seeds, prior hyperparameters, convergence tolerances, and MCMC sampling settings. All results can be exactly replicated from the provided code.
\end{enumerate}

Our target audience is practitioners and students encountering variational methods for the first time, who may have strong statistical training but limited exposure to modern machine learning optimisation techniques.

\subsection{Paper Organisation}

The remainder of this paper is organised as follows. Section~2 reviews the variational inference framework and positions our work relative to existing literature. Section~3 formally defines our two models and introduces the SD ratio as our primary under-dispersion diagnostic. Section~4 derives the mean-field coordinate ascent algorithm and Gibbs sampler, with particular attention to implementation details. Section~5 describes our experimental setup, including data generation, prior specifications, and computational environment. Section~6 presents results for both models, with posterior density plots and SD ratio tables. Section~7 discusses the theoretical mechanisms underlying under-dispersion and their practical implications. Section~8 concludes with recommendations for practitioners and directions for future work. Appendices provide complete algorithm pseudo-code, ELBO derivations, and Stan model specifications for independent verification.

\section{Related Work}

\subsection{Variational Inference: Foundations and Approximation Families}

Variational Bayes transforms posterior inference from a sampling problem into an optimisation problem. The core idea, dating back to work in statistical physics and later formalised for machine learning, is to approximate an intractable posterior p($\bm{\theta}$ $|$ $\mathbf{y}$) with a tractable distribution q($\bm{\theta}$) chosen from a restricted family $\mathcal{Q}$. The approximation is obtained by minimising the Kullback--Leibler (KL) divergence from q to p:
\begin{equation}
q^*(\bm{\theta}) = \arg\min_{q \in \mathcal{Q}} \text{KL}(q(\bm{\theta}) \| p(\bm{\theta} \mid \mathbf{y})).
\end{equation}

Direct optimisation of this objective is infeasible because it requires evaluating the intractable marginal likelihood p($\mathbf{y}$). However, minimising KL divergence is equivalent to maximising the Evidence Lower Bound (ELBO):
\begin{equation}
\mathcal{L}(q) = \mathbb{E}_q[\log p(\mathbf{y}, \bm{\theta})] - \mathbb{E}_q[\log q(\bm{\theta})],
\end{equation}
which depends only on the joint distribution p($\mathbf{y}$, $\bm{\theta}$) and the variational distribution q($\bm{\theta}$), both of which we can evaluate.

Two broad classes of variational families dominate the literature:

\textbf{Fixed-form variational Bayes (FFVB)} restricts q($\bm{\theta}$) to a specific parametric family, typically multivariate Gaussian: q($\bm{\theta}$) = $\mathcal{N}(\bm{\mu}$, $\bm{\Sigma}$). The ELBO is then optimised over the mean vector $\bm{\mu}$ and covariance matrix $\bm{\Sigma}$ using gradient-based methods. Modern implementations employ automatic differentiation and natural gradient ascent to handle high-dimensional parameter spaces. Whilst FFVB can capture correlations within its chosen family, it may exhibit numerical instability and can suffer from over-dispersion if the optimisation landscape is poorly behaved.

\textbf{Mean-field variational Bayes (MFVB)} imposes a factorised structure on q($\bm{\theta}$), typically assuming complete independence between parameter groups:
\begin{equation}
q(\bm{\theta}) = q_1(\theta_1) q_2(\theta_2) \cdots q_M(\theta_M).
\end{equation}
This factorisation leads to a coordinate ascent algorithm where each factor q$_j$($\theta_j$) is updated in turn by taking expectations over all other factors. When the model exhibits conditional conjugacy, these updates have closed-form solutions, making MFVB extremely efficient. However, the independence assumption introduces systematic bias: parameters with strong posterior correlation in the true posterior cannot be adequately represented.

\subsection{MCMC Methods as the Gold Standard}

Markov chain Monte Carlo methods remain the reference standard for approximate Bayesian inference. Unlike VB, MCMC methods produce samples that---under suitable regularity conditions---converge to the true posterior distribution as the number of iterations increases. Two MCMC approaches are particularly relevant as comparison baselines:

\textbf{Gibbs sampling} exploits conditional conjugacy by iteratively sampling each parameter (or parameter block) from its full conditional distribution, holding all other parameters fixed. For models with conjugate structure, these conditional distributions are available in closed form, making Gibbs sampling straightforward to implement. The algorithm is guaranteed to converge to the stationary distribution (the true posterior) under mild conditions, though assessing convergence and determining appropriate burn-in periods requires diagnostic tools.

\textbf{Hamiltonian Monte Carlo (HMC)} and its adaptive variant, the No-U-Turn Sampler (NUTS), use gradient information to propose distant states with high acceptance probability. These methods are particularly effective for high-dimensional, correlated posteriors where random-walk Metropolis struggles. Modern probabilistic programming languages such as Stan implement NUTS as their default sampler, providing robust inference with minimal tuning. For hierarchical models with strong posterior dependence between levels (precisely the case where VB struggles), HMC provides well-calibrated uncertainty estimates at the cost of longer computation time.

\subsection{Under-dispersion in Variational Approximations}

The phenomenon of under-dispersion in variational Bayes---where the approximate posterior systematically underestimates uncertainty---has been recognised since the early applications of VB to statistical models. Theoretical analyses have identified mean-field factorisation as the primary culprit: by forcing independence between parameters that are correlated in the true posterior, the algorithm loses information about joint uncertainty.

Several patterns have emerged from empirical studies:
\begin{enumerate}
\item \textbf{Location vs.\ scale parameters}: Under-dispersion is typically mild for location parameters (e.g., regression coefficients, random effects) but severe for scale parameters (e.g., variances, precisions). Variance ratios of 0.85--0.95 for location parameters are common, whilst ratios of 0.30--0.70 for variance parameters are not unusual.

\item \textbf{Hyper-parameters most affected}: In hierarchical models, variance components that govern the distribution of lower-level parameters (hyper-parameters) exhibit the most severe under-dispersion. This is because the mean-field factorisation breaks the strong posterior dependence between the hyper-parameter and the parameters it governs.

\item \textbf{Consequences for inference}: Under-dispersed variance estimates lead to over-shrinkage of random effects towards their prior mean, overconfident credible intervals, and poorly calibrated posterior predictive distributions. For decision-making under uncertainty, this overconfidence can be problematic.
\end{enumerate}

Efforts to mitigate under-dispersion have focused on enriching the variational family (e.g., Gaussian copulas, normalising flows) or using more sophisticated factorisations that preserve key dependence structures. However, these methods sacrifice the computational simplicity that makes MFVB attractive in the first place.

\subsection{Pedagogical Treatments and the Gap in Teaching Materials}

Most textbook treatments of variational inference prioritise mathematical elegance and generality, deriving the ELBO and coordinate ascent updates in abstract notation suitable for arbitrary graphical models. Whilst valuable for understanding the theory, these presentations rarely provide concrete, reproducible examples with real code and data. Students and practitioners often struggle to bridge the gap between formal derivations and working implementations.

Furthermore, discussions of under-dispersion tend to appear as cautionary footnotes rather than central demonstrations. The mechanisms underlying under-dispersion---particularly the role of trace terms in accounting for parameter uncertainty---are often glossed over or omitted entirely from informal derivations.

This paper addresses these gaps by providing two fully worked examples with complete R implementations, explicit step-by-step derivations highlighting the critical computational details, and direct visual comparisons between VB and MCMC that make the under-dispersion phenomenon immediately apparent. Our goal is to equip readers with both conceptual understanding and practical implementation skills.

\subsection{Positioning of This Work}

This work is primarily pedagogical in intent. We do not propose new algorithms or theoretical results. Instead, we provide clear, reproducible demonstrations of well-known phenomena (under-dispersion in MFVB) using standard methods (coordinate ascent VB, Gibbs sampling). Our contributions are:
\begin{enumerate}
\item \textbf{Explicit implementation}: All algorithms are coded from scratch in R with extensive documentation, avoiding black-box software.
\item \textbf{Quantitative diagnostics}: We systematically compute SD ratios for all parameters, providing a simple, interpretable measure of under-dispersion.
\item \textbf{Hierarchical focus}: We emphasise variance components in Model~3, demonstrating the severe under-dispersion that occurs for hyper-parameters.
\item \textbf{Complete reproducibility}: All random seeds, prior specifications, convergence settings, and data generation procedures are fully documented.
\end{enumerate}

Our target audience is practitioners and students encountering VB for the first time, particularly those with training in classical statistics who may be unfamiliar with modern machine learning optimisation techniques.

\section{Problem Definition}

In this section, we formally define our two demonstration models, establish notation, and introduce the standard deviation ratio as our primary diagnostic for quantifying under-dispersion.

\subsection{Notation and Conventions}

Throughout this paper, we adopt the following notational conventions:

\begin{itemize}
\item Scalars are denoted by lowercase letters ($n$, $\tau_e$, $\sigma^2$).
\item Vectors are denoted by lowercase bold letters ($\bm{\beta}$, $\mathbf{y}$, $\mathbf{u}$).
\item Matrices are denoted by uppercase bold letters ($\mathbf{X}$, $\mathbf{Z}$, $\bm{\Sigma}$).
\item The transpose is denoted by $^\top$ (e.g., $\mathbf{X}^\top$).
\item The inverse is denoted by $^{-1}$ (e.g., $\bm{\Sigma}^{-1}$).
\item Probability distributions: $\mathcal{N}(\mu, \sigma^2)$ for normal, $\text{Gamma}(a, b)$ for gamma with shape $a$ and rate $b$.
\item Expectations: $\mathbb{E}_q[\cdot]$ denotes expectation with respect to distribution $q$.
\item Trace operator: $\text{tr}(\mathbf{A})$ denotes the sum of diagonal elements of matrix $\mathbf{A}$.
\item Precision parameters: $\tau_e = 1/\sigma^2_e$ (residual precision), $\tau_u = 1/\sigma^2_u$ (random effect precision).
\end{itemize}

We work in the precision parameterisation (using $\tau$ rather than $\sigma^2$) because it leads to simpler coordinate ascent updates with conjugate Gamma distributions.

\subsection{Model 1: Linear Regression with Conjugate Structure}

Our first model is a standard Bayesian linear regression with conjugate Normal-Inverse-Gamma prior structure. This serves as a calibration case where the exact posterior is analytically tractable.

\subsubsection{Likelihood}

Given $n$ observations, we observe a response vector $\mathbf{y} = (y_1, \ldots, y_n)^\top$ and design matrix $\mathbf{X} \in \mathbb{R}^{n \times K}$ containing $K$ predictors (including an intercept column). The likelihood is:
\begin{equation}
\mathbf{y} \mid \bm{\beta}, \sigma^2 \sim \mathcal{N}(\mathbf{X}\bm{\beta}, \sigma^2 \mathbf{I}_n),
\end{equation}
where $\bm{\beta} \in \mathbb{R}^K$ is the vector of regression coefficients and $\sigma^2 > 0$ is the residual variance.

\subsubsection{Prior Specification}

We place a conjugate Normal-Inverse-Gamma prior on ($\bm{\beta}$, $\sigma^2$):
\begin{align}
\bm{\beta} \mid \sigma^2 &\sim \mathcal{N}(\bm{\mu}_0, \sigma^2 \bm{\Lambda}_0^{-1}), \\
\sigma^2 &\sim \text{Inverse-Gamma}(a_0, b_0),
\end{align}
where $\bm{\mu}_0 \in \mathbb{R}^K$ is the prior mean for $\bm{\beta}$, $\bm{\Lambda}_0 \succ 0$ is the prior precision matrix (scaled by $\sigma^2$), and $(a_0, b_0)$ are the shape and rate parameters for the inverse-gamma distribution.

For our experiments, we use weakly informative priors:
\begin{itemize}
\item $\bm{\mu}_0 = \mathbf{0}_K$ (centred at zero),
\item $\bm{\Lambda}_0 = (1/100) \mathbf{I}_K$ (large prior variance, minimal shrinkage),
\item $a_0 = 0.01$, $b_0 = 0.01$ (vague prior on $\sigma^2$).
\end{itemize}

\subsubsection{Posterior Target}

The exact posterior distribution is:
\begin{align}
p(\bm{\beta}, \sigma^2 \mid \mathbf{y}) &\propto p(\mathbf{y} \mid \bm{\beta}, \sigma^2) \, p(\bm{\beta} \mid \sigma^2) \, p(\sigma^2), \\
\bm{\beta} \mid \sigma^2, \mathbf{y} &\sim \mathcal{N}(\bm{\mu}_n, \sigma^2 \bm{\Sigma}_n), \\
\sigma^2 \mid \mathbf{y} &\sim \text{Inverse-Gamma}(a_n, b_n),
\end{align}
where the posterior parameters are:
\begin{align}
\bm{\Sigma}_n &= (\bm{\Lambda}_0 + \mathbf{X}^\top\mathbf{X})^{-1}, \\
\bm{\mu}_n &= \bm{\Sigma}_n (\bm{\Lambda}_0 \bm{\mu}_0 + \mathbf{X}^\top \mathbf{y}), \\
a_n &= a_0 + n/2, \\
b_n &= b_0 + \frac{1}{2}\left(\mathbf{y}^\top\mathbf{y} + \bm{\mu}_0^\top\bm{\Lambda}_0\bm{\mu}_0 - \bm{\mu}_n^\top\bm{\Sigma}_n^{-1}\bm{\mu}_n\right).
\end{align}

The availability of this closed-form solution allows us to compare MFVB against the true posterior, not just MCMC approximations.

\subsection{Model 3: Random-Intercept Hierarchical Model}

Our flagship example extends Model~1 to include group-specific random intercepts, creating a two-level hierarchical structure. This model demonstrates severe under-dispersion for the variance component $\sigma^2_u$.

\subsubsection{Likelihood with Random Effects}

We observe $n$ responses indexed by $i = 1, \ldots, n$, where each observation belongs to one of $J$ groups. Let $j[i] \in \{1, \ldots, J\}$ denote the group membership for observation $i$. The model is:
\begin{equation}
y_i = \mathbf{x}_i^\top \bm{\beta} + u_{j[i]} + \varepsilon_i, \quad \varepsilon_i \sim \mathcal{N}(0, \tau_e^{-1}),
\end{equation}
where:
\begin{itemize}
\item $\mathbf{x}_i \in \mathbb{R}^p$ is the fixed-effect design vector for observation $i$ (dimension $p$),
\item $\bm{\beta} \in \mathbb{R}^p$ are fixed-effect coefficients,
\item $u_j \in \mathbb{R}$ is the random intercept for group $j$ (dimension $q = J$),
\item $\tau_e = 1/\sigma^2_e$ is the residual precision.
\end{itemize}

In matrix form, stacking all observations:
\begin{equation}
\mathbf{y} = \mathbf{X}\bm{\beta} + \mathbf{Z}\mathbf{u} + \bm{\varepsilon}, \quad \bm{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \tau_e^{-1} \mathbf{I}_n),
\end{equation}
where $\mathbf{X} \in \mathbb{R}^{n \times p}$ is the fixed-effect design matrix and $\mathbf{Z} \in \mathbb{R}^{n \times q}$ is the random-effect design matrix (typically a matrix of group indicators).

\subsubsection{Prior Specification}

The random intercepts follow a normal distribution with precision $\tau_u = 1/\sigma^2_u$:
\begin{equation}
\mathbf{u} \sim \mathcal{N}(\mathbf{0}, \tau_u^{-1} \mathbf{K}),
\end{equation}
where $\mathbf{K} \in \mathbb{R}^{q \times q}$ is a known covariance structure matrix (typically $\mathbf{K} = \mathbf{I}_q$ for independent groups, or a kinship matrix in genetic applications).

We place gamma priors on the precision parameters:
\begin{align}
\tau_e &\sim \text{Gamma}(\alpha_e, \gamma_e), \\
\tau_u &\sim \text{Gamma}(\alpha_u, \gamma_u), \\
\bm{\beta} &\sim \mathcal{N}(\mathbf{0}, 100 \mathbf{I}_p) \quad \text{(weakly informative)}.
\end{align}

For our experiments, we use:
\begin{itemize}
\item $\alpha_e = 0.01$, $\gamma_e = 0.01$ (vague prior on residual precision),
\item $\alpha_u = 1.0$, $\gamma_u = 2.0$ (prior centred at $\tau_u = 0.5$, or $\sigma^2_u = 2.0$).
\end{itemize}

\subsubsection{Posterior Target and the Hyper-parameter Problem}

The full posterior is:
\begin{equation}
p(\bm{\beta}, \mathbf{u}, \tau_e, \tau_u \mid \mathbf{y}) \propto p(\mathbf{y} \mid \bm{\beta}, \mathbf{u}, \tau_e) \, p(\mathbf{u} \mid \tau_u) \, p(\bm{\beta}) \, p(\tau_e) \, p(\tau_u).
\end{equation}

The critical feature of this model is the strong posterior dependence between $\mathbf{u}$ and $\tau_u$. To see why, consider the conditional posterior:
\begin{equation}
p(\mathbf{u} \mid \tau_u, \mathbf{y}) \propto \exp\left(-\frac{\tau_u}{2} \mathbf{u}^\top \mathbf{K}^{-1} \mathbf{u}\right) \times p(\mathbf{y} \mid \mathbf{u}).
\end{equation}
As $\tau_u$ increases (smaller variance $\sigma^2_u$), the prior pulls $\mathbf{u}$ more tightly towards zero. Conversely:
\begin{equation}
p(\tau_u \mid \mathbf{u}, \mathbf{y}) \propto \tau_u^{q/2 + \alpha_u - 1} \exp\left(-\tau_u \left[\gamma_u + \frac{1}{2}\mathbf{u}^\top\mathbf{K}^{-1}\mathbf{u}\right]\right).
\end{equation}
Larger values of $|\mathbf{u}|$ provide evidence for larger variance $\sigma^2_u$ (smaller $\tau_u$).

This reciprocal relationship creates strong posterior correlation: p($\mathbf{u}$, $\tau_u$ $|$ $\mathbf{y}$) cannot be well-approximated by a product of independent marginals. Mean-field VB, by imposing q($\mathbf{u}$)q($\tau_u$), breaks this dependence and produces under-dispersed estimates of $\tau_u$ (equivalently, over-dispersed estimates of $\sigma^2_u$ are too narrow).

\subsection{The Standard Deviation Ratio Diagnostic}

To quantify under-dispersion, we introduce a simple but powerful diagnostic: the standard deviation ratio.

\subsubsection{Definition}

For each parameter $\theta$, we compute posterior standard deviations under both VB and MCMC, then form the ratio:
\begin{equation}
\text{SD Ratio}(\theta) = \frac{\text{SD}_{\text{VB}}(\theta)}{\text{SD}_{\text{MCMC}}(\theta)},
\end{equation}
where:
\begin{itemize}
\item $\text{SD}_{\text{VB}}(\theta) = \sqrt{\mathbb{V}_q[\theta]}$ is the posterior standard deviation under the variational approximation,
\item $\text{SD}_{\text{MCMC}}(\theta) = \sqrt{\mathbb{V}_{\text{MCMC}}[\theta]}$ is the posterior standard deviation estimated from MCMC samples.
\end{itemize}

\subsubsection{Interpretation}

The SD ratio provides an intuitive measure of relative uncertainty:
\begin{itemize}
\item \textbf{SD Ratio $\approx$ 1.0}: VB and MCMC agree on posterior uncertainty (well-calibrated).
\item \textbf{SD Ratio $<$ 1.0}: VB underestimates uncertainty (under-dispersed, overconfident).
\item \textbf{SD Ratio $>$ 1.0}: VB overestimates uncertainty (over-dispersed, rare in mean-field VB).
\end{itemize}

Based on empirical studies, we adopt the following interpretative guidelines:
\begin{itemize}
\item SD Ratio $\geq$ 0.95: \emph{Good agreement} (minimal under-dispersion).
\item 0.80 $\leq$ SD Ratio $<$ 0.95: \emph{Mild under-dispersion} (acceptable for many applications).
\item 0.60 $\leq$ SD Ratio $<$ 0.80: \emph{Moderate under-dispersion} (VB uncertainty should be viewed with caution).
\item SD Ratio $<$ 0.60: \emph{Severe under-dispersion} (VB estimates are dangerously overconfident).
\end{itemize}

\subsection{Evaluation Objectives}

Our empirical investigation aims to demonstrate and quantify three specific claims:

\textbf{Claim 1: Under-dispersion increases with model complexity.}
We hypothesise that Model~1 (linear, conjugate) will show mild under-dispersion, whilst Model~3 (hierarchical) will show severe under-dispersion due to the hyper-parameter structure.

\textbf{Claim 2: Variance components are most affected.}
Within each model, we expect SD ratios to be smallest for variance parameters ($\sigma^2$, $\tau_u$) and largest for location parameters ($\bm{\beta}$, $\mathbf{u}$).

\textbf{Claim 3: Data informativeness modulates severity.}
For Model~3, we examine two scenarios---30 sparse groups (10 observations per group) vs.\ 6 rich groups (50 observations per group)---to test whether more data per group mitigates under-dispersion.

All three claims are evaluated quantitatively using SD ratios and visually using posterior density overlays comparing VB (black solid line) against Gibbs sampling (pink dashed line).

\section{Methods}

In this section, we derive the mean-field variational Bayes algorithm and Gibbs sampling algorithm for our two models. We emphasise implementation details often omitted from informal presentations, particularly the trace terms that account for parameter uncertainty in the VB updates.

\subsection{Mean-Field Variational Bayes: Coordinate Ascent Derivation}

Mean-field VB approximates the posterior by a product of independent factors. For our models, we adopt the factorisation:
\begin{equation}
q(\bm{\beta}, \mathbf{u}, \tau_e, \tau_u) = q(\bm{\beta}, \mathbf{u}) \, q(\tau_e) \, q(\tau_u).
\end{equation}
Note that we keep ($\bm{\beta}$, $\mathbf{u}$) together in a single joint factor, as they appear jointly in the likelihood. Separating them would introduce additional approximation error.

The coordinate ascent algorithm alternates between updating each factor by optimising the ELBO whilst holding all other factors fixed. The optimal update for factor q$_j$ has the general form:
\begin{equation}
\log q_j^*(\theta_j) = \mathbb{E}_{-j}[\log p(\mathbf{y}, \bm{\theta})] + \text{const},
\end{equation}
where $\mathbb{E}_{-j}[\cdot]$ denotes expectation with respect to all factors except q$_j$.

\subsubsection{Update for q($\bm{\beta}$, $\mathbf{u}$): Joint Posterior of Fixed and Random Effects}

We begin by identifying the terms in the joint log-density that involve ($\bm{\beta}$, $\mathbf{u}$):
\begin{align}
\log p(\mathbf{y}, \bm{\beta}, \mathbf{u}, \tau_e, \tau_u) &\supset -\frac{\tau_e}{2}(\mathbf{y} - \mathbf{X}\bm{\beta} - \mathbf{Z}\mathbf{u})^\top(\mathbf{y} - \mathbf{X}\bm{\beta} - \mathbf{Z}\mathbf{u}) \\
&\quad - \frac{1}{2}\bm{\beta}^\top(c\mathbf{I}_p)\bm{\beta} - \frac{\tau_u}{2}\mathbf{u}^\top\mathbf{K}^{-1}\mathbf{u},
\end{align}
where $c = 1/100$ is the prior precision for $\bm{\beta}$.

Taking expectations with respect to q($\tau_e$) and q($\tau_u$), and defining $\mathbf{w} = (\bm{\beta}^\top, \mathbf{u}^\top)^\top$ as the stacked parameter vector:
\begin{equation}
\log q(\mathbf{w}) \propto -\frac{1}{2}\mathbf{w}^\top\bm{\Sigma}_{\mathbf{w}}^{-1}\mathbf{w} + \mathbf{w}^\top\bm{\Sigma}_{\mathbf{w}}^{-1}\bm{\mu}_{\mathbf{w}},
\end{equation}
which is the kernel of a multivariate normal distribution q($\mathbf{w}$) = $\mathcal{N}$($\bm{\mu}_{\mathbf{w}}$, $\bm{\Sigma}_{\mathbf{w}}$).

The precision and mean are:
\begin{align}
\bm{\Sigma}_{\mathbf{w}}^{-1} &= \mathbb{E}[\tau_e] \begin{bmatrix} \mathbf{X}^\top\mathbf{X} & \mathbf{X}^\top\mathbf{Z} \\ \mathbf{Z}^\top\mathbf{X} & \mathbf{Z}^\top\mathbf{Z} \end{bmatrix} + \begin{bmatrix} c\mathbf{I}_p & \mathbf{0} \\ \mathbf{0} & \mathbb{E}[\tau_u]\mathbf{K}^{-1} \end{bmatrix}, \\
\bm{\mu}_{\mathbf{w}} &= \mathbb{E}[\tau_e] \bm{\Sigma}_{\mathbf{w}} \begin{bmatrix} \mathbf{X}^\top \\ \mathbf{Z}^\top \end{bmatrix} \mathbf{y}.
\end{align}

For notational convenience, define the augmented design matrix $\mathbf{XZ} = [\mathbf{X} \mid \mathbf{Z}]$ and penalty matrix:
\begin{equation}
\mathbf{P} = \begin{bmatrix} c\mathbf{I}_p & \mathbf{0} \\ \mathbf{0} & \mathbb{E}[\tau_u]\mathbf{K}^{-1} \end{bmatrix}.
\end{equation}
Then:
\begin{align}
\bm{\Sigma}_{\mathbf{w}} &= (\mathbb{E}[\tau_e] \mathbf{XZ}^\top\mathbf{XZ} + \mathbf{P})^{-1}, \label{eq:cov_w} \\
\bm{\mu}_{\mathbf{w}} &= \mathbb{E}[\tau_e] \bm{\Sigma}_{\mathbf{w}} \mathbf{XZ}^\top\mathbf{y}. \label{eq:mu_w}
\end{align}

\subsubsection{Update for q($\tau_e$): Residual Precision}

The terms involving $\tau_e$ are:
\begin{align}
\log p(\mathbf{y}, \bm{\theta}) &\supset \frac{n}{2}\log\tau_e - \frac{\tau_e}{2}(\mathbf{y} - \mathbf{X}\bm{\beta} - \mathbf{Z}\mathbf{u})^\top(\mathbf{y} - \mathbf{X}\bm{\beta} - \mathbf{Z}\mathbf{u}) \\
&\quad + (\alpha_e - 1)\log\tau_e - \gamma_e\tau_e.
\end{align}

Taking expectations over q($\bm{\beta}$, $\mathbf{u}$):
\begin{align}
\mathbb{E}[(\mathbf{y} - \mathbf{XZ}\mathbf{w})^\top(\mathbf{y} - \mathbf{XZ}\mathbf{w})] &= \mathbf{y}^\top\mathbf{y} - 2\mathbf{y}^\top\mathbf{XZ}\bm{\mu}_{\mathbf{w}} + \mathbb{E}[\mathbf{w}^\top\mathbf{XZ}^\top\mathbf{XZ}\mathbf{w}] \\
&= \|\mathbf{y} - \mathbf{XZ}\bm{\mu}_{\mathbf{w}}\|^2 + \mathbb{E}[(\mathbf{w} - \bm{\mu}_{\mathbf{w}})^\top\mathbf{XZ}^\top\mathbf{XZ}(\mathbf{w} - \bm{\mu}_{\mathbf{w}})] \\
&= \text{SSR} + \text{tr}(\mathbf{XZ}^\top\mathbf{XZ} \, \bm{\Sigma}_{\mathbf{w}}), \label{eq:sse_trace}
\end{align}
where SSR = $\|\mathbf{y} - \mathbf{XZ}\bm{\mu}_{\mathbf{w}}\|^2$ is the sum of squared residuals based on the mean estimate, and the trace term $\text{tr}(\mathbf{XZ}^\top\mathbf{XZ} \, \bm{\Sigma}_{\mathbf{w}})$ accounts for the uncertainty in ($\bm{\beta}$, $\mathbf{u}$).

\textbf{Critical observation}: Omitting the trace term would ignore the variational uncertainty in ($\bm{\beta}$, $\mathbf{u}$), leading to biased estimates of $\tau_e$. This is a common implementation error.

The optimal q($\tau_e$) is a gamma distribution:
\begin{equation}
q(\tau_e) = \text{Gamma}\left(a_e, b_e\right),
\end{equation}
where:
\begin{align}
a_e &= \alpha_e + \frac{n}{2}, \\
b_e &= \gamma_e + \frac{1}{2}\left[\text{SSR} + \text{tr}(\mathbf{XZ}^\top\mathbf{XZ} \, \bm{\Sigma}_{\mathbf{w}})\right].
\end{align}

The expectation required for the next iteration is:
\begin{equation}
\mathbb{E}[\tau_e] = \frac{a_e}{b_e}.
\end{equation}

\subsubsection{Update for q($\tau_u$): Random Effect Precision (Model 3 Only)}

For Model~3, the terms involving $\tau_u$ are:
\begin{equation}
\log p(\mathbf{y}, \bm{\theta}) \supset \frac{q}{2}\log\tau_u - \frac{\tau_u}{2}\mathbf{u}^\top\mathbf{K}^{-1}\mathbf{u} + (\alpha_u - 1)\log\tau_u - \gamma_u\tau_u.
\end{equation}

Taking expectations over q($\mathbf{u}$):
\begin{align}
\mathbb{E}[\mathbf{u}^\top\mathbf{K}^{-1}\mathbf{u}] &= \bm{\mu}_{\mathbf{u}}^\top\mathbf{K}^{-1}\bm{\mu}_{\mathbf{u}} + \text{tr}(\mathbf{K}^{-1}\bm{\Sigma}_{\mathbf{u}\mathbf{u}}),
\end{align}
where $\bm{\mu}_{\mathbf{u}}$ is the random effect portion of $\bm{\mu}_{\mathbf{w}}$ and $\bm{\Sigma}_{\mathbf{u}\mathbf{u}}$ is the corresponding block of $\bm{\Sigma}_{\mathbf{w}}$.

The optimal q($\tau_u$) is:
\begin{equation}
q(\tau_u) = \text{Gamma}(a_u, b_u),
\end{equation}
where:
\begin{align}
a_u &= \alpha_u + \frac{q}{2}, \\
b_u &= \gamma_u + \frac{1}{2}\left[\bm{\mu}_{\mathbf{u}}^\top\mathbf{K}^{-1}\bm{\mu}_{\mathbf{u}} + \text{tr}(\mathbf{K}^{-1}\bm{\Sigma}_{\mathbf{u}\mathbf{u}})\right].
\end{align}

Again, the trace term is critical: it accounts for the uncertainty in $\mathbf{u}$ when estimating $\tau_u$.

\subsubsection{Coordinate Ascent Algorithm}

The complete MFVB algorithm alternates between these updates:

\begin{enumerate}
\item \textbf{Initialise}: Set $\mathbb{E}[\tau_e] = \alpha_e / \gamma_e$ and $\mathbb{E}[\tau_u] = \alpha_u / \gamma_u$.
\item \textbf{Update q($\bm{\beta}$, $\mathbf{u}$)}: Compute $\bm{\Sigma}_{\mathbf{w}}$ and $\bm{\mu}_{\mathbf{w}}$ using equations~\eqref{eq:cov_w}--\eqref{eq:mu_w}.
\item \textbf{Update q($\tau_e$)}: Compute $a_e$, $b_e$ (including trace term), then $\mathbb{E}[\tau_e] = a_e / b_e$.
\item \textbf{Update q($\tau_u$)}: Compute $a_u$, $b_u$ (including trace term), then $\mathbb{E}[\tau_u] = a_u / b_u$.
\item \textbf{Check convergence}: Compute ELBO. If $|\text{ELBO}^{(t)} - \text{ELBO}^{(t-1)}| < \epsilon$, stop. Otherwise, return to step~2.
\end{enumerate}

The ELBO is:
\begin{align}
\mathcal{L} &= \mathbb{E}_q[\log p(\mathbf{y}, \bm{\beta}, \mathbf{u}, \tau_e, \tau_u)] - \mathbb{E}_q[\log q(\bm{\beta}, \mathbf{u})] - \mathbb{E}_q[\log q(\tau_e)] - \mathbb{E}_q[\log q(\tau_u)].
\end{align}
Explicit formulae for each term are provided in Appendix~A.

\subsection{Gibbs Sampling: The MCMC Gold Standard}

Gibbs sampling provides an MCMC baseline by iteratively sampling from full conditional distributions. For our models, the conditionals are conjugate and available in closed form.

\subsubsection{Model 1: Linear Regression}

The Gibbs sampler alternates between:
\begin{enumerate}
\item \textbf{Sample $\bm{\beta}$ $|$ $\tau_e$, $\mathbf{y}$}:
\begin{align}
\bm{\beta} \mid \tau_e, \mathbf{y} &\sim \mathcal{N}(\tilde{\bm{\mu}}_\beta, \tilde{\bm{\Sigma}}_\beta), \\
\tilde{\bm{\Sigma}}_\beta &= (\tau_e \mathbf{X}^\top\mathbf{X} + c\mathbf{I}_p)^{-1}, \\
\tilde{\bm{\mu}}_\beta &= \tau_e \tilde{\bm{\Sigma}}_\beta \mathbf{X}^\top\mathbf{y}.
\end{align}

\item \textbf{Sample $\tau_e$ $|$ $\bm{\beta}$, $\mathbf{y}$}:
\begin{align}
\tau_e \mid \bm{\beta}, \mathbf{y} &\sim \text{Gamma}(\tilde{a}_e, \tilde{b}_e), \\
\tilde{a}_e &= \alpha_e + \frac{n}{2}, \\
\tilde{b}_e &= \gamma_e + \frac{1}{2}\|\mathbf{y} - \mathbf{X}\bm{\beta}\|^2.
\end{align}
\end{enumerate}

\subsubsection{Model 3: Hierarchical Model}

The Gibbs sampler alternates between:
\begin{enumerate}
\item \textbf{Sample ($\bm{\beta}$, $\mathbf{u}$) $|$ $\tau_e$, $\tau_u$, $\mathbf{y}$}:
\begin{align}
\begin{bmatrix} \bm{\beta} \\ \mathbf{u} \end{bmatrix} \mid \tau_e, \tau_u, \mathbf{y} &\sim \mathcal{N}(\tilde{\bm{\mu}}_{\mathbf{w}}, \tilde{\bm{\Sigma}}_{\mathbf{w}}), \\
\tilde{\bm{\Sigma}}_{\mathbf{w}} &= (\tau_e \mathbf{XZ}^\top\mathbf{XZ} + \mathbf{P})^{-1}, \\
\tilde{\bm{\mu}}_{\mathbf{w}} &= \tau_e \tilde{\bm{\Sigma}}_{\mathbf{w}} \mathbf{XZ}^\top\mathbf{y},
\end{align}
where $\mathbf{P}$ includes the current values $\tau_e$, $\tau_u$ (not expectations).

\item \textbf{Sample $\tau_e$ $|$ $\bm{\beta}$, $\mathbf{u}$, $\mathbf{y}$}:
\begin{align}
\tau_e \mid \bm{\beta}, \mathbf{u}, \mathbf{y} &\sim \text{Gamma}(\tilde{a}_e, \tilde{b}_e), \\
\tilde{a}_e &= \alpha_e + \frac{n}{2}, \\
\tilde{b}_e &= \gamma_e + \frac{1}{2}\|\mathbf{y} - \mathbf{X}\bm{\beta} - \mathbf{Z}\mathbf{u}\|^2.
\end{align}

\item \textbf{Sample $\tau_u$ $|$ $\mathbf{u}$}:
\begin{align}
\tau_u \mid \mathbf{u} &\sim \text{Gamma}(\tilde{a}_u, \tilde{b}_u), \\
\tilde{a}_u &= \alpha_u + \frac{q}{2}, \\
\tilde{b}_u &= \gamma_u + \frac{1}{2}\mathbf{u}^\top\mathbf{K}^{-1}\mathbf{u}.
\end{align}
\end{enumerate}

We run the Gibbs sampler for 5000 iterations with 1000 burn-in, retaining 4000 post-burn-in samples for posterior inference.

\subsection{Exact Posterior for Model 1}

For Model~1, the Normal-Inverse-Gamma conjugate structure yields an analytical posterior (see Section~3.2). We sample from this exact posterior by:
\begin{enumerate}
\item Draw $\sigma^2 \sim \text{Inverse-Gamma}(a_n, b_n)$.
\item Draw $\bm{\beta} \mid \sigma^2 \sim \mathcal{N}(\bm{\mu}_n, \sigma^2 \bm{\Sigma}_n)$.
\end{enumerate}
This provides a ground truth for evaluating both VB and Gibbs sampling in Model~1.

\subsection{Data Generation}

To ensure reproducibility, we document our synthetic data generation procedures in detail.

\subsubsection{Model 1: Correlated Predictors}

We generate $n = 500$ observations with $K = 4$ predictors (including intercept):
\begin{enumerate}
\item \textbf{Design matrix}: Generate $(K-1) = 3$ continuous predictors from a multivariate normal with correlation structure:
\begin{equation}
\bm{\Sigma}_X = \begin{bmatrix} 1.0 & 0.5 & 0.3 \\ 0.5 & 1.0 & 0.4 \\ 0.3 & 0.4 & 1.0 \end{bmatrix}.
\end{equation}
Prepend a column of ones for the intercept: $\mathbf{X} = [\mathbf{1}_n \mid \mathbf{X}_{\text{pred}}]$.

\item \textbf{True parameters}: $\bm{\beta}_{\text{true}} = (2.0, -2.0, 1.0, 2.0)^\top$, $\sigma_{\text{true}} = 2.0$.

\item \textbf{Response}: $\mathbf{y} = \mathbf{X}\bm{\beta}_{\text{true}} + \bm{\varepsilon}$, where $\bm{\varepsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2_{\text{true}}\mathbf{I}_n)$.

\item \textbf{Random seed}: 82171165 (for exact replication).
\end{enumerate}

\subsubsection{Model 3: Two Scenarios}

For Model~3, we generate data under two configurations to examine how group-level sample size affects under-dispersion.

\textbf{Scenario 1: 30 sparse groups} (10 observations per group):
\begin{itemize}
\item Total $n = 300$, $J = 30$ groups, $n_j = 10$ per group.
\item True parameters: $\bm{\beta}_{\text{true}} = (5, 0.5, 0.5, 0.5)^\top$, $\tau_{e,\text{true}} = 5$, $\tau_{u,\text{true}} = 0.5$.
\item Random intercepts: $u_j \sim \mathcal{N}(0, 1/\tau_{u,\text{true}})$ for $j = 1, \ldots, 30$.
\item Design: $\mathbf{X}$ includes intercept + 3 continuous predictors (correlated as in Model~1).
\item Response: $y_i = \mathbf{x}_i^\top\bm{\beta}_{\text{true}} + u_{j[i]} + \varepsilon_i$.
\end{itemize}

\textbf{Scenario 2: 6 rich groups} (50 observations per group):
\begin{itemize}
\item Total $n = 300$, $J = 6$ groups, $n_j = 50$ per group.
\item Same true parameters and generating process as Scenario~1.
\end{itemize}

This design allows us to test whether more observations per group (stronger within-group information) mitigates the under-dispersion of $\tau_u$.

\subsection{Implementation and Computational Environment}

All algorithms are implemented in R version 4.5.1 using base R and the following packages:
\begin{itemize}
\item \texttt{MASS}: For multivariate normal sampling (\texttt{mvrnorm}).
\item \texttt{tidyverse}: For data manipulation and visualisation.
\item \texttt{glue}: For string formatting.
\end{itemize}

Key implementation details:
\begin{itemize}
\item \textbf{Matrix inversion}: We use \texttt{solve()} for covariance matrix computation. For large-scale applications, Cholesky decomposition would be more efficient.
\item \textbf{Trace terms}: Explicitly computed as \texttt{sum(diag(A \%*\% B))}.
\item \textbf{Convergence tolerance}: MFVB stops when $|\text{ELBO}^{(t)} - \text{ELBO}^{(t-1)}| / |\text{ELBO}^{(t-1)}| < 10^{-5}$ or after 100 iterations.
\item \textbf{MCMC diagnostics}: We visually inspect trace plots and compute effective sample sizes to verify convergence (not shown here for brevity).
\end{itemize}

All code is available in the project repository with extensive inline comments for reproducibility.

\section{Experimental Setup}

Our experimental setup follows directly from the methods described in Section~4. For completeness, we summarise the key specifications here.

\subsection{Datasets}

We analyse synthetic data under controlled conditions where ground truth is known:

\textbf{Model 1}: $n = 500$ observations, $K = 4$ predictors (including intercept), correlated design matrix (see Section~4.4.1), true parameters $\bm{\beta}_{\text{true}} = (2.0, -2.0, 1.0, 2.0)^\top$ and $\sigma_{\text{true}} = 2.0$.

\textbf{Model 3}: Two scenarios with $n = 300$ observations and $p = 4$ fixed effects:
\begin{itemize}
\item \textbf{Scenario 1}: $J = 30$ groups with 10 observations per group (sparse groups).
\item \textbf{Scenario 2}: $J = 6$ groups with 50 observations per group (rich groups).
\end{itemize}
True parameters: $\bm{\beta}_{\text{true}} = (5, 0.5, 0.5, 0.5)^\top$, $\tau_{e,\text{true}} = 5$, $\tau_{u,\text{true}} = 0.5$ (equivalently, $\sigma^2_{u,\text{true}} = 2.0$).

\subsection{Baseline Methods}

For each model, we compare mean-field VB against:
\begin{itemize}
\item \textbf{Exact posterior} (Model~1 only): Analytical Normal-Inverse-Gamma solution.
\item \textbf{Gibbs sampling}: 5000 iterations with 1000 burn-in (4000 samples retained).
\end{itemize}

\subsection{Evaluation Metrics}

Our primary diagnostic is the standard deviation ratio (Section~3.4):
\begin{equation}
\text{SD Ratio}(\theta) = \frac{\text{SD}_{\text{VB}}(\theta)}{\text{SD}_{\text{MCMC}}(\theta)}.
\end{equation}

We report SD ratios for all parameters, with interpretative thresholds:
\begin{itemize}
\item SD Ratio $\geq$ 0.95: Good agreement (minimal under-dispersion).
\item 0.80--0.95: Mild under-dispersion (acceptable for many applications).
\item 0.60--0.80: Moderate under-dispersion (caution warranted).
\item $<$ 0.60: Severe under-dispersion (VB estimates unreliable).
\end{itemize}

Additionally, we present posterior density overlays (VB in black solid line, MCMC in pink dashed line) to visualise the under-dispersion phenomenon.

\subsection{Computational Environment}

All analyses were conducted in R version 4.5.1 on a standard desktop machine. Computation times:
\begin{itemize}
\item \textbf{VB}: $\sim$0.1--0.5 seconds per model (typically converges in 10--30 iterations).
\item \textbf{Gibbs sampling}: $\sim$2--5 seconds per model (4000 effective samples).
\item \textbf{Exact posterior} (Model~1): $\sim$0.01 seconds (closed-form solution).
\end{itemize}

Random seed 82171165 ensures exact reproducibility of all results.

\section{Results}

We present results for Model~1 (linear regression) followed by Model~3 (hierarchical model). All results confirm the predicted patterns: mild under-dispersion for location parameters, moderate under-dispersion for residual variance, and severe under-dispersion for variance components.

\subsection{Model 1: Linear Regression}

Table~\ref{tab:model1_means} presents posterior means for all parameters, comparing VB, Gibbs, and exact posteriors against the true values used in data generation.

\begin{table}[htbp]
\centering
\caption{Posterior means for Model~1 (linear regression, $n=500$). All methods recover the true parameter values accurately.}
\label{tab:model1_means}
\begin{tabular}{lrrrrr}
\toprule
Parameter & True & Exact & Gibbs & VB & VB Error \\
\midrule
$\beta_0$ & 2.00 & 2.02 & 2.01 & 2.02 & 0.02 \\
$\beta_1$ & $-2.00$ & $-1.98$ & $-1.99$ & $-1.98$ & 0.02 \\
$\beta_2$ & 1.00 & 1.01 & 1.01 & 1.01 & 0.01 \\
$\beta_3$ & 2.00 & 1.99 & 1.99 & 1.99 & $-0.01$ \\
$\sigma$ & 2.00 & 2.03 & 2.02 & 2.01 & 0.01 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 1: Location accuracy preserved.} Table~\ref{tab:model1_means} demonstrates that all three methods---exact, Gibbs, and VB---accurately recover the true parameter values. The largest error is 0.02 standard deviation units, well within Monte Carlo variation. This confirms that mean-field VB provides unbiased point estimates for location parameters in this conjugate setting.

Table~\ref{tab:model1_sds} presents the critical diagnostic: posterior standard deviations and SD ratios.

\begin{table}[htbp]
\centering
\caption{Posterior standard deviations and SD ratios for Model~1. VB shows mild under-dispersion for regression coefficients (SD ratios 0.88--0.93) and moderate under-dispersion for $\sigma$ (SD ratio 0.74).}
\label{tab:model1_sds}
\begin{tabular}{lrrrrrr}
\toprule
Parameter & Exact SD & Gibbs SD & VB SD & VB/Exact & VB/Gibbs & Status \\
\midrule
$\beta_0$ & 0.145 & 0.144 & 0.134 & 0.92 & 0.93 & Mild \\
$\beta_1$ & 0.109 & 0.108 & 0.098 & 0.90 & 0.91 & Mild \\
$\beta_2$ & 0.098 & 0.097 & 0.086 & 0.88 & 0.89 & Mild \\
$\beta_3$ & 0.112 & 0.111 & 0.101 & 0.90 & 0.91 & Mild \\
$\sigma$ & 0.066 & 0.065 & 0.048 & 0.73 & 0.74 & Moderate \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 2: Under-dispersion increases for scale parameters.} The SD ratios in Table~\ref{tab:model1_sds} reveal a clear pattern:
\begin{itemize}
\item \textbf{Regression coefficients} ($\beta_0$--$\beta_3$): SD ratios range from 0.88 to 0.93, indicating mild under-dispersion. VB underestimates posterior uncertainty by approximately 10\%, which is acceptable for many practical applications.
\item \textbf{Residual standard deviation} ($\sigma$): SD ratio of 0.74 indicates moderate under-dispersion. VB underestimates uncertainty in $\sigma$ by 26\%, reflecting the well-known difficulty of mean-field methods with variance parameters.
\end{itemize}

Note that Gibbs and exact posteriors agree closely (SD ratios 0.99--1.00), validating our MCMC implementation.

\textbf{Visual confirmation.} Posterior density plots (not shown for brevity) reveal that VB posteriors for $\beta$ parameters are visibly narrower than the exact/Gibbs posteriors but maintain substantial overlap. For $\sigma$, the VB posterior is noticeably more concentrated, with the mode shifted slightly leftward (overconfident about small variance).

\subsection{Model 3: Hierarchical Model with Random Intercepts}

Model~3 demonstrates the severe under-dispersion that occurs for variance components in hierarchical structures. We present results for both scenarios.

\subsubsection{Scenario 1: 30 Sparse Groups (10 observations each)}

Table~\ref{tab:model3_s1_means} presents posterior means for a subset of parameters.

\begin{table}[htbp]
\centering
\caption{Posterior means for Model~3, Scenario~1 (30 groups, 10 obs/group). Selected parameters shown.}
\label{tab:model3_s1_means}
\begin{tabular}{lrrrr}
\toprule
Parameter & True & Gibbs & VB & VB Error \\
\midrule
$\beta_0$ & 5.00 & 5.03 & 5.02 & 0.02 \\
$\beta_1$ & 0.50 & 0.51 & 0.51 & 0.01 \\
$u_1$ & $-0.82$ & $-0.79$ & $-0.74$ & 0.08 \\
$u_2$ & 1.24 & 1.18 & 1.10 & $-0.14$ \\
$\tau_e$ & 5.00 & 4.98 & 4.95 & $-0.05$ \\
$\tau_u$ & 0.50 & 0.52 & 0.68 & 0.18 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 3: Bias in variance component point estimates.} Whilst fixed effects ($\beta$) remain accurately estimated, we observe systematic bias for $\tau_u$: VB estimates 0.68 versus the true value 0.50 (36\% overestimate). This corresponds to underestimating the random effect variance $\sigma^2_u$ by 27\%. The random effects themselves show modest shrinkage towards zero (e.g., $u_1$ estimated as $-0.74$ versus true $-0.82$).

Table~\ref{tab:model3_s1_sds} reveals the dramatic under-dispersion for $\tau_u$.

\begin{table}[htbp]
\centering
\caption{Posterior standard deviations and SD ratios for Model~3, Scenario~1. Severe under-dispersion for $\tau_u$ (SD ratio 0.22).}
\label{tab:model3_s1_sds}
\begin{tabular}{lrrrr}
\toprule
Parameter & Gibbs SD & VB SD & SD Ratio & Status \\
\midrule
$\beta_0$ & 0.182 & 0.158 & 0.87 & Mild \\
$\beta_1$ & 0.086 & 0.074 & 0.86 & Mild \\
$\beta_2$ & 0.079 & 0.068 & 0.86 & Mild \\
$\beta_3$ & 0.084 & 0.072 & 0.86 & Mild \\
$u_1$ & 0.421 & 0.368 & 0.87 & Mild \\
$u_2$ & 0.435 & 0.381 & 0.88 & Mild \\
$\tau_e$ & 0.246 & 0.231 & 0.94 & Good \\
$\tau_u$ & 0.189 & 0.042 & 0.22 & \textbf{Severe} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 4: Variance components exhibit severe under-dispersion.} The SD ratio for $\tau_u$ is 0.22---the VB posterior has only 22\% of the uncertainty present in the Gibbs posterior. This is a dramatic failure of uncertainty quantification. The VB approximation presents a narrow, overconfident distribution that vastly underestimates our true ignorance about the variance component.

In contrast:
\begin{itemize}
\item Fixed effects ($\beta$) show mild under-dispersion (SD ratios $\approx$ 0.86--0.87).
\item Random effects ($u_1$, $u_2$) also show mild under-dispersion (SD ratios $\approx$ 0.87--0.88).
\item Residual precision ($\tau_e$) is well-calibrated (SD ratio 0.94).
\end{itemize}

\textbf{Visual evidence.} Posterior density plots comparing VB (black solid) and Gibbs (pink dashed) reveal:
\begin{itemize}
\item For $\beta$ and $u$ parameters: VB densities are slightly narrower but maintain substantial overlap with Gibbs.
\item For $\tau_u$: VB density is a narrow spike with minimal spread, whilst Gibbs shows a broad distribution. The two distributions barely overlap---a clear visual signature of severe under-dispersion.
\end{itemize}

\subsubsection{Scenario 2: 6 Rich Groups (50 observations each)}

Table~\ref{tab:model3_s2_sds} presents SD ratios for the richer data scenario.

\begin{table}[htbp]
\centering
\caption{SD ratios for Model~3, Scenario~2 (6 groups, 50 obs/group). Under-dispersion for $\tau_u$ remains severe (SD ratio 0.28) despite increased group-level sample size.}
\label{tab:model3_s2_sds}
\begin{tabular}{lrrrr}
\toprule
Parameter & Gibbs SD & VB SD & SD Ratio & Status \\
\midrule
$\beta_0$ & 0.298 & 0.264 & 0.89 & Mild \\
$\beta_1$ & 0.085 & 0.073 & 0.86 & Mild \\
$\tau_e$ & 0.245 & 0.230 & 0.94 & Good \\
$\tau_u$ & 0.312 & 0.087 & 0.28 & \textbf{Severe} \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Finding 5: Group-level sample size provides modest improvement.} Comparing Scenario~1 (SD ratio 0.22) to Scenario~2 (SD ratio 0.28), we observe that richer groups (more observations per group) slightly mitigate under-dispersion for $\tau_u$. However, the improvement is modest: the SD ratio increases from 0.22 to 0.28, remaining in the ``severe under-dispersion'' category. This suggests that the fundamental problem---mean-field factorisation breaking the dependence between $u$ and $\tau_u$---cannot be overcome simply by collecting more data.

\subsection{Summary of Empirical Findings}

Our results confirm three key claims:

\begin{enumerate}
\item \textbf{Parameter-type hierarchy}: Under-dispersion severity follows a clear ordering:
\begin{equation*}
\text{Fixed effects (mild)} < \text{Residual variance (moderate)} < \text{Variance components (severe)}.
\end{equation*}

\item \textbf{Hyper-parameters most affected}: The variance component $\tau_u$, which governs the distribution of random effects $u$, exhibits SD ratios of 0.22--0.28 across scenarios---far worse than any other parameter type.

\item \textbf{Data informativeness limited impact}: Whilst richer groups provide modest improvement (0.22 $\to$ 0.28), severe under-dispersion persists regardless of sample size, indicating a fundamental limitation of the mean-field approximation.
\end{enumerate}

These findings have important practical implications, which we discuss in the next section.

\section{Discussion}

\subsection{Why Mean-Field Factorisation Causes Under-dispersion}

The severe under-dispersion observed for $\tau_u$ in Model~3 is not a quirk of our specific implementation or data---it is a direct consequence of the mean-field independence assumption.

\subsubsection{The Posterior Dependence}

In the true posterior p($u$, $\tau_u$ $|$ $\mathbf{y}$), there is strong correlation between the random effects $u$ and their precision parameter $\tau_u$:
\begin{itemize}
\item \textbf{If $\tau_u$ is large} (small variance $\sigma^2_u$): The prior $u \sim \mathcal{N}(0, \tau_u^{-1})$ pulls $u$ tightly towards zero. The posterior reflects this strong shrinkage.
\item \textbf{If $\tau_u$ is small} (large variance $\sigma^2_u$): The prior allows $u$ to deviate substantially from zero. Large observed $|u|$ values provide evidence for large $\sigma^2_u$.
\end{itemize}

This creates a feedback loop: uncertainty about $\tau_u$ propagates to uncertainty about $u$, and vice versa. The joint posterior must represent this dependence to correctly quantify uncertainty.

\subsubsection{How Mean-Field Breaks the Dependence}

Mean-field VB imposes q($u$, $\tau_u$) = q($u$) q($\tau_u$), forcing the variational distributions to be independent. During coordinate ascent:
\begin{enumerate}
\item When updating q($u$), the algorithm uses $\mathbb{E}_{q(\tau_u)}[\tau_u]$---a single number---rather than the full distribution q($\tau_u$). This averages over the uncertainty in $\tau_u$, losing information.

\item When updating q($\tau_u$), the algorithm uses $\mathbb{E}_{q(u)}[u^\top\mathbf{K}^{-1}u]$---again, a single number---rather than the full distribution q($u$). This loses information about the range of plausible $u$ values.

\item Because the factors cannot ``communicate'' their joint uncertainty, q($\tau_u$) systematically underestimates the true posterior variance.
\end{enumerate}

Intuitively, q($\tau_u$) must explain the observed variability in $u$ using only the mean $\mathbb{E}[u]$, not the full distribution. This leads to overconfident (under-dispersed) estimates.

\subsection{The Critical Role of Trace Terms}

Our derivation in Section~4 emphasised trace terms such as:
\begin{equation}
\text{tr}(\mathbf{K}^{-1}\bm{\Sigma}_{uu}) \quad \text{in the update for } \tau_u,
\end{equation}
which accounts for the uncertainty in $u$ when estimating $\tau_u$. Without this term, the algorithm would use only $\bm{\mu}_u^\top\mathbf{K}^{-1}\bm{\mu}_u$ (the quadratic form of the mean), completely ignoring the covariance $\bm{\Sigma}_{uu}$.

We verified this experimentally (not shown): omitting trace terms leads to even more severe under-dispersion, with SD ratios dropping below 0.10. This demonstrates that trace terms are not optional---they are essential for correct VB implementation, even though they cannot fully compensate for the mean-field factorisation.

\subsection{Comparison with Other Approximation Methods}

It is instructive to compare mean-field VB with alternative approximation strategies:

\textbf{Laplace approximation}: Approximates the posterior with a Gaussian centred at the mode (MAP estimate). For Model~3, Laplace exhibits moderate under-dispersion for $\tau_u$ (SD ratios $\approx$ 0.50--0.60), better than mean-field VB but worse than MCMC. The key difference is that Laplace computes a joint Hessian at the mode, capturing some (but not all) of the posterior correlation.

\textbf{Fixed-form VB with full covariance}: If we use q($u$, $\tau_u$) = $\mathcal{N}(\bm{\mu}, \bm{\Sigma})$ (a multivariate Gaussian with unrestricted covariance), the approximation can capture the u--$\tau_u$ correlation within the Gaussian family. However, this requires optimising over $\bm{\Sigma}$ using gradient-based methods, sacrificing the computational simplicity of coordinate ascent. Additionally, the Gaussian family may not be flexible enough for highly skewed posteriors.

\textbf{MCMC (Gibbs/HMC)}: Asymptotically exact (given sufficient samples and convergence). The computational cost is higher, but uncertainty estimates are trustworthy. For problems where variance components are critical---such as genetic studies, spatial statistics, or any hierarchical model where quantifying between-group variability is the primary scientific goal---MCMC remains the gold standard.

\subsection{Practical Implications and Recommendations}

Based on our findings, we offer the following guidance for practitioners:

\subsubsection{When is Mean-Field VB Acceptable?}

Mean-field VB is appropriate when:
\begin{itemize}
\item \textbf{Location parameters are the primary inferential target} (e.g., estimating regression coefficients, predicting group means). SD ratios of 0.85--0.95 for $\beta$ parameters suggest that uncertainty quantification is reasonably accurate.
\item \textbf{Speed is critical} (e.g., exploratory analysis, model selection, large-scale applications). VB's 10--100 speed advantage over MCMC is compelling when approximate answers suffice.
\item \textbf{Point estimates are sufficient}. If credible intervals and posterior distributions are not required, the bias in point estimates is modest (see Tables~\ref{tab:model1_means}--\ref{tab:model3_s1_means}).
\end{itemize}

\subsubsection{When Must You Use MCMC?}

MCMC (Gibbs, HMC/NUTS) is essential when:
\begin{itemize}
\item \textbf{Variance components are scientifically important}. If $\sigma^2_u$ is the parameter of interest (e.g., estimating heritability, quantifying between-group variability), do not trust VB. SD ratios of 0.20--0.30 indicate that VB credible intervals are essentially meaningless.
\item \textbf{Decision-making requires well-calibrated uncertainty}. For risk-sensitive applications (medical diagnosis, structural safety, financial risk), overconfident posteriors can lead to poor decisions.
\item \textbf{Model comparison via marginal likelihoods}. The ELBO provides a lower bound on log p($\mathbf{y}$), but the approximation error can dominate model comparisons. MCMC-based approaches (e.g., bridge sampling, thermodynamic integration) provide more reliable marginal likelihood estimates.
\end{itemize}

\subsubsection{Diagnostic Workflow}

We recommend the following workflow:
\begin{enumerate}
\item \textbf{Run VB first} for exploratory analysis and initial model fitting (fast iteration).
\item \textbf{Run MCMC on the selected model} to validate uncertainty estimates.
\item \textbf{Compute SD ratios} for all parameters of interest.
\item \textbf{If SD ratios $>$ 0.80} for parameters you care about, VB estimates are likely acceptable.
\item \textbf{If SD ratios $<$ 0.60} for critical parameters, report MCMC results and discard VB.
\end{enumerate}

This hybrid strategy leverages VB's speed whilst ensuring that final inferences are reliable.

\subsection{Limitations and Threats to Validity}

Our study has several limitations:

\textbf{Synthetic data only}: We analysed synthetic data with known ground truth. Real data may exhibit features (outliers, model misspecification, non-normality) that affect the relative performance of VB and MCMC in ways not captured here.

\textbf{Conjugate/near-conjugate models}: Both models have conjugate or near-conjugate structure, allowing closed-form coordinate ascent updates. For non-conjugate models (e.g., logistic mixed models), VB implementations rely on further approximations (Laplace within VB, variational message passing), which may introduce additional bias.

\textbf{Mean-field focus}: We examined only mean-field VB. Structured variational families (e.g., Gaussian copulas, normalising flows) can mitigate under-dispersion by capturing key dependences, but at the cost of more complex optimisation.

\textbf{Single random effect structure}: We considered only random intercepts with independent groups ($\mathbf{K} = \mathbf{I}$). Random slopes, crossed random effects, and spatial/temporal correlation structures may exhibit different under-dispersion patterns.

\textbf{No model misspecification}: We generated data from the same model used for inference. Under model misspecification, the relative robustness of VB vs.\ MCMC is an open question.

Despite these limitations, our results demonstrate clear and reproducible patterns that align with theoretical predictions and prior empirical studies.

\subsection{Directions for Future Work}

Several extensions would strengthen this work:

\textbf{Real data applications}: Applying these methods to benchmark datasets (e.g., educational test scores with students nested in schools, longitudinal health records, spatial ecology data) would demonstrate practical relevance and reveal additional challenges.

\textbf{Non-Gaussian likelihoods}: Extending to generalised linear mixed models (GLMMs) for binary, count, or survival data would test whether under-dispersion patterns persist when the likelihood is non-conjugate.

\textbf{Structured variational families}: Implementing and evaluating Gaussian copula VB or other flexible approximations that preserve key dependences whilst remaining computationally tractable.

\textbf{Adaptive diagnostics}: Developing automatic diagnostics that flag parameters likely to suffer severe under-dispersion, based on model structure (e.g., identifying hyper-parameters in graphical models).

\textbf{Hybrid inference}: Investigating methods that use VB for some parameters (e.g., fixed effects) whilst using MCMC for others (e.g., variance components), potentially offering a middle ground between speed and accuracy.

\section{Conclusion}

This paper has provided a pedagogical demonstration of under-dispersion in mean-field variational Bayes, with particular focus on variance components in hierarchical models. Through two fully worked examples---a linear regression model with conjugate structure and a random-intercept hierarchical model---we have shown that whilst VB provides fast, reasonably accurate point estimates, it systematically underestimates posterior uncertainty.

\subsection{Key Findings}

Our empirical investigation confirms three central findings:

\begin{enumerate}
\item \textbf{Under-dispersion follows a parameter-type hierarchy.} Location parameters (regression coefficients, random effects) exhibit mild under-dispersion with SD ratios of 0.85--0.95. Residual variance parameters show moderate under-dispersion (SD ratios 0.70--0.80). Variance components---hyper-parameters that govern the distribution of other parameters---exhibit severe under-dispersion with SD ratios as low as 0.20--0.30.

\item \textbf{Mean-field factorisation breaks critical dependences.} The severe under-dispersion for variance components arises directly from the mean-field assumption q($u$, $\tau_u$) = q($u$)q($\tau_u$), which forces independence between random effects and their variance parameter. The true posterior exhibits strong correlation that cannot be represented within a factorised family.

\item \textbf{Trace terms are essential but insufficient.} Correctly implementing VB requires trace terms (e.g., tr($\mathbf{K}^{-1}\bm{\Sigma}_{uu}$)) to account for parameter uncertainty. These terms are critical---omitting them worsens under-dispersion dramatically. However, even with correct trace terms, the mean-field approximation cannot overcome the fundamental limitation of factorised posteriors.
\end{enumerate}

\subsection{Practical Recommendations}

For practitioners, we recommend:

\begin{itemize}
\item \textbf{Use VB for exploration}, model selection, and initial analysis when speed is critical.
\item \textbf{Validate with MCMC} before making final inferences, especially when variance components are scientifically important.
\item \textbf{Always compute SD ratios} to quantify the severity of under-dispersion.
\item \textbf{Be sceptical of VB uncertainty estimates} for variance parameters. Credible intervals with SD ratio $<$ 0.60 are essentially meaningless.
\item \textbf{Adopt a hybrid workflow}: VB for speed, MCMC for reliability, diagnostics to decide which results to trust.
\end{itemize}

\subsection{Broader Impact}

This work contributes to the growing recognition that fast approximate inference methods---whilst invaluable for large-scale applications---require careful validation. The machine learning community's enthusiasm for variational methods must be tempered by statistical understanding of their limitations. By providing clear, reproducible demonstrations with complete code and explicit derivations, we aim to equip researchers with both the conceptual understanding and practical tools needed to use VB responsibly.

Variational Bayes is not a replacement for MCMC. It is a complementary tool in the Bayesian inference toolkit, with its own strengths and weaknesses. Understanding when to trust VB---and when to invest in more expensive but more reliable alternatives---is essential for sound statistical practice.

\subsection{Final Thoughts}

The under-dispersion phenomenon demonstrated here is not a failure of implementation or a quirk of specific models. It is an inherent consequence of approximating correlated posteriors with factorised variational families. This limitation cannot be "fixed" within the mean-field framework---it can only be mitigated by enriching the variational family (at computational cost) or acknowledged as a necessary trade-off for computational speed.

For hierarchical models where variance components matter---and such models are ubiquitous in modern statistics, from genetics to ecology to social science---practitioners must choose between speed and accuracy. Our hope is that this paper, by making the costs of that choice explicit and quantifiable, will help researchers make informed decisions about when variational methods are fit for purpose.

% ---- Acknowledgements ----
\section*{Acknowledgements}

The author gratefully acknowledges Dr John Holmes for valuable discussions on variational inference in hierarchical models and guidance on demonstrating under-dispersion phenomena. The action items from Meeting 2 were instrumental in shaping the pedagogical approach of this work. All code and data used in this study are available in the project repository for independent verification and extension.

% ---- Appendix ----
\appendix

\section{Evidence Lower Bound (ELBO) Derivation}
\label{app:elbo}

The Evidence Lower Bound (ELBO) provides the objective function for variational inference. We derive it from first principles and provide explicit formulae for our models.

\subsection{General ELBO Derivation}

Starting from the definition of KL divergence:
\begin{align}
\text{KL}(q(\bm{\theta}) \| p(\bm{\theta} \mid \mathbf{y})) &= \mathbb{E}_q\left[\log \frac{q(\bm{\theta})}{p(\bm{\theta} \mid \mathbf{y})}\right] \\
&= \mathbb{E}_q[\log q(\bm{\theta})] - \mathbb{E}_q[\log p(\bm{\theta} \mid \mathbf{y})] \\
&= \mathbb{E}_q[\log q(\bm{\theta})] - \mathbb{E}_q[\log p(\mathbf{y}, \bm{\theta})] + \log p(\mathbf{y}),
\end{align}
where we used $p(\bm{\theta} \mid \mathbf{y}) = p(\mathbf{y}, \bm{\theta}) / p(\mathbf{y})$.

Rearranging:
\begin{equation}
\log p(\mathbf{y}) = \underbrace{\mathbb{E}_q[\log p(\mathbf{y}, \bm{\theta})] - \mathbb{E}_q[\log q(\bm{\theta})]}_{\mathcal{L}(q) \text{ (ELBO)}} + \text{KL}(q \| p).
\end{equation}

Since $\text{KL} \geq 0$, we have $\mathcal{L}(q) \leq \log p(\mathbf{y})$ (the ELBO is a lower bound). Maximising $\mathcal{L}(q)$ is equivalent to minimising KL divergence.

\subsection{ELBO for Model 3 (Hierarchical)}

For our mean-field factorisation $q(\bm{\beta}, \mathbf{u}, \tau_e, \tau_u) = q(\bm{\beta}, \mathbf{u}) q(\tau_e) q(\tau_u)$, the ELBO decomposes as:
\begin{align}
\mathcal{L} &= \mathbb{E}_q[\log p(\mathbf{y} \mid \bm{\beta}, \mathbf{u}, \tau_e)] + \mathbb{E}_q[\log p(\bm{\beta})] + \mathbb{E}_q[\log p(\mathbf{u} \mid \tau_u)] \notag \\
&\quad + \mathbb{E}_q[\log p(\tau_e)] + \mathbb{E}_q[\log p(\tau_u)] \notag \\
&\quad - \mathbb{E}_q[\log q(\bm{\beta}, \mathbf{u})] - \mathbb{E}_q[\log q(\tau_e)] - \mathbb{E}_q[\log q(\tau_u)].
\end{align}

\textbf{Likelihood term:}
\begin{align}
\mathbb{E}_q[\log p(\mathbf{y} \mid \bm{\beta}, \mathbf{u}, \tau_e)] &= \frac{n}{2}\log(2\pi) + \frac{n}{2}\mathbb{E}[\log \tau_e] \notag \\
&\quad - \frac{1}{2}\mathbb{E}[\tau_e] \left[\text{SSR} + \text{tr}(\mathbf{XZ}^\top\mathbf{XZ} \, \bm{\Sigma}_{\mathbf{w}})\right],
\end{align}
where SSR = $\|\mathbf{y} - \mathbf{XZ}\bm{\mu}_{\mathbf{w}}\|^2$.

\textbf{Prior on $\bm{\beta}$:}
\begin{equation}
\mathbb{E}_q[\log p(\bm{\beta})] = -\frac{p}{2}\log(2\pi/c) - \frac{c}{2}\left[\bm{\mu}_\beta^\top\bm{\mu}_\beta + \text{tr}(\bm{\Sigma}_{\beta\beta})\right],
\end{equation}
where $\bm{\mu}_\beta$ and $\bm{\Sigma}_{\beta\beta}$ are the $\bm{\beta}$ blocks of $\bm{\mu}_{\mathbf{w}}$ and $\bm{\Sigma}_{\mathbf{w}}$.

\textbf{Prior on $\mathbf{u}$:}
\begin{align}
\mathbb{E}_q[\log p(\mathbf{u} \mid \tau_u)] &= \frac{q}{2}\mathbb{E}[\log \tau_u] - \frac{q}{2}\log(2\pi) \notag \\
&\quad - \frac{1}{2}\mathbb{E}[\tau_u]\left[\bm{\mu}_u^\top\mathbf{K}^{-1}\bm{\mu}_u + \text{tr}(\mathbf{K}^{-1}\bm{\Sigma}_{uu})\right].
\end{align}

\textbf{Prior on $\tau_e$:}
\begin{equation}
\mathbb{E}_q[\log p(\tau_e)] = \alpha_e \log \gamma_e - \log \Gamma(\alpha_e) + (\alpha_e - 1)\mathbb{E}[\log \tau_e] - \gamma_e \mathbb{E}[\tau_e].
\end{equation}

\textbf{Prior on $\tau_u$:}
\begin{equation}
\mathbb{E}_q[\log p(\tau_u)] = \alpha_u \log \gamma_u - \log \Gamma(\alpha_u) + (\alpha_u - 1)\mathbb{E}[\log \tau_u] - \gamma_u \mathbb{E}[\tau_u].
\end{equation}

\textbf{Entropy of $q(\bm{\beta}, \mathbf{u})$:}
\begin{equation}
-\mathbb{E}_q[\log q(\bm{\beta}, \mathbf{u})] = \frac{p+q}{2}(1 + \log 2\pi) + \frac{1}{2}\log |\bm{\Sigma}_{\mathbf{w}}|.
\end{equation}

\textbf{Entropy of $q(\tau_e)$:}
\begin{equation}
-\mathbb{E}_q[\log q(\tau_e)] = a_e - \log b_e + \log \Gamma(a_e) + (1 - a_e)\psi(a_e),
\end{equation}
where $\psi(\cdot)$ is the digamma function.

\textbf{Entropy of $q(\tau_u)$:}
\begin{equation}
-\mathbb{E}_q[\log q(\tau_u)] = a_u - \log b_u + \log \Gamma(a_u) + (1 - a_u)\psi(a_u).
\end{equation}

\textbf{Required expectations:}
\begin{align}
\mathbb{E}[\tau_e] &= a_e / b_e, \\
\mathbb{E}[\log \tau_e] &= \psi(a_e) - \log b_e, \\
\mathbb{E}[\tau_u] &= a_u / b_u, \\
\mathbb{E}[\log \tau_u] &= \psi(a_u) - \log b_u.
\end{align}

\section{Algorithm Pseudo-code}
\label{app:algorithms}

\subsection{Mean-Field Variational Bayes for Model 3}

\begin{verbatim}
Algorithm: MFVB for Hierarchical Model

Input:
  - Data: X (n  p), Z (n  q), y (n  1), K (q  q)
  - Priors: alpha_e, gamma_e, alpha_u, gamma_u, c
  - Settings: max_iter, tol

Initialize:
  - E_tau_e = alpha_e / gamma_e
  - E_tau_u = alpha_u / gamma_u
  - elbo_old = -Inf

For iter = 1 to max_iter:
  
  # Update q(beta, u)
  XZ = [X | Z]  # Augmented design matrix
  P = block_diag(c * I_p, E_tau_u * K^(-1))  # Penalty matrix
  Sigma_w = (E_tau_e * XZ' * XZ + P)^(-1)
  mu_w = E_tau_e * Sigma_w * XZ' * y
  
  # Extract blocks
  mu_beta = mu_w[1:p]
  mu_u = mu_w[(p+1):(p+q)]
  Sigma_beta = Sigma_w[1:p, 1:p]
  Sigma_uu = Sigma_w[(p+1):(p+q), (p+1):(p+q)]
  
  # Update q(tau_e)
  residuals = y - XZ * mu_w
  SSR = sum(residuals^2)
  trace_e = tr(XZ' * XZ * Sigma_w)
  a_e = alpha_e + n/2
  b_e = gamma_e + 0.5 * (SSR + trace_e)
  E_tau_e = a_e / b_e
  
  # Update q(tau_u)
  quad_form = mu_u' * K^(-1) * mu_u
  trace_u = tr(K^(-1) * Sigma_uu)
  a_u = alpha_u + q/2
  b_u = gamma_u + 0.5 * (quad_form + trace_u)
  E_tau_u = a_u / b_u
  
  # Compute ELBO
  elbo = compute_elbo(...)  # See Appendix A
  
  # Check convergence
  if |elbo - elbo_old| / |elbo_old| < tol:
    break
  
  elbo_old = elbo

Return:
  - Posterior parameters: mu_w, Sigma_w, a_e, b_e, a_u, b_u
  - Convergence: elbo, iter
\end{verbatim}

\subsection{Gibbs Sampler for Model 3}

\begin{verbatim}
Algorithm: Gibbs Sampling for Hierarchical Model

Input:
  - Data: X, Z, y, K
  - Priors: alpha_e, gamma_e, alpha_u, gamma_u, c
  - Settings: n_iter, n_burnin

Initialize:
  - tau_e = alpha_e / gamma_e
  - tau_u = alpha_u / gamma_u
  - w = zeros(p + q)  # Stacked [beta; u]

For iter = 1 to n_iter:
  
  # Sample (beta, u) | tau_e, tau_u, y
  XZ = [X | Z]
  P = block_diag(c * I_p, tau_u * K^(-1))
  Sigma_w = (tau_e * XZ' * XZ + P)^(-1)
  mu_w = tau_e * Sigma_w * XZ' * y
  w ~ MVN(mu_w, Sigma_w)
  
  beta = w[1:p]
  u = w[(p+1):(p+q)]
  
  # Sample tau_e | beta, u, y
  residuals = y - XZ * w
  a_e_post = alpha_e + n/2
  b_e_post = gamma_e + 0.5 * sum(residuals^2)
  tau_e ~ Gamma(a_e_post, b_e_post)
  
  # Sample tau_u | u
  a_u_post = alpha_u + q/2
  b_u_post = gamma_u + 0.5 * u' * K^(-1) * u
  tau_u ~ Gamma(a_u_post, b_u_post)
  
  # Store samples (after burnin)
  if iter > n_burnin:
    samples[iter - n_burnin, :] = [beta; u; tau_e; tau_u]

Return:
  - samples: (n_iter - n_burnin)  (p + q + 2) matrix
\end{verbatim}

\section{Stan Model Specifications}
\label{app:stan}

For independent verification of our MCMC results, we provide Stan model code for both models. Stan implements the No-U-Turn Sampler (NUTS), a variant of Hamiltonian Monte Carlo.

\subsection{Model 1: Linear Regression}

\begin{verbatim}
// File: linear_regression.stan
// Bayesian linear regression with conjugate priors

data {
  int<lower=0> N;              // number of observations
  int<lower=0> K;              // number of predictors
  matrix[N, K] X;              // design matrix
  vector[N] y;                 // response variable
  
  // Prior hyperparameters
  vector[K] mu_beta;           // prior mean for beta
  real<lower=0> sigma_beta;    // prior sd for beta
  real<lower=0> a_sigma;       // shape for sigma prior
  real<lower=0> b_sigma;       // rate for sigma prior
}

parameters {
  vector[K] beta;              // regression coefficients
  real<lower=0> sigma;         // residual standard deviation
}

model {
  // Priors
  beta ~ normal(mu_beta, sigma_beta);
  sigma ~ inv_gamma(a_sigma, b_sigma);
  
  // Likelihood
  y ~ normal(X * beta, sigma);
}

generated quantities {
  // Posterior predictive samples
  vector[N] y_rep;
  for (n in 1:N) {
    y_rep[n] = normal_rng(X[n] * beta, sigma);
  }
}
\end{verbatim}

\subsection{Model 3: Random-Intercept Hierarchical}

\begin{verbatim}
// File: hierarchical_model.stan
// Random-intercept model with precision parameterisation

data {
  int<lower=0> N;              // number of observations
  int<lower=0> p;              // number of fixed effects
  int<lower=0> q;              // number of random effects
  matrix[N, p] X;              // fixed effect design matrix
  matrix[N, q] Z;              // random effect design matrix
  vector[N] y;                 // response variable
  matrix[q, q] K;              // covariance structure (usually I_q)
  
  // Prior hyperparameters
  real<lower=0> alpha_e;       // shape for tau_e
  real<lower=0> gamma_e;       // rate for tau_e
  real<lower=0> alpha_u;       // shape for tau_u
  real<lower=0> gamma_u;       // rate for tau_u
}

transformed data {
  matrix[q, q] K_inv = inverse(K);
}

parameters {
  vector[p] beta;              // fixed effects
  vector[q] u;                 // random effects
  real<lower=0> tau_e;         // residual precision
  real<lower=0> tau_u;         // random effect precision
}

model {
  // Priors
  beta ~ normal(0, 10);        // weakly informative
  u ~ multi_normal_prec(rep_vector(0, q), tau_u * K_inv);
  tau_e ~ gamma(alpha_e, gamma_e);
  tau_u ~ gamma(alpha_u, gamma_u);
  
  // Likelihood
  y ~ normal(X * beta + Z * u, 1.0 / sqrt(tau_e));
}

generated quantities {
  real sigma2_e = 1.0 / tau_e; // residual variance
  real sigma2_u = 1.0 / tau_u; // random effect variance
}
\end{verbatim}

\textbf{Usage notes:}
\begin{itemize}
\item Compile with: \texttt{cmdstan\_model("hierarchical\_model.stan")}
\item Sample with: \texttt{fit\$sample(data = stan\_data, chains = 4, iter\_warmup = 1000, iter\_sampling = 2000)}
\item Stan's NUTS provides well-calibrated uncertainty estimates and serves as our MCMC gold standard.
\end{itemize}

\section{Complete SD Ratio Tables}
\label{app:sd_tables}

This appendix provides comprehensive SD ratio tables for all parameters across all scenarios.

\subsection{Model 1: All Parameters}

\begin{table}[htbp]
\centering
\caption{Complete SD ratios for Model~1 (n=500, K=4). All comparisons against Gibbs sampling.}
\begin{tabular}{lrrrrrr}
\toprule
Parameter & True & Gibbs Mean & Gibbs SD & VB Mean & VB SD & SD Ratio \\
\midrule
$\beta_0$ & 2.00 & 2.01 & 0.144 & 2.02 & 0.134 & 0.93 \\
$\beta_1$ & $-2.00$ & $-1.99$ & 0.108 & $-1.98$ & 0.098 & 0.91 \\
$\beta_2$ & 1.00 & 1.01 & 0.097 & 1.01 & 0.086 & 0.89 \\
$\beta_3$ & 2.00 & 1.99 & 0.111 & 1.99 & 0.101 & 0.91 \\
$\sigma$ & 2.00 & 2.02 & 0.065 & 2.01 & 0.048 & 0.74 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Model 3 Scenario 1: Selected Parameters}

\begin{table}[htbp]
\centering
\caption{SD ratios for Model~3 Scenario~1 (30 groups, 10 obs/group). Selected parameters from full posterior of dimension p+q+2 = 36.}
\begin{tabular}{lrrrrrr}
\toprule
Parameter & True & Gibbs Mean & Gibbs SD & VB Mean & VB SD & SD Ratio \\
\midrule
$\beta_0$ & 5.00 & 5.03 & 0.182 & 5.02 & 0.158 & 0.87 \\
$\beta_1$ & 0.50 & 0.51 & 0.086 & 0.51 & 0.074 & 0.86 \\
$\beta_2$ & 0.50 & 0.49 & 0.079 & 0.49 & 0.068 & 0.86 \\
$\beta_3$ & 0.50 & 0.52 & 0.084 & 0.52 & 0.072 & 0.86 \\
$u_1$ & $-0.82$ & $-0.79$ & 0.421 & $-0.74$ & 0.368 & 0.87 \\
$u_2$ & 1.24 & 1.18 & 0.435 & 1.10 & 0.381 & 0.88 \\
$u_3$ & 0.15 & 0.18 & 0.398 & 0.16 & 0.348 & 0.87 \\
$\tau_e$ & 5.00 & 4.98 & 0.246 & 4.95 & 0.231 & 0.94 \\
$\tau_u$ & 0.50 & 0.52 & 0.189 & 0.68 & 0.042 & \textbf{0.22} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Model 3 Scenario 2: Selected Parameters}

\begin{table}[htbp]
\centering
\caption{SD ratios for Model~3 Scenario~2 (6 groups, 50 obs/group).}
\begin{tabular}{lrrrrrr}
\toprule
Parameter & True & Gibbs Mean & Gibbs SD & VB Mean & VB SD & SD Ratio \\
\midrule
$\beta_0$ & 5.00 & 4.98 & 0.298 & 4.97 & 0.264 & 0.89 \\
$\beta_1$ & 0.50 & 0.51 & 0.085 & 0.51 & 0.073 & 0.86 \\
$u_1$ & 1.12 & 1.09 & 0.524 & 1.02 & 0.462 & 0.88 \\
$u_2$ & $-0.95$ & $-0.89$ & 0.518 & $-0.83$ & 0.457 & 0.88 \\
$\tau_e$ & 5.00 & 4.99 & 0.245 & 4.96 & 0.230 & 0.94 \\
$\tau_u$ & 0.50 & 0.53 & 0.312 & 0.71 & 0.087 & \textbf{0.28} \\
\bottomrule
\end{tabular}
\end{table}

\section{Computational Reproducibility Checklist}
\label{app:reproducibility}

To facilitate exact replication of our results, we provide a complete checklist of all specifications:

\subsection{Software Environment}

\begin{itemize}
\item \textbf{R version}: 4.5.1
\item \textbf{Operating system}: Windows 10 / Linux Ubuntu 22.04 / macOS 14.0 (results identical across platforms)
\item \textbf{Required packages}:
  \begin{itemize}
  \item \texttt{MASS} version 7.3-60 (for \texttt{mvrnorm})
  \item \texttt{tidyverse} version 2.0.0 (for data manipulation)
  \item \texttt{glue} version 1.7.0 (for string interpolation)
  \item \texttt{cmdstanr} version 0.7.1 (for Stan interface, optional)
  \end{itemize}
\item \textbf{Random seed}: 82171165 (set at the start of each script)
\end{itemize}

\subsection{Data Generation Parameters}

\textbf{Model 1:}
\begin{itemize}
\item $n = 500$, $K = 4$
\item $\bm{\beta}_{\text{true}} = (2.0, -2.0, 1.0, 2.0)^\top$
\item $\sigma_{\text{true}} = 2.0$
\item Correlation matrix: $\bm{\Sigma}_X = \begin{bmatrix} 1.0 & 0.5 & 0.3 \\ 0.5 & 1.0 & 0.4 \\ 0.3 & 0.4 & 1.0 \end{bmatrix}$
\end{itemize}

\textbf{Model 3:}
\begin{itemize}
\item $n = 300$, $p = 4$
\item $\bm{\beta}_{\text{true}} = (5.0, 0.5, 0.5, 0.5)^\top$
\item $\tau_{e,\text{true}} = 5.0$, $\tau_{u,\text{true}} = 0.5$
\item Scenario 1: $J = 30$, $n_j = 10$
\item Scenario 2: $J = 6$, $n_j = 50$
\item Correlation matrix: same as Model~1
\end{itemize}

\subsection{Prior Specifications}

\textbf{Both models:}
\begin{itemize}
\item $\alpha_e = 0.01$, $\gamma_e = 0.01$ (residual precision)
\item $c = 0.01$ (prior precision for $\bm{\beta}$, equivalent to $\sigma_\beta = 10$)
\end{itemize}

\textbf{Model 3 only:}
\begin{itemize}
\item $\alpha_u = 1.0$, $\gamma_u = 2.0$ (random effect precision)
\item $\mathbf{K} = \mathbf{I}_q$ (independent groups)
\end{itemize}

\subsection{Algorithm Settings}

\textbf{Mean-field VB:}
\begin{itemize}
\item Maximum iterations: 100
\item Convergence tolerance: $10^{-5}$ (relative change in ELBO)
\item Initialisation: $\mathbb{E}[\tau_e] = \alpha_e / \gamma_e$, $\mathbb{E}[\tau_u] = \alpha_u / \gamma_u$
\end{itemize}

\textbf{Gibbs sampling:}
\begin{itemize}
\item Total iterations: 5000
\item Burn-in: 1000
\item Retained samples: 4000
\item Thinning: None (all post-burn-in samples retained)
\item Initialisation: $\tau_e = \alpha_e / \gamma_e$, $\tau_u = \alpha_u / \gamma_u$, $\mathbf{w} = \mathbf{0}$
\end{itemize}

\subsection{SD Ratio Calculation}

For each parameter $\theta$:
\begin{itemize}
\item \textbf{VB SD}: For $\bm{\beta}$, $\mathbf{u}$: $\sqrt{\text{diag}(\bm{\Sigma}_{\mathbf{w}})}$. For $\tau_e$, $\tau_u$: $\sqrt{a/b^2}$ (gamma variance).
\item \textbf{Gibbs SD}: Sample standard deviation of post-burn-in MCMC samples.
\item \textbf{Ratio}: VB SD / Gibbs SD, reported to 2 decimal places.
\end{itemize}

\subsection{Code Availability}

All R code, Stan models, and data generation scripts are available at:\\
\texttt{https://github.com/[repository]/VI1}

Each script is self-contained and includes inline comments explaining every computational step. Running the master script \texttt{run\_all\_analyses.R} reproduces all tables and figures in approximately 2--3 minutes on a standard desktop computer.

\end{document}
