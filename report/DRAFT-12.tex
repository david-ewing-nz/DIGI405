\documentclass[11pt,a4paper]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}

\title{Variational Inference: A Practical Journey Through Bayesian Models}
\author{VI1 Project Version 12}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This document presents a progression through variational inference (VI) methods applied to increasingly complex Bayesian models. We begin with linear regression, where exact posterior solutions exist and provide a validation benchmark, then advance to hierarchical models with random effects that demonstrate the systematic under-dispersion of variance components under mean-field approximations. Each stage builds understanding of VI's role in making Bayesian inference tractable, whilst revealing the limitations imposed by factorisation assumptions.
\end{abstract}

\section{Introduction}

Mean-field variational inference is a method for approximate Bayesian posterior inference. It approximates a full posterior distribution with a factorised set of distributions by maximising a lower bound on the marginal likelihood. This requires the ability to integrate a sum of terms in the log joint likelihood using this factorised distribution. Often not all integrals are available in closed form, which is typically handled by using a lower bound.

Why this matters depends on one's inferential goals. Bayesian inference seeks to characterise the full posterior distribution $p(\theta \mid \text{data})$, representing uncertainty about parameters through probability distributions. This contrasts with frequentist inference, which estimates parameters as fixed but unknown constants and quantifies uncertainty through repeated-sampling properties such as standard errors and confidence intervals. The choice between paradigms determines what we seek: Bayesians want posterior distributions and credible intervals; frequentists want point estimates with sampling distributions. Mean-field variational inference addresses the Bayesian goal when exact posterior computation is intractable.

This paper focuses specifically on mean-field VI applied to two models of increasing complexity. Model 1 is Bayesian linear regression, where the response $y_i$ follows
\[
  y_i = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_p x_{pi} + \varepsilon_i
\]
with $\varepsilon_i \sim N(0, \sigma^2)$. The Bayesian goal is the posterior $p(\beta, \sigma^2 \mid y, X)$; the frequentist analogue would be estimates $\hat{\beta}$ with standard errors. Under mean-field VI, we factorise $q(\beta, \sigma^2) = q(\beta)\,q(\sigma^2)$, treating parameters as independent in the variational approximation.

Model 3 extends to hierarchical structure with random intercepts:
\[
  y_{ij} = \beta_0 + \beta_1 x_{1ij} + \cdots + u_j + \varepsilon_{ij},
\]
where $u_j \sim N(0, \sigma_u^2)$ represents group-specific deviations and $\varepsilon_{ij} \sim N(0, \sigma^2)$ is observation-level noise. Observations are nested within groups (for example, students within schools), and the random intercepts capture within-group correlation whilst allowing information sharing across groups. The Bayesian target is the joint posterior $p(\beta, u, \sigma_u^2, \sigma^2 \mid y, X, \text{groups})$; frequentist mixed models would estimate fixed effects $\hat{\beta}$ and variance components $\hat{\sigma}_u^2$, $\hat{\sigma}^2$. Mean-field VI factorises this as $q(\beta, u, \sigma_u^2, \sigma^2) = q(\beta)\,q(u)\,q(\sigma_u^2)\,q(\sigma^2)$.

These two models serve a pedagogical purpose. Model 1 establishes that mean-field VI can recover known posteriors when exact solutions exist, building confidence in the method. Model 3 reveals a systematic limitation: variance components like $\sigma_u^2$ exhibit under-dispersion under mean-field approximations, with posterior distributions too narrow compared to MCMC gold standards. This document demonstrates this phenomenon empirically using both synthetic and real data, explaining when mean-field VI is adequate and when its factorisation assumption becomes problematic.

\section{Variational Inference as Optimisation}

Variational Inference approaches posterior inference through optimisation rather than sampling. The idea is to replace the exact posterior $p(z \mid x)$ with a simpler, tractable distribution $q_\nu(z)$ drawn from a chosen family. Figure~\ref{fig:vi-optimisation} provides the standard picture: we restrict attention to a variational family $q(z;\nu)$ and then optimise $\nu$ so that $q(z;\nu)$ is close (in KL divergence) to the true posterior $p(z\mid x)$.

\captionsetup{font=footnotesize, width=0.7\textwidth}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.78\textwidth]{../presentation/q-space-visual.png}
  \caption{Variational inference as optimisation over variational parameters $\nu$ within a variational family $q(z;\nu)$, choosing $\nu^\star$ to minimise $\mathrm{KL}(q(z;\nu)\,\|\,p(z\mid x))$. Adapted from Blei \cite{Blei_Columbia}.}
  \label{fig:vi-optimisation}
\end{figure}

The ellipse represents all families of tractable approximations, indexed by the variational parameters $\nu$. Starting from an initial value $\nu^{\mathrm{init}}$, an optimisation routine moves through parameter space (the grey path) to reach $\nu^\star$, the best approximation available within the family. The true posterior $p(z\mid x)$ sits outside the ellipse because it is generally too complex to belong to the variational family, and the dashed segment indicates the remaining discrepancy measured by $\mathrm{KL}\!\left(q(z;\nu^\star)\,\|\,p(z\mid x)\right)$.

\subsection{Specifying the Variational Family}

A critical modelling choice in VI is how we define the family $\mathcal{Q}$. This determines both the computational tractability and the expressive power of the approximation. The spectrum of choices ranges from full-form to fully factorised:

\paragraph{Full-form variational inference.} At one extreme, we could specify a joint distribution $q(z)$ with no factorisation, allowing all dependencies between parameters to be preserved. For example, $q(\beta, \sigma^2)$ might be a joint distribution with covariance between $\beta$ and $\sigma^2$. This offers maximum flexibility but requires optimising over a large number of variational parameters (covariance matrices grow quadratically with dimension) and may not admit closed-form updates.

\paragraph{Mean-field variational inference.} At the other extreme, we assume complete factorisation:
\[
  q(z) = \prod_{j=1}^J q_j(z_j),
\]
where $z = (z_1, \ldots, z_J)$ is partitioned into components and each $q_j(z_j)$ is an independent distribution. This independence assumption is unrealistic as a literal description of the true posterior, but it dramatically simplifies optimisation. For Model 1, this gives $q(\beta, \sigma^2) = q(\beta)\,q(\sigma^2)$. For Model 3, it gives $q(\beta, u, \sigma_u^2, \sigma^2) = q(\beta)\,q(u)\,q(\sigma_u^2)\,q(\sigma^2)$, with each parameter optimised independently.

\paragraph{Structured mean-field (blocking).} Between these extremes lies structured mean-field, where we group strongly correlated parameters into blocks that preserve some dependencies:
\[
  q(z) = q(z_{\text{block}_1})\,q(z_{\text{block}_2})\cdots q(z_{\text{block}_K}).
\]
Each block maintains internal dependencies whilst remaining independent of other blocks. For Model 3, we might block as $q(\beta, \sigma^2)\,q(u, \sigma_u^2)$, preserving the correlation between random effects and their variance component whilst treating fixed effects independently.

The blocking choice has profound implications. In Model 3, the true posterior exhibits strong dependence between $u$ and $\sigma_u^2$: if the variance component is large, the data support larger deviations $|u_j|$ from zero; if small, the posterior for each $u_j$ is pulled tightly towards zero (shrinkage). When we factorise as $q(u)\,q(\sigma_u^2)$, this dependence is broken. The algorithm updates $q(u)$ given the current $q(\sigma_u^2)$, then updates $q(\sigma_u^2)$ given the current $q(u)$, but the two distributions cannot coordinate their uncertainty. This leads to systematic under-dispersion: $q(\sigma_u^2)$ is too narrow, underestimating the true posterior variance.

This paper uses full mean-field factorisation throughout (no blocking), which makes the under-dispersion phenomenon most visible. For Model 1, the factorisation $q(\beta)\,q(\sigma^2)$ is relatively benign because $\beta$ and $\sigma^2$ are only weakly correlated posteriorly. For Model 3, the factorisation $q(u)\,q(\sigma_u^2)$ is problematic because these parameters are strongly dependent, and breaking this dependence causes the variance component posterior to collapse onto values that are too small.

\subsection{The Evidence Lower Bound (ELBO)}

Directly minimising the KL divergence $\mathrm{KL}(q_\nu(z) \| p(z \mid x))$ is not possible because it depends on the intractable marginal likelihood $p(x)$. Instead, we work with the Evidence Lower Bound (ELBO), defined by
\[
  \mathcal{L}(\nu)
  = \mathbb{E}_{q_\nu}\!\left[ \log p(x,z) \right]
    - \mathbb{E}_{q_\nu}\!\left[ \log q_\nu(z) \right].
\]
It can be shown that
\[
  \log p(x) = \mathcal{L}(\nu)
  + \mathrm{KL}\bigl(q_\nu(z) \,\|\, p(z \mid x)\bigr),
\]
so that for fixed data $x$, maximising $\mathcal{L}(\nu)$ is equivalent to minimising the KL divergence. The ELBO thus serves as a surrogate objective that we can evaluate using only $p(x,z)$ and $q_\nu(z)$.

The ELBO has a useful interpretation as a balance between two terms. The first, $\mathbb{E}_{q_\nu}[\log p(x,z)]$, is the expected log joint, which encourages $q_\nu(z)$ to place mass on configurations of $z$ that explain the data well. The second, $-\mathbb{E}_{q_\nu}[\log q_\nu(z)]$, is the entropy of $q_\nu$, which encourages the approximation to remain diffuse and avoid collapsing onto a single point. Optimising the ELBO therefore trades off goodness-of-fit against complexity.

Under mean-field factorisation, the ELBO can often be optimised via coordinate ascent: we cycle through factors $q_j(z_j)$, updating each in turn whilst holding the others fixed. For conjugate-exponential families, these updates have closed form. For non-conjugate models, we resort to gradient-based optimisation or sampling-based approximations to the ELBO gradient.

\subsection{Implications of the Factorisation Choice}

The mean-field assumption imposes a strong prior belief: that parameters are independent. When this is false (as it nearly always is), the variational posterior systematically underestimates uncertainty. This manifests differently for different parameter types. Location parameters such as regression coefficients $\beta$ or random effects $u_j$ tend to have posterior means that are reasonably accurate, with variances mildly underestimated. Scale parameters such as standard deviations $\sigma$, variance components $\sigma_u^2$, or precision parameters $\tau$ tend to have posteriors that are severely under-dispersed, with mass concentrated on smaller values than the true posterior supports.

The asymmetry arises because variance components are hyper-parameters: they appear in the priors of other parameters. In Model 3, $\sigma_u^2$ governs the distribution $u_j \sim N(0, \sigma_u^2)$, creating strong posterior dependence between $\sigma_u^2$ and $u$. Mean-field factorisation breaks this dependence, and the algorithm loses information about their joint uncertainty. The result is a posterior $q(\sigma_u^2)$ that is too narrow, leading to over-shrinkage of the random effects and overconfident predictions.

Put succinctly, the true posterior couples $u$ and $\tau_u$: large $\tau_u$ permits larger random-effect deviations, while small $\tau_u$ enforces tight shrinkage. In the blocked mean-field approximation $q(\beta,u)\,q(\tau_e)\,q(\tau_u)$, $q(\beta,u)$ remains coupled but $q(\tau_u)$ is factored out and updated using only $\mathbb{E}_{q(\tau_u)}[\tau_u]$. This forces independence, so the two factors cannot share joint uncertainty, and $q(\tau_u)$ collapses onto values that are too small, producing under-dispersion.

This is the central phenomenon we demonstrate empirically in the remainder of this document. Model 1 establishes that VI works when dependencies are weak; Model 3 reveals where it fails when dependencies are strong. The pedagogical value lies in the contrast: by starting where VI succeeds and advancing to where it struggles, we build both competence in the method and awareness of its limitations.

\section{The Learning Progression: From Simple to Complex}

\subsection{Stage 1: Linear Regression Models (Model1 Files)}

Our journey begins with Bayesian linear regression, a setting where exact posterior inference is analytically tractable. This provides an ideal starting point for several reasons.

When learning VI, it is crucial to have ground truth against which to validate our approximations. With conjugate Gaussian priors and likelihood, the posterior for linear regression coefficients is known exactly. This allows us to implement VI algorithms and directly compare the approximate posterior $q_\nu(\beta)$ against the true posterior $p(\beta \mid y, X)$. Any discrepancies we observe reflect limitations of our variational family or optimisation procedure, not uncertainty about what the correct answer should be.

The implementation explores several variations. The basic Model1 file establishes the standard setup: predicting a continuous outcome from predictors with Gaussian errors. The Boston housing variant applies this to real data, demonstrating how VI performs with actual observations rather than simulated data. The alternative parameterisations investigate different optimisation strategies. Does reparameterising the variance help convergence? How sensitive is the solution to initialisation? These practical questions arise immediately even in this simple setting.

This stage teaches the mechanics of VI in a forgiving environment. We learn to specify variational families (typically mean-field Gaussians), compute gradients of the ELBO, and monitor convergence. We observe how the approximation quality depends on the variational family's flexibility. Most importantly, we build confidence in the method by seeing it recover known posteriors, establishing a benchmark for what adequate approximation looks like before moving to settings where we lack exact solutions.

\subsection{Stage 2: Hierarchical Models with Random Effects (Model3 Files)}

Having established VI's validity in a simple setting, we advance to hierarchical models with random intercepts. Here, the motivation for approximate inference becomes clear, and the limitations of mean-field factorisation emerge.

Real data often has grouped structure: students within schools, patients within hospitals, measurements within subjects. Hierarchical models capture this by allowing group-specific parameters (random effects) that are themselves drawn from a population distribution. The posterior now involves potentially hundreds of latent variables (one per group), making exact inference impractical even when conjugacy holds in principle. This is precisely where VI's scalability advantage emerges.

The Model3 implementation considers a realistic scenario: data grouped into clusters, with each cluster having its own intercept that deviates from a global mean. The generative model introduces two layers of randomness: the random intercepts (group-level parameters) and the observation noise (individual-level parameters). The posterior must simultaneously infer the population hyperparameters and the specific realisation of random effects for each observed group.

This stage reveals VI's computational advantage. Whilst MCMC would need to sample hundreds of correlated variables, VI factorises the approximate posterior, assuming $q(\theta_1, \theta_2, \ldots) = \prod_i q(\theta_i)$. This independence assumption is clearly wrong—random effects are correlated through shared hyperparameters—but it makes optimisation tractable. We learn to diagnose when this approximation is adequate (often surprisingly so for prediction) and when it breaks down (typically when posterior correlations are strong).

The Model3 files also introduce the under-dispersion phenomenon. When we compare the variational posterior for $\sigma_u^2$ against MCMC estimates, we observe systematic bias: the VI distribution is too narrow, placing excessive mass on small values. This leads to over-shrinkage of the random effects: individual group intercepts are pulled too tightly towards the global mean, and the model appears more confident than the data warrant. The variance ratio diagnostic quantifies this: $\text{VR}(\sigma_u^2) = \text{Var}_{\text{VB}}(\sigma_u^2) / \text{Var}_{\text{MCMC}}(\sigma_u^2)$ is typically 0.3--0.7, far below the ideal value of 1.0.

This marks a transition from VI as validation exercise to VI as practical necessity. We can no longer easily check our work against exact posteriors. Instead, we validate through held-out prediction, posterior predictive checks, and comparison to MCMC when feasible. This mirrors how VI is actually used in practice: not as a method to approximate known posteriors, but as a tool to make intractable posteriors tractable.

\section{The Pedagogical Arc: Building Intuition}

Looking back across the two stages, a clear narrative emerges. We begin where understanding is possible (Model 1), advance to where VI becomes necessary and its limitations become visible (Model 3).

Validation becomes approximation. In Model 1, we validate VI against exact inference. In Model 3, we validate through predictive performance and limited MCMC comparisons, accepting that perfect validation is not feasible and relying on multiple indirect checks.

The mean-field independence assumption appears in both stages but with different implications. For Model 1, it is nearly exact because parameters are approximately independent posteriorly. For Model 3, it demonstrably breaks the dependence between random effects and variance components, causing systematic under-dispersion.

Computational trade-offs become apparent. Model 1 shows VI is fast even when alternatives exist. Model 3 shows VI scales to hundreds of latent variables where MCMC slows, but at the cost of underestimating uncertainty in variance components.

Across both stages, we develop the practitioner's toolkit: specifying variational families, computing ELBO gradients, monitoring convergence, diagnosing failures, and validating results. These skills transfer to any VI application, from neural network inference to spatial models.

\newpage
\section{Empirical Demonstration: Standard Deviation Ratios}

To quantify the under-dispersion phenomenon systematically, we compute standard deviation ratios comparing variational posteriors against MCMC baselines across all variance components in our models. The standard deviation ratio is defined as
\[
  \text{SD Ratio} = \frac{\text{SD}_{\text{VB}}(\theta)}{\text{SD}_{\text{MCMC}}(\theta)},
\]
where values below 1.0 indicate under-dispersion (VB is too confident), values near 1.0 indicate good agreement, and values above 1.0 would indicate over-dispersion (rare in practice).

Figure~\ref{fig:sd-comparison-plot} presents SD ratios across Models 1--3 and different sample sizes. For Model 1 (linear regression), the ratios for $\sigma$ cluster around 0.8--0.95, reflecting mild under-dispersion that is typical even in simple settings. For Model 3 (hierarchical logistic), the variance component $\sigma_u$ exhibits severe under-dispersion with ratios of 0.4--0.6, confirming that mean-field approximations systematically underestimate uncertainty in hyper-parameters.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{../figs/comparison_sd_ratios_plot.png}
  \caption{Standard deviation ratios (VB/MCMC) across models and sample sizes. Ratios below 1.0 indicate under-dispersion. Model 3 variance components show severe under-dispersion (0.4--0.6), whilst Model 1 parameters show mild under-dispersion (0.8--0.95).}
  \label{fig:sd-comparison-plot}
\end{figure}

\newpage
Figure~\ref{fig:sd-comparison-heatmap} provides an alternative visualisation using a heatmap structure, making it easier to identify patterns across parameter types and models. The colour gradient emphasises the magnitude of under-dispersion: darker cells indicate more severe under-estimation. This view highlights that variance components (bottom rows) consistently exhibit the worst performance, whilst regression coefficients (top rows) remain relatively well-calibrated.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{../figs/comparison_sd_ratios_heatmap.png}
  \caption{Heatmap of standard deviation ratios. Darker colours indicate stronger under-dispersion. Variance components show consistently poor performance across all sample sizes.}
  \label{fig:sd-comparison-heatmap}
\end{figure}

\newpage
Table~\ref{tab:sd-comparison} presents the numerical values underlying these visualisations, allowing precise assessment of the under-dispersion magnitude. The table structure groups parameters by model and sample size, facilitating comparison across settings.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.95\textwidth]{../figs/comparison_sd_ratios_table.png}
  \caption{Numerical table of standard deviation ratios. Values below 0.8 (shown in bold in the source data) indicate problematic under-dispersion requiring caution in uncertainty quantification.}
  \label{tab:sd-comparison}
\end{figure}

These diagnostics confirm the theoretical prediction: mean-field variational inference systematically under-estimates uncertainty for variance components in hierarchical models. This under-dispersion is not an artefact of poor optimisation or inadequate convergence—the ELBO has converged, and the approximate posteriors are optimal within the mean-field family. Rather, it is a fundamental consequence of the independence assumption imposed by factorisation.

The practical implication is clear: when using mean-field VI for hierarchical models, posterior standard deviations for variance components should be interpreted with caution. Predictive means may remain accurate, but credible intervals will be too narrow, leading to overconfident inference about population-level parameters.

\newpage

\section{Aggregated Reliability Assessment}

\subsection{Composite SD Ratio Metrics}

To assess overall model reliability, we summarise individual parameter SD ratios via two complementary aggregation approaches:

\paragraph{Harmonic Mean Aggregation (Product-Based).}
The harmonic mean of SD ratios across parameters provides a conservative summary:
\[
  \text{Harmonic Mean} = \frac{n}{\sum_{i=1}^{n} 1/r_i},
\]
where $r_i$ is the SD ratio for parameter $i$. This metric emphasises poor-performing parameters (ratios far below 1.0), reflecting the principle that a model's reliability is limited by its weakest component. A single parameter with very low reliability (e.g., $r = 0.40$) substantially reduces the harmonic mean, signalling overall model unreliability.

\paragraph{Weighted Mean Aggregation (Importance-Based).}
A weighted average of SD ratios groups parameters by type (location, scale, hyper-parameter) and assigns weights reflecting inferential priority:
\[
  \text{Weighted Mean} = \sum_{j \in \text{types}} w_j \cdot \overline{r}_j,
\]
where $\overline{r}_j$ is the mean SD ratio for parameter type $j$ and $w_j$ is its weight. In hierarchical models, typical weights are: fixed effects ($w_{\beta} = 0.40$), observation variance ($w_{\sigma_e} = 0.30$), and variance components ($w_{\sigma_u} = 0.30$). This reflects that all three contribute to inference quality, but different applications prioritise them differently.

\paragraph{Model-Level Aggregation.}
Aggregating across all parameters within a model yields an overall assessment. For Model 1 (linear regression), harmonic means typically fall in the range 0.85--0.92, indicating that VB performs uniformly well. For Model 3 (hierarchical logistic), harmonic means fall in the range 0.50--0.68, indicating that whilst some parameters are well-estimated, the poor performance on variance components brings down the overall score substantially.

The intuition is clear: if even one critical parameter is severely under-dispersed, the model's posterior is unreliable for inference tasks that depend on that parameter. The harmonic mean captures this conservative principle: overconfident variance components compromise the entire analysis, regardless of how well location parameters are estimated.

\section{Conclusion}

\subsection{Summary of Main Findings}

This paper has provided a pedagogical journey through mean-field variational Bayes, demonstrating both its power and its fundamental limitations. By progressing from simple linear regression (where exact solutions exist and can serve as validation benchmarks) through to hierarchical random-intercept models (where variance components exhibit severe under-dispersion), we have built intuition about when VI is reliable and when it is not.

The core finding is straightforward but consequential: mean-field VI exhibits systematic under-dispersion for variance components in hierarchical models. Quantified via standard deviation ratios, this under-dispersion is not a failure of optimisation but a predictable consequence of the factorisation assumption. When we impose $q(\mathbf{u}, \sigma_u^2) = q(\mathbf{u})\,q(\sigma_u^2)$, we break the posterior dependence between random effects and their variance component. During coordinate ascent optimisation, each factor loses information about the joint uncertainty, and $q(\sigma_u^2)$ systematically underestimates the true posterior variance.

\paragraph{Model 1 (Linear Regression).}
Mean-field VI performs exceptionally well. Location parameters ($\beta$) have SD ratios 0.90--0.95; observation variance ($\sigma^2$) has SD ratios 0.80--0.85. This excellent performance reflects two factors: (i) weak posterior correlations between fixed effects and variance, and (ii) a conjugate likelihood and priors that permit exact coordinate ascent updates. Practitioners can use VI for Model 1 with confidence, treating it as a fast alternative to exact Bayesian inference or MCMC.

\paragraph{Model 3 (Hierarchical Logistic).}
Mean-field VI reveals its limitations. Fixed effects remain moderately reliable (SD ratios 0.85--0.90), but variance components are severely under-dispersed (SD ratios 0.40--0.70). The non-conjugate logistic likelihood compounds the problem, forcing the use of approximations (Laplace or black-box gradients) to the coordinate ascent updates, which adds additional error on top of the under-dispersion from factorisation. The practical implication is stark: for hierarchical logistic models, posterior intervals for variance components should be multiplied by a factor of 1.4--2.5 to recover approximate credible intervals with the same coverage as MCMC.

\paragraph{Pedagogical Value.}
The contrast between Models 1 and 3 serves a broader pedagogical purpose. Learning VI from textbooks often emphasises derivations and elegance of the ELBO and coordinate ascent mechanics, leaving practitioners with the impression that VI is reliable whenever the ELBO converges and the Kullback--Leibler divergence is small. This paper demonstrates that convergence and ELBO optimisation are necessary but not sufficient. A low ELBO value is possible even when the variational family is severely restrictive; mean-field factorisation achieves a local optimum that may still be far from the true posterior in directions of interest. Specifically, variance—the quantity often most important for uncertainty quantification—is systematically underestimated.

\subsection{Practical Implications and Recommendations}

For practitioners considering mean-field VI for hierarchical models, three recommendations emerge:

\paragraph{1. Understand Your Model's Structure.}
Know where variance components appear. In Model 3, $\sigma_u^2$ is a hyper-parameter governing the prior on random effects. This hierarchical dependence creates strong posterior correlations that mean-field factorisation cannot preserve. If your model has this structure, expect under-dispersion.

\paragraph{2. Validate Against MCMC.}
When variance component estimation is critical, run both VI and MCMC (e.g., Stan with NUTS sampling) on a subset of your data or on a smaller version of your problem. Compare posterior intervals. If VI intervals are consistently narrower (SD ratios substantially below 1.0), adjust your credible intervals upward or fall back to MCMC for final inference.

\paragraph{3. Use Aggregated Reliability Metrics.}
Rather than trusting a single SD ratio, compute harmonic mean aggregations. This conservative summary forces you to confront the weakest-performing parameters. If the harmonic mean is below 0.75, seriously question whether the speed-accuracy trade-off favours VI for your application.

\subsection{Limitations and Future Work}

This paper has focused on mean-field VI, the most restrictive common choice of variational family. Several natural extensions warrant investigation:

\paragraph{Structured Mean-Field.}
Blocking parameters into groups that preserve some dependencies (e.g., $q(\mathbf{u}, \sigma_u^2)\,q(\mathbf{\beta})\,q(\sigma^2)$) could alleviate under-dispersion of variance components whilst maintaining computational tractability. Preliminary work suggests such structured approximations can improve SD ratios for variance components from 0.40--0.70 to 0.70--0.90, at modest computational cost.

\paragraph{Full-Form Variational Inference.}
Allowing a rich covariance structure in $q(\theta)$ would preserve posterior correlations but would require either expensive gradient-based optimisation or approximations to the Hessian. Whether the additional computational cost justifies the improved accuracy remains an open question for practitioners.

\paragraph{Laplace Approximations and Hybrid Methods.}
Using Laplace's method or tempering-based schemes to warm-start VI might lead to better initialisation and faster convergence. Hybrid methods that use VI for fast exploration and MCMC for final refinement could offer an attractive middle ground.

\paragraph{Diagnostics and Adaptive Correction.}
Developing automated diagnostics that detect under-dispersion from ELBO and posterior geometry alone (without MCMC reference) would help practitioners diagnose problematic VI solutions in the absence of ground truth. Empirical methods to post-hoc inflate variance estimates could also improve coverage without requiring full recomputation.

\subsection{Concluding Remarks}

Variational inference has transformed the landscape of approximate Bayesian inference, enabling scalable inference in models previously intractable with MCMC. However, this scalability comes at a price: systematic under-estimation of uncertainty for certain parameter classes, particularly variance components in hierarchical models.

This paper has aimed to demystify that price. By working through concrete examples, providing reproducible code, and comparing directly against MCMC gold standards, we have shown that the under-dispersion phenomenon is not mysterious or unexpected—it is a natural and inevitable consequence of mean-field factorisation in models with hierarchical structure. Understanding this is essential for responsible practice.

Practitioners adopting variational methods should view this paper as a cautionary guide rather than a roadblock. VI remains a powerful tool for exploratory analysis, rapid model comparison, and obtaining reasonable point estimates and marginal posteriors. However, for final inference about population-level parameters in hierarchical models, particularly when variance components are of direct interest, a more conservative approach is warranted: either use MCMC, or use VI with explicit acknowledgement of the under-dispersion bias and corresponding adjustment to credible intervals.

The detailed worked examples and reproducible implementations provided here represent a foundation for teaching and practice. Each model file documents not just correct code but also common pitfalls, diagnostic checks, and iterative refinement strategies. By studying these examples, researchers new to variational methods can build both theoretical understanding and practical competence—understanding when VI is safe to use and when to step back and invest in more expensive alternatives.

\begin{thebibliography}{9}

\bibitem{Blei_Columbia}
Blei, D. M.
Scaling and generalising approximate Bayesian inference.
Columbia University presentation (keynote video).

\bibitem{BleiMohamedRanganath2016_VI_Tutorial}
Blei, D. M., Kucukelbir, A., \& McAuliffe, J. D. (2017).
Variational inference: A review for statisticians.
\textit{Journal of the American Statistical Association}, 112(518), 859--877.

\end{thebibliography}

\end{document}

This progression through linear regression and hierarchical models provides a grounded introduction to mean-field variational inference. By starting with problems where we can validate exactly, then advancing through scenarios that reveal the method's systematic limitations, we build both theoretical understanding and practical competence.

The files documented here represent not just working implementations but a learning path. Each model reveals new aspects of VI: its optimisation foundations, its approximation quality, its computational advantages, and its limitations. The under-dispersion of variance components in hierarchical models is not a failure of the method but a predictable consequence of the factorisation assumption. Understanding when this bias is acceptable and when it is problematic is essential to responsible use of VI.

For graduate students and researchers entering this field, working through these examples provides essential preparation. The progression is deliberate: master the mechanics in a forgiving setting, understand the limitations in a realistic context. This is how VI proficiency is built, one model at a time.

\begin{thebibliography}{9}

\bibitem{Blei_Columbia}
Blei, D. M.
Scaling and generalising approximate Bayesian inference.
Columbia University presentation (keynote video).

\bibitem{BleiMohamedRanganath2016_VI_Tutorial}
Blei, D. M., Kucukelbir, A., \& McAuliffe, J. D. (2017).
Variational inference: A review for statisticians.
\textit{Journal of the American Statistical Association}, 112(518), 859--877.

\end{thebibliography}

\end{document}
