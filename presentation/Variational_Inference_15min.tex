% Compile with: xelatex
\documentclass{beamer}
\usetheme{Madrid}
\usecolortheme{default}

\usepackage{fontspec}
\setmainfont{TeX Gyre Heros}

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{xcolor}

% Define light blue background color
\definecolor{lightblue}{RGB}{200,220,240}

\title{Scaling and Generalising Approximate Bayesian Inference}
\subtitle{A 15-minute introduction to Variational Inference}
\author{Based on David Blei's keynote}
\date{}

\setbeamertemplate{navigation symbols}{}

\begin{document}

\frame{\titlepage}

% Slide 1: Bayes Theorem
\begin{frame}
\frametitle{Bayesian Inference: Bayes' Theorem}

\begin{center}
\includegraphics[width=0.85\textwidth]{Bayesian.png}
\end{center}

\end{frame}

% Slide 2: Bayesian vs Variational
\begin{frame}
\frametitle{Bayesian Inference vs Variational Inference}

\begin{center}
\includegraphics[width=0.85\textwidth]{BI and VI.png}
\end{center}

\end{frame}

% Slide 3: Q-space Optimisation (Visual) with Key Definitions
\begin{frame}
\frametitle{Variational Inference: Finding $q_{\text{opt}}$}

\begin{columns}[T]
\begin{column}{0.55\textwidth}
\includegraphics[width=\textwidth]{q-space-visual.png}
\end{column}
\begin{column}{0.43\textwidth}
\footnotesize
\textbf{Key Definitions:}
\begin{itemize}
  \item[$\bullet$] \textcolor{red}{$Q$}: Space of all possible approximate distributions
  \item[$\bullet$] \textcolor{blue}{$q_{\text{init}}$}: Initial guess for approximate posterior
  \item[$\bullet$] \textcolor{blue}{$q_{\text{opt}}$}: Optimal approximate posterior (maxIBO)
  \item[$\bullet$] \textcolor{green}{$p(z|x)$}: True posterior
  \item[$\bullet$] \textcolor{green}{$*$}: $KL(q_{\text{opt}} \| p(z|x))$ — remaining divergence
\end{itemize}
\end{column}
\end{columns}

\vspace{0.3cm}

The goal is to find the best approximate posterior $q_{\text{opt}}$ within the family $Q$ that minimises the KL divergence to the true posterior $p(z|x)$.

\end{frame}

% Slide 4: Q-space Definitions (REMOVED - merged with Slide 3)
% \begin{frame}
% \frametitle{Variational Inference: Key Definitions}
% 
% \begin{center}
% \includegraphics[width=0.85\textwidth]{q-space-bullets.png}
% \end{center}
% 
% \end{frame}

% Slide 5: Conditionally Conjugate Models
\begin{frame}
\frametitle{Conditionally Conjugate Models}

\begin{center}
\includegraphics[width=0.7\textwidth]{Models that can use the score gradient.png}
\end{center}

\vspace{0.5cm}

To formalise variational inference, David Blei introduces a broad class of probabilistic models known as \textbf{conditionally conjugate models}. These models distinguish between two types of latent variables: \textbf{global variables} (influencing the entire dataset) and \textbf{local variables} (specific to individual data points).

\end{frame}

% Slide 6: Exponential Families
\begin{frame}
\frametitle{Exponential Family Structure}
\setbeamercolor{background canvas}{bg=lightblue}

In conditionally conjugate models, the complete conditional of each latent variable belongs to an \textbf{exponential family} of distributions.

\vspace{0.5cm}

This framework encompasses a wide range of models:

\begin{itemize}
  \item Bayesian mixture models
  \item Dirichlet process mixtures
  \item Multilevel regression models
  \item Stochastic block models
  \item Matrix factorisation models
  \item Topic models (Latent Dirichlet Allocation)
\end{itemize}

\vspace{0.5cm}

\textit{Many influential models from the 1990s and early 2000s fall into this class.}

\end{frame}

% Slide 7: Three Fundamental Models (M1, M2, M3 Comparison)
\begin{frame}
\frametitle{Three Fundamental Models}
\setbeamercolor{background canvas}{bg=lightblue}

\begin{columns}[t]
\begin{column}{0.31\textwidth}
\centering
\textbf{Linear Regression (M1)}

\vspace{0.3cm}

\small
\begin{align*}
y_i &\sim N(x_i^T\beta, \tau_e^{-1}) \\
\beta &\sim N(0, \Gamma_\beta) \\
\tau_e &\sim \text{Gamma}(\alpha_e, \beta_e)
\end{align*}

\vspace{0.2cm}

\textit{No random effects}
\end{column}

\begin{column}{0.31\textwidth}
\centering
\textbf{Hierarchical Linear (M2)}

\vspace{0.3cm}

\small
\begin{align*}
y_{ij} &\sim N(x_{ij}^T\beta + u_i, \tau_e^{-1}) \\
\beta &\sim N(0, \Gamma_\beta) \\
u_i &\sim N(0, \tau_u^{-1}) \\
\tau_e &\sim \text{Gamma}(\alpha_e, \beta_e) \\
\tau_u &\sim \text{Gamma}(\alpha_u, \beta_u)
\end{align*}

\vspace{0.2cm}

\textit{Random intercepts}
\end{column}

\begin{column}{0.31\textwidth}
\centering
\textbf{Hierarchical Logistic (M3)}

\vspace{0.3cm}

\small
\begin{align*}
y_{ij} &\sim \text{Bernoulli}(\text{logit}^{-1}(x_{ij}^T\beta + u_i)) \\
\beta &\sim N(0, \Gamma_\beta) \\
u_i &\sim N(0, \tau_u^{-1}) \\
\tau_u &\sim \text{Gamma}(\alpha_u, \beta_u)
\end{align*}

\vspace{0.2cm}

\textit{Random intercepts, binary outcomes}
\end{column}
\end{columns}

\vspace{0.5cm}

\footnotesize
\textbf{Challenge:} How to handle the joint posterior over $(\beta, u, \tau_u, \tau_e)$ when full coupling makes closed-form updates intractable?

\end{frame}

% Slide 8: Why Factorisation? (M2 Deep-Dive)
\begin{frame}
\frametitle{Why Factorisation? (M2 Hierarchical Linear)}
\setbeamercolor{background canvas}{bg=lightblue}

\textbf{Full joint posterior} couples everything: $p(\beta, u, \tau_u, \tau_e \mid y) \propto p(y \mid \beta, u, \tau_e) \cdot p(u \mid \tau_u) \cdot p(\beta) \cdot p(\tau_u) \cdot p(\tau_e)$

\vspace{0.3cm}

\begin{itemize}
  \item Dense precision matrix — no closed-form updates
  \item \textbf{Mean-field choice} to regain tractability:
  \begin{itemize}
    \item $q(\beta, u)$ together (they share the precision term)
    \item $q(\tau_u)$ separate
    \item $q(\tau_e)$ separate
  \end{itemize}
  \vspace{0.3cm}
  \item \textbf{Result:} Closed-form Gamma updates for $\{\tau_u, \tau_e\}$; Gaussian update for $\{(\beta, u)\}$
\end{itemize}

\vspace{0.3cm}

\textbf{Factorisation we use:}
\[
q(\beta, u, \tau_u, \tau_e) = q(\beta, u) \cdot q(\tau_u) \cdot q(\tau_e)
\]

\vspace{0.3cm}

\footnotesize
This positions the equations as \textbf{solutions to an intractability problem}, not arbitrary choices.

\end{frame}

% Slide 9: M2 Grouping Strategy
\begin{frame}
\frametitle{M2 Grouping Strategy}
\setbeamercolor{background canvas}{bg=lightblue}

\textbf{Mean-field grouping:} $q(\beta, u) \cdot q(\tau_u) \cdot q(\tau_e)$

\vspace{0.5cm}

\textbf{Question:} Should $\tau_u$ and $\tau_e$ be grouped together or separate?

\vspace{0.5cm}

\textbf{Answer:} \textcolor{red}{Separate} enables closed-form updates

\vspace{0.5cm}

When separated:
\begin{itemize}
  \item $q(\tau_e)$ is Gamma (conjugate to normal likelihood)
  \item $q(\tau_u)$ is Gamma (conjugate to normal prior on $u$)
  \item $q(\beta, u)$ is Multivariate Normal
\end{itemize}

\vspace{0.5cm}

\textbf{Result:} Coordinate ascent with closed-form updates at each iteration

\end{frame}

% Slide 10: M2 Update Equations
\begin{frame}[fragile]
\frametitle{M2 Update Equations}
\setbeamercolor{background canvas}{bg=lightblue}

\textbf{Coordinate ascent updates (from Dr. John's code):}

\vspace{0.3cm}

\small
\textbf{1. Update $q(\beta, u)$:}
\[
\Sigma_{\beta u}^{\text{new}} = \left( X^T X \mathbb{E}[\tau_e] + \text{diag}(0_\beta, \mathbb{E}[\tau_u] \cdot \mathbf{1}_u) + \Gamma_\beta^{-1} \right)^{-1}
\]
\[
\mu_{\beta u}^{\text{new}} = \Sigma_{\beta u}^{\text{new}} \left( X^T y \mathbb{E}[\tau_e] \right)
\]

\vspace{0.3cm}

\textbf{2. Update $q(\tau_e)$:}
\[
a_e^{\text{new}} = a_e + \frac{n}{2}, \quad b_e^{\text{new}} = b_e + \frac{1}{2} \mathbb{E}[(y - X\beta - Zu)^T(y - X\beta - Zu)]
\]

\vspace{0.3cm}

\textbf{3. Update $q(\tau_u)$:}
\[
a_u^{\text{new}} = a_u + \frac{Q}{2}, \quad b_u^{\text{new}} = b_u + \frac{1}{2} \mathbb{E}[u^T u]
\]

\vspace{0.3cm}

\footnotesize
Expectations computed using current variational parameters. Iterate until ELBO convergence.

\end{frame}

% Slide 11: Mean-Field Variational Inference
\begin{frame}
\frametitle{Mean-Field Variational Inference}
\setbeamercolor{background canvas}{bg=lightblue}

In mean-field variational inference, the variational distribution \textbf{factorises completely} across latent variables:

\[
q(z) = \prod_i q_i(z_i)
\]

\vspace{0.5cm}

Each latent variable is governed by its own independent variational factor with its own parameters.

\vspace{0.5cm}

\textbf{Trade-off:} This independence assumption is restrictive, but it enables \textbf{tractable optimisation}.

\vspace{0.5cm}

The variational parameters are optimised by maximising the \textbf{Evidence Lower Bound (ELBO)}, which is equivalent to minimising the KL divergence to the true posterior.

\end{frame}

% Slide 12: Coordinate Ascent Algorithm
\begin{frame}
\frametitle{Optimisation: Coordinate Ascent}

Classical variational inference optimises the ELBO using \textbf{coordinate ascent}:

\vspace{0.5cm}

\begin{enumerate}
  \item Initialise variational parameters
  \vspace{0.3cm}
  \item For each iteration:
  \begin{itemize}
    \item Update each variational parameter in turn
    \item Hold all others fixed
    \item Use expectations of complete conditionals
  \end{itemize}
  \vspace{0.3cm}
  \item Repeat until convergence
\end{enumerate}

\vspace{0.5cm}

\textbf{Key insight:} For conditionally conjugate models, these updates have closed-form expressions.

\vspace{0.5cm}

\textit{Resembles Gibbs sampling, but deterministic optimisation instead of stochastic sampling.}

\end{frame}

% Slide 13: Latent Dirichlet Allocation
\begin{frame}
\frametitle{Example: Latent Dirichlet Allocation (LDA)}

Topic modelling in large document collections.

\vspace{0.5cm}

\begin{itemize}
  \item \textbf{Global variables:} Topics (distributions over words)
  \vspace{0.3cm}
  \item \textbf{Local variables:} Per-document topic proportions and word-topic assignments
  \vspace{0.3cm}
  \item \textbf{Observations:} Words in each document
\end{itemize}

\vspace{0.5cm}

\textbf{Inference goal:} Discover hidden topics from a large collection of unlabelled documents.

\vspace{0.5cm}

Mean-field variational inference is tractable and effective for moderately-sized datasets (tens of thousands of documents).

\end{frame}

% Slide 10: Stochastic Variational Inference
\begin{frame}
\frametitle{Scaling Up: Stochastic Variational Inference}

Early applications were limited by computational inefficiency: each iteration requires processing every data point.

\vspace{0.5cm}

\textbf{Stochastic variational inference} addresses this by combining variational inference with stochastic optimisation:

\vspace{0.5cm}

\begin{itemize}
  \item Subsample data points (minibatches) at each iteration
  \vspace{0.3cm}
  \item Construct noisy but unbiased gradient estimates
  \vspace{0.3cm}
  \item Update global parameters using natural gradients
  \vspace{0.3cm}
  \item Converges to local optima with properly decreasing step sizes
\end{itemize}

\vspace{0.5cm}

This enables inference on massive datasets (millions of documents).

\end{frame}

% Slide 15: Natural Gradients
\begin{frame}
\frametitle{Natural Gradients: Geometric Insight}

Natural gradients account for the geometric structure of probability distributions.

\vspace{0.5cm}

In exponential family models, the natural gradient has a particularly simple form:

\[
\text{natural gradient} = \text{(coordinate ascent update)} - \text{(current parameter)}
\]

\vspace{0.5cm}

\textbf{Why it matters:}

\begin{itemize}
  \item Combines scalability of stochastic optimisation
  \item Retains stability and interpretability of mean-field updates
  \item Unbiased gradient estimates even with subsampled data
\end{itemize}

\end{frame}

% Slide 16: Under-Dispersion Analysis - SD Ratios Table
\begin{frame}
\frametitle{VB Under-Dispersion: SD Ratios Across Models}

\begin{center}
\includegraphics[width=0.95\textwidth]{comparison_sd_ratios_table.png}
\end{center}

\vspace{0.3cm}

\small
SD Ratio = VB\_SD / Gibbs\_SD. Values < 1.0 indicate VB under-dispersion (uncertainty underestimated).

\end{frame}

% Slide 17: Under-Dispersion Patterns - Heatmap
\begin{frame}
\frametitle{Under-Dispersion Patterns by Parameter and Model}

\begin{center}
\includegraphics[width=0.95\textwidth]{comparison_sd_ratios_heatmap.png}
\end{center}

\vspace{0.3cm}

\small
Note severe under-dispersion for variance components ($\tau_u$), especially in hierarchical models (M0, M2, M3) with increasing Q.

\end{frame}

% Slide 18: Summary
\begin{frame}
\frametitle{Summary: Variational Inference for Bayesian Inference}

\begin{enumerate}
  \item \textbf{Problem:} Computing exact posteriors is often intractable
  \vspace{0.5cm}
  
  \item \textbf{Solution:} Reframe as an optimisation problem
  \begin{itemize}
    \item Choose a tractable family $Q$ of distributions
    \item Find $q \in Q$ closest to true posterior $p(z|x)$
    \item Maximise the ELBO
  \end{itemize}
  \vspace{0.5cm}
  
  \item \textbf{Scalability:} Stochastic variational inference enables massive datasets
  \vspace{0.5cm}
  
  \item \textbf{Generality:} Applies to conditionally conjugate models (a broad class)
\end{enumerate}

\vspace{1cm}

\textbf{Result:} A scalable and flexible approach to Bayesian inference.

\end{frame}

\end{document}
