% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\documentclass[
]{article}
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\usepackage{tcolorbox}
\usepackage{placeins}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={On Variational Bayesian Methods},
  pdfauthor={David Ewing},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{On Variational Bayesian Methods}
\author{David Ewing}
\date{2026-01-27}

\begin{document}
\maketitle

\section{Slide 1: Bayes' Theorem}\label{slide-1-bayes-theorem}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{Bayesian} 

}

\caption{Bayes' theorem : prior, likelihood, posterior, and evidence.}\label{fig:slide1-bayes}
\end{figure}

\emph{(Silent slide while I make my way to the podium)}

\emph{note: P(A) in the numerator needs to be fixed.} \newpage

\section{Slide 2: Bayesian vs Variational
Inference}\label{slide-2-bayesian-vs-variational-inference}

\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{BI and VI} 

}

\caption{Exact Bayesian inference versus variational inference: sampling vs optimisation.}\label{fig:slide2-bayesian-vs-vi}
\end{figure}

At the podium: Good afternoon. My name is David Ewing, and I'm with the
MADS programme. My presentation is on Variational Bayes Methods, also
known as Variational Inference. Bayes' Rule gives us the exact posterior
--- assuming we can compute it. But in practice, especially with complex
models, that denominator --- p(B) or p(x) --- becomes intractable. This
is where variational inference comes in. Rather than computing the true
posterior directly, we approximate it --- by reframing inference as an
optimisation problem. Let me show you what that looks like
geometrically.

\emph{note: P(A) in the numerator needs to be fixed.}

\newpage 
\FloatBarrier

\section{Slide 3: Finding the Optimal q in
Q-space}\label{slide-3-finding-the-optimal-q-in-q-space}

\subsubsection{Visualising the variational
family}\label{visualising-the-variational-family}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{q-space-visual-fixed} 

}

\caption{Visualising the search for $q_{\text{opt}}$ within the variational family $Q$.}\label{fig:slide3-qspace-visual}
\end{figure}

``This diagram, adapted from Dr David Blei of Columbia University, shows
the core idea behind variational inference. We begin with a space of
candidate distributions --- that's the red region labelled Q. Inside it,
we choose an initial guess --- q\_init --- and then optimise it to find
q\_opt, the best approximation we can achieve within that space. The
true posterior --- p(zx) --- lies outside this space. It's what we'd
ideally compute, but often cannot. The green asterisk marks the
remaining divergence --- the KL divergence between our best
approximation and the true posterior. In essence, variational inference
is about choosing a tractable family of distributions, and then finding
the member of that family that gets us as close as possible to the
truth. It's not exact, but it's fast, scalable, and surprisingly
effective --- especially in high-dimensional models.'' \newpage
\FloatBarrier

\section{Slide 6: Coordinate Ascent
VI}\label{slide-6-coordinate-ascent-vi}

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{coordinate_ascent} 

}

\caption{Coordinate ascent variational inference: iterate factor updates to maximise the ELBO.}\label{fig:slide6-coordinate-ascent}
\end{figure}

Coordinate ascent provides a simple and intuitive optimisation strategy.
By updating one component at a time whilst holding the others fixed, a
complex optimisation problem is broken into simpler steps. Under certain
factorisation assumptions, these updates can be derived in closed form.
This idea underlies many classical variational inference algorithms and
explains their computational efficiency. \textbar{} This slide
illustrates the optimisation strategy behind variational inference in
conditionally conjugate models. We use coordinate ascent: we cycle
through each variational factor in turn, updating it while holding the
others fixed. Each update improves the ELBO --- the evidence lower bound
--- and we continue until convergence. The path shown here is schematic:
we start at an initial point, and climb step by step along each
coordinate direction until we reach the optimum. In the models we focus
on, these updates are closed-form --- thanks to conjugacy --- which
makes the procedure fast and scalable.

To orient the empirical comparisons, here is a concise overview of the
three core models used throughout the project.

To orient the empirical comparisons, this slide introduces the three
core models used throughout the project. Model 1 is a simple linear
regression --- no hierarchy, no random effects. Model 2 introduces
hierarchical structure with Gaussian random effects. Model 3 is a
hierarchical logistic model --- structurally similar to M2, but with a
Bernoulli likelihood. Each model includes regression coefficients
\beta , and where applicable, random effects u, residual precision
\tau \_e, and random-effects precision \tau \_u. This side-by-side
summary sets expectations for which variance components are present ---
and that's crucial, because under-dispersion is most severe in the
hierarchical cases, especially for \tau \_u and \tau \_e.

\newpage
\FloatBarrier

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{three_models_clean} 

}

\caption{Overview of core models (M1 linear, M2 hierarchical linear, M3 hierarchical logistic).}\label{fig:models-comparison}
\end{figure}
\FloatBarrier

This side-by-side summary sets expectations for the presence or absence
of variance components (τ\_u, τ\_e) by model, which in turn explains why
under-dispersion is most severe for hierarchical models (M2/M3).

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{why_factorisation_trim} 

}

\caption{Mean-field factorisation strategy and coordinate ascent update equations.}\label{fig:factorisation-pdf}
\end{figure}

This slide shows how we structure the variational approximation and
derive the update equations. We begin with the full joint posterior ---
which couples all parameters through the likelihood. To make inference
tractable, we impose a mean-field factorisation: we preserve the
coupling between \beta  and u, but factor out the precision parameters
\tau \_u and \tau \_e. This structure admits closed-form updates for
each factor. The regression block --- q(\beta ,u) --- is Gaussian, and
its mean and covariance are updated using expectations of the precision
parameters. The precision parameters --- q(\tau \_e) and q(\tau \_u) ---
are Gamma distributions, updated using expectations of squared residuals
and squared random effects. All expectations are computed using the
current variational parameters, and the loop repeats until the ELBO
converges.

\FloatBarrier

\subsection{Empirical Illustration: Under-dispersion in M2 Variance
Component}\label{empirical-illustration-under-dispersion-in-m2-variance-component}

Mean-field factorisation enables efficient inference through conditional
independence, but this independence induces systematic under-dispersion
in hyper-parameters. The posterior for the random-effects precision
\(\tau_u\) in Model 2 exemplifies this effect:

\begin{figure}

{\centering \includegraphics[width=0.85\linewidth]{M2_tau_u_overlay_comparison} 

}

\caption{VB vs Gibbs for $\tau_u$ in Model 2: variational posterior is too narrow (under-dispersion).}\label{fig:slide7-m2-tauu-overlay}
\end{figure}

This slide illustrates a key limitation of mean-field variational
inference: under-dispersion. We're looking at the posterior for the
random-effects precision \tau \_u in Model 2. On the left, we see Gibbs
sampling posteriors --- consistent across configurations. On the right,
we see VB posteriors --- noticeably narrower, especially when the number
of groups is small. This narrowing reflects overconfidence: the
variational posterior underestimates uncertainty. It's a direct
consequence of the mean-field assumption --- which imposes conditional
independence and breaks the coupling that would otherwise inflate
variance. The effect is systematic, and it's most severe in hierarchical
models with limited group structure.

\newpage

To demonstrate the systematic under‑dispersion of variance components in
mean‑field variational inference, we computed standard deviation ratios
comparing VB posteriors against Gibbs baselines. The ratio is simple:
\%\mathrm{SD\  Ratio}=\frac{\mathrm{SD_{VB}}(\theta )}{\mathrm{SD_{Gibbs}}(\theta )}
\%Values below 1.0 indicate under‑dispersion --- VB is too confident.
\%Values near 1.0 indicate good agreement. The table shows results
grouped by model and configuration. M1 appears first, followed by M2 and
M3 rows sorted by Q. The pattern is clear: mean‑field VB consistently
underestimates uncertainty for variance components --- especially
\tau \_u and \tau \_e in hierarchical models. This confirms the
theoretical prediction: conditional independence in the variational
approximation leads to overconfident posteriors

\newpage
\FloatBarrier

\section{Empirical Results: Standard Deviation
Ratios}\label{empirical-results-standard-deviation-ratios}

To demonstrate the systematic under-dispersion of variance components in
mean-field variational inference, we computed standard deviation ratios
comparing VB posteriors against Gibbs baselines:

\[
\text{SD Ratio} = \frac{\text{SD}_{\text{VB}}(\theta)}{\text{SD}_{\text{Gibbs}}(\theta)}
\]

Values below 1.0 indicate under-dispersion (VB too confident); values
near 1.0 indicate good agreement.

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{../figs/comparison_sd_ratios_table} 

}

\caption{Standard deviation ratios (VB / Gibbs) across all model configurations.}\label{fig:sd-table-final}
\end{figure}

This table presents the standard deviation ratios --- VB over Gibbs ---
across all model configurations. Each row corresponds to a model and a
group count Q. Each column corresponds to a parameter. Values near 1.0
indicate good agreement. Values below 1.0 indicate under‑dispersion ---
VB is too confident. The colour coding helps: - Yellow means acceptable
agreement - Orange and red signal systematic under‑dispersion - Green
indicates rare over‑dispersion The pattern is clear: - M1 behaves well
--- no hierarchy, no problem - M2 shows consistent under‑dispersion for
\tau \_u, especially at low Q - M3 shows even stronger under‑dispersion
--- especially for logistic models with few groups This confirms the
theoretical prediction: mean‑field VB systematically underestimates
uncertainty in hierarchical models, especially for variance components.

\newpage
\FloatBarrier

\section{Computational Advantage: VB vs Gibbs
Sampling}\label{computational-advantage-vb-vs-gibbs-sampling}

While variational Bayes exhibits under-dispersion for variance
components, it offers substantial computational advantages. We measured
elapsed time for VB and Gibbs sampling across three models and varying
group counts (Q) to quantify this trade-off.

\subsection{Overall Performance
Comparison}\label{overall-performance-comparison}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{timing_vb_vs_gibbs_bars} 

}

\caption{Side-by-side comparison of VB (blue) and Gibbs (red) computational times across all model configurations.}\label{fig:timing-bars}
\end{figure}

This slide shows the computational time for each method --- Gibbs
Sampling in red, Variational Bayes in blue. The message is simple: lower
is better, and VB is consistently faster. For example, in the M2\_Q100
configuration, Gibbs takes nearly 96 seconds. VB takes less than a tenth
of a second. This speed advantage holds across all models and
configurations. It's one of the main reasons VB is so widely used in
large-scale applications --- especially when exact inference is
computationally prohibitive. Of course, we've seen that this speed comes
at a cost: under-dispersion in variance components. But in many
settings, that trade-off is acceptable --- especially when scalability
is paramount.

\newpage
\FloatBarrier

\subsection{Speedup Ratios}\label{speedup-ratios}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{timing_speedup_ratios} 

}

\caption{Speedup ratios (Gibbs time / VB time) showing VB's computational advantage.}\label{fig:timing-speedup}
\end{figure}

This slide compares how computational time scales with model complexity
--- specifically, the number of groups Q. The top panel shows Gibbs
sampling: time increases steeply with Q. The bottom panel shows VB: time
increases much more gradually. This difference reflects the fundamental
contrast in computational strategy. Gibbs relies on iterative sampling
--- and that cost grows with each latent variable. VB uses closed-form
updates --- and those scale linearly with model size. In practice, this
means VB remains tractable even in large hierarchical models. It's not
just faster --- it's more scalable.

\newpage
\FloatBarrier

\subsection{Scaling with Model
Complexity}\label{scaling-with-model-complexity}

\begin{figure}

{\centering \includegraphics[width=0.95\linewidth]{timing_scaling_with_Q} 

}

\caption{Computational time scaling with number of groups (Q) — separate panels show different scales for Gibbs vs VB.}\label{fig:timing-scaling}
\end{figure}

The scaling behaviour reveals model-specific computational
characteristics:

\textbf{Model 2 (Hierarchical Linear, orange):}\\
Gibbs exhibits exponential scaling (9s → 96s as Q increases 5 → 100),
while VB remains essentially constant (≈0.1--0.2s). This demonstrates
VB's insensitivity to hierarchical complexity in linear models.

\textbf{Model 3 (Hierarchical Logistic, purple):}\\
Gibbs remains relatively stable (≈2--5s), while VB shows moderate linear
growth (0.1s → 0.6s). For logistic models, both methods scale
reasonably, though VB maintains a consistent speed advantage.

\textbf{Key Finding:} VB's computational advantage is most dramatic for
hierarchical linear models (M2) where Gibbs sampling scales poorly with
the number of groups. Overall, VB achieves \textbf{134× average speedup}
across all configurations, making it practical for large-scale
hierarchical models despite its under-dispersion limitations.

\newpage
\FloatBarrier

\end{document}
