{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7f82d5",
   "metadata": {},
   "source": [
    "# RNZ Climate Corpus Exploration\n",
    "\n",
    "Integrated analysis combining corpus structure, loading strategies, file sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d775b675",
   "metadata": {},
   "source": [
    "## Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141dc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed4ed0",
   "metadata": {},
   "source": [
    "## Environment Paths\n",
    "\n",
    "SERVER_PATH/LOCAL_PATH/BASE_PATH pattern for easy environment switching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5cc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVER_PATH = '/srv/corpora/rnz-climate.corpus'\n",
    "LOCAL_PATH  = 'D:/github/DIGI405/corpora/nzd-climate'\n",
    "BASE_PATH   = Path(LOCAL_PATH)\n",
    "\n",
    "# Output folders\n",
    "RESULTS_DIR = Path('../results')\n",
    "FIGS_DIR    = Path('../figs')\n",
    "\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "FIGS_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249335b2",
   "metadata": {},
   "source": [
    "## Corpus Structure Exploration\n",
    "\n",
    "Examine parquet files - dimensions, columns, types, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6418d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus structure saved to: ..\\results\\corpus_structure.txt\n"
     ]
    }
   ],
   "source": [
    "parquet_files = ['vocab.parquet', 'tokens.parquet', 'metadata.parquet', 'spaces.parquet', 'puncts.parquet']\n",
    "\n",
    "output_file = RESULTS_DIR / 'corpus_structure.txt'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"RNZ CLIMATE CORPUS - DATA STRUCTURE EXPLORATION\\n\\n\")\n",
    "    \n",
    "    for filename in parquet_files:\n",
    "        filepath = BASE_PATH / filename\n",
    "        \n",
    "        f.write(f\"\\nFILE: {filename}\\n\\n\")\n",
    "        \n",
    "        df = pl.read_parquet(filepath)\n",
    "        \n",
    "        f.write(f\"Dimensions: {df.shape[0]:,} rows × {df.shape[1]} columns\\n\\n\")\n",
    "        \n",
    "        f.write(\"Columns and Types:\\n\")\n",
    "        max_col_len = max(len(col) for col in df.columns)\n",
    "        for col_name, col_type in zip(df.columns, df.dtypes):\n",
    "            f.write(f\"  - {col_name:<{max_col_len}}: {col_type}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nFirst 10 Rows:\\n{df.head(10)}\\n\")\n",
    "        \n",
    "        file_size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "        f.write(f\"\\nFile Size: {file_size_mb:.2f} MB\\n\")\n",
    "    \n",
    "    f.write(\"\\nEXPLORATION COMPLETE\\n\")\n",
    "\n",
    "print(f\"Corpus structure saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "683847c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNZ CLIMATE CORPUS - DATA STRUCTURE EXPLORATION\n",
      "\n",
      "\n",
      "FILE: vocab.parquet\n",
      "\n",
      "Dimensions: 104,970 rows × 8 columns\n",
      "\n",
      "Columns and Types:\n",
      "  - rank             : UInt32\n",
      "  - tokens_sort_order: UInt32\n",
      "  - token_id         : UInt32\n",
      "  - token            : String\n",
      "  - frequency_lower  : UInt32\n",
      "  - frequency_orth   : UInt32\n",
      "  - is_punct         : Boolean\n",
      "  - is_space         : Boolean\n",
      "\n",
      "First 10 Rows:\n",
      "shape: (10, 8)\n",
      "┌──────┬────────────────┬──────────┬───────┬────────────────┬────────────────┬──────────┬──────────┐\n",
      "│ rank ┆ tokens_sort_or ┆ token_id ┆ token ┆ frequency_lowe ┆ frequency_orth ┆ is_punct ┆ is_space │\n",
      "│ ---  ┆ der            ┆ ---      ┆ ---   ┆ r              ┆ ---            ┆ ---      ┆ ---      │\n",
      "│ u32  ┆ ---            ┆ u32      ┆ str   ┆ ---            ┆ u32            ┆ bool     ┆ bool     │\n",
      "│      ┆ u32            ┆          ┆       ┆ u32            ┆                ┆          ┆          │\n",
      "╞══════╪════════════════╪══════════╪═══════╪════════════════╪════════════════╪══════════╪══════════╡\n",
      "│ 1    ┆ 92162          ┆ 42385    ┆ the   ┆ 341937         ┆ 300789         ┆ false    ┆ false    │\n",
      "│ 2    ┆ 278            ┆ 71740    ┆ .     ┆ 240692         ┆ 240692         ┆ true     ┆ false    │\n",
      "│ 3    ┆ 55             ┆ 14989    ┆ ,     ┆ 239056         ┆ 239056         ┆ true     ┆ false    │\n",
      "│ 4    ┆ 93145          ┆ 21767    ┆ to    ┆ 174873         ┆ 174069         ┆ false    ┆ false    │\n",
      "│ 5    ┆ 9118           ┆ 13213    ┆ and   ┆ 150226         ┆ 146950         ┆ false    ┆ false    │\n",
      "│ 6    ┆ 66316          ┆ 5148     ┆ of    ┆ 141686         ┆ 141365         ┆ false    ┆ false    │\n",
      "│ 7    ┆ 3              ┆ 90170    ┆ \"     ┆ 121351         ┆ 121351         ┆ true     ┆ false    │\n",
      "│ 8    ┆ 5932           ┆ 67567    ┆ a     ┆ 109109         ┆ 103377         ┆ false    ┆ false    │\n",
      "│ 9    ┆ 44980          ┆ 17306    ┆ in    ┆ 106063         ┆ 99754          ┆ false    ┆ false    │\n",
      "│ 10   ┆ 56             ┆ 52036    ┆ -     ┆ 64662          ┆ 64662          ┆ true     ┆ false    │\n",
      "└──────┴────────────────┴──────────┴───────┴────────────────┴────────────────┴──────────┴──────────┘\n",
      "\n",
      "File Size: 1.36 MB\n",
      "\n",
      "FILE: tokens.parquet\n",
      "\n",
      "Dimensions: 6,387,921 rows × 4 columns\n",
      "\n",
      "Columns and Types:\n",
      "  - orth_index     : UInt32\n",
      "  - lower_index    : UInt32\n",
      "  - token2doc_index: Int32\n",
      "  - has_spaces     : Boolean\n",
      "\n",
      "First 10 Rows:\n",
      "shape: (10, 4)\n",
      "┌────────────┬─────────────┬─────────────────┬────────────┐\n",
      "│ orth_index ┆ lower_index ┆ token2doc_index ┆ has_spaces │\n",
      "│ ---        ┆ ---         ┆ ---             ┆ ---        │\n",
      "│ u32        ┆ u32         ┆ i32             ┆ bool       │\n",
      "╞════════════╪═════════════╪═════════════════╪════════════╡\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "│ 86796      ┆ 86796       ┆ -1              ┆ false      │\n",
      "└────────────┴─────────────┴─────────────────┴────────────┘\n",
      "\n",
      "File Size: 21.04 MB\n",
      "\n",
      "FILE: metadata.parquet\n",
      "\n",
      "Dimensions: 11,052 rows × 5 columns\n",
      "\n",
      "Columns and Types:\n",
      "  - source_url: String\n",
      "  - title     : String\n",
      "  - date      : String\n",
      "  - year      : Int64\n",
      "  - category  : String\n",
      "\n",
      "First 10 Rows:\n",
      "shape: (10, 5)\n",
      "┌───────────────────────────┬──────────────────────────┬─────────────────────────┬──────┬──────────┐\n",
      "│ source_url                ┆ title                    ┆ date                    ┆ year ┆ category │\n",
      "│ ---                       ┆ ---                      ┆ ---                     ┆ ---  ┆ ---      │\n",
      "│ str                       ┆ str                      ┆ str                     ┆ i64  ┆ str      │\n",
      "╞═══════════════════════════╪══════════════════════════╪═════════════════════════╪══════╪══════════╡\n",
      "│ http://api.digitalnz.org/ ┆ Fonterra fears oil, gas  ┆ 2019-05-21 21:02:03 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ explor…                  ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ China promising more     ┆ 2019-05-21 15:28:50 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ than deli…               ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ EU election could change ┆ 2019-05-26 22:39:06 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ the b…                   ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ Cook Islands and NZ      ┆ 2019-05-27 18:39:41 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ ministers …              ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ West Coast clean up      ┆ 2019-05-28 09:11:07 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ proves too…              ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ 2019 Australia election: ┆ 2019-05-22 05:54:50 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ Liber…                   ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ Malaysia begins sending  ┆ 2019-05-22 15:47:57 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ scrap …                  ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ Vanuatu PM pleased with  ┆ 2019-05-23 10:54:42 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ NZ cli…                  ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ Forum puts spotlight on  ┆ 2019-05-23 13:31:45 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ housin…                  ┆                         ┆      ┆          │\n",
      "│ http://api.digitalnz.org/ ┆ Students' laid-back      ┆ 2019-05-24 10:30:58 UTC ┆ 2019 ┆ Articles │\n",
      "│ recor…                    ┆ preparatio…              ┆                         ┆      ┆          │\n",
      "└───────────────────────────┴──────────────────────────┴─────────────────────────┴──────┴──────────┘\n",
      "\n",
      "File Size: 0.36 MB\n",
      "\n",
      "FILE: spaces.parquet\n",
      "\n",
      "Dimensions: 136 rows × 5 columns\n",
      "\n",
      "Columns and Types:\n",
      "  - position       : UInt32\n",
      "  - orth_index     : UInt32\n",
      "  - lower_index    : UInt32\n",
      "  - token2doc_index: Int32\n",
      "  - has_spaces     : Boolean\n",
      "\n",
      "First 10 Rows:\n",
      "shape: (10, 5)\n",
      "┌──────────┬────────────┬─────────────┬─────────────────┬────────────┐\n",
      "│ position ┆ orth_index ┆ lower_index ┆ token2doc_index ┆ has_spaces │\n",
      "│ ---      ┆ ---        ┆ ---         ┆ ---             ┆ ---        │\n",
      "│ u32      ┆ u32        ┆ u32         ┆ i32             ┆ bool       │\n",
      "╞══════════╪════════════╪═════════════╪═════════════════╪════════════╡\n",
      "│ 101364   ┆ 19820      ┆ 19820       ┆ 237             ┆ false      │\n",
      "│ 391341   ┆ 19820      ┆ 19820       ┆ 1004            ┆ false      │\n",
      "│ 427065   ┆ 19820      ┆ 19820       ┆ 1056            ┆ false      │\n",
      "│ 427086   ┆ 19820      ┆ 19820       ┆ 1056            ┆ false      │\n",
      "│ 686781   ┆ 19820      ┆ 19820       ┆ 1669            ┆ false      │\n",
      "│ 897130   ┆ 46462      ┆ 46462       ┆ 2411            ┆ false      │\n",
      "│ 922636   ┆ 52315      ┆ 52315       ┆ 2479            ┆ false      │\n",
      "│ 1004568  ┆ 19820      ┆ 19820       ┆ 2663            ┆ false      │\n",
      "│ 1122385  ┆ 19820      ┆ 19820       ┆ 2958            ┆ false      │\n",
      "│ 1512737  ┆ 52315      ┆ 52315       ┆ 3881            ┆ false      │\n",
      "└──────────┴────────────┴─────────────┴─────────────────┴────────────┘\n",
      "\n",
      "File Size: 0.00 MB\n",
      "\n",
      "FILE: puncts.parquet\n",
      "\n",
      "Dimensions: 725,831 rows × 1 columns\n",
      "\n",
      "Columns and Types:\n",
      "  - position: UInt32\n",
      "\n",
      "First 10 Rows:\n",
      "shape: (10, 1)\n",
      "┌──────────┐\n",
      "│ position │\n",
      "│ ---      │\n",
      "│ u32      │\n",
      "╞══════════╡\n",
      "│ 129      │\n",
      "│ 134      │\n",
      "│ 138      │\n",
      "│ 144      │\n",
      "│ 158      │\n",
      "│ 175      │\n",
      "│ 192      │\n",
      "│ 194      │\n",
      "│ 213      │\n",
      "│ 215      │\n",
      "└──────────┘\n",
      "\n",
      "File Size: 1.98 MB\n",
      "\n",
      "EXPLORATION COMPLETE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back and display\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5b83a5",
   "metadata": {},
   "source": [
    "## Loading Strategy Comparison\n",
    "\n",
    "Compare eager vs lazy loading - timing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0943a5fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading strategies saved to: ..\\results\\loading_strategies.txt\n"
     ]
    }
   ],
   "source": [
    "test_file = BASE_PATH / 'tokens.parquet'\n",
    "output_file = RESULTS_DIR / 'loading_strategies.txt'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"POLARS LOADING STRATEGY COMPARISON\\n\\n\")\n",
    "    f.write(f\"Testing with: {test_file.name}\\n\")\n",
    "    f.write(f\"File size: {test_file.stat().st_size / (1024*1024):.2f} MB\\n\\n\")\n",
    "    \n",
    "    # Eager loading\n",
    "    f.write(\"EAGER LOADING: pl.read_parquet()\\n\\n\")\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    tokens_eager = pl.read_parquet(test_file)\n",
    "    end = time.perf_counter()\n",
    "    eager_time = end - start\n",
    "    \n",
    "    f.write(f\"Time to load: {eager_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Data loaded:  {tokens_eager.shape[0]:,} rows × {tokens_eager.shape[1]} columns\\n\")\n",
    "    f.write(f\"Memory:       Data is in RAM immediately\\n\\n\")\n",
    "    \n",
    "    # Lazy loading\n",
    "    f.write(\"LAZY LOADING: pl.scan_parquet() + .collect()\\n\\n\")\n",
    "    \n",
    "    start_scan = time.perf_counter()\n",
    "    tokens_lazy = pl.scan_parquet(test_file)\n",
    "    end_scan = time.perf_counter()\n",
    "    scan_time = end_scan - start_scan\n",
    "    \n",
    "    f.write(f\"Step 1 - Create lazy frame: {scan_time:.6f} seconds (near instant)\\n\")\n",
    "    f.write(f\"Memory: No data loaded yet - just query plan\\n\\n\")\n",
    "    \n",
    "    start_collect = time.perf_counter()\n",
    "    tokens_lazy_result = tokens_lazy.collect()\n",
    "    end_collect = time.perf_counter()\n",
    "    collect_time = end_collect - start_collect\n",
    "    \n",
    "    f.write(f\"Step 2 - Execute .collect(): {collect_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Data loaded: {tokens_lazy_result.shape[0]:,} rows × {tokens_lazy_result.shape[1]} columns\\n\\n\")\n",
    "    \n",
    "    total_lazy_time = scan_time + collect_time\n",
    "    f.write(f\"Total lazy time: {total_lazy_time:.4f} seconds\\n\\n\")\n",
    "    \n",
    "    # Lazy with optimisation\n",
    "    f.write(\"LAZY LOADING WITH QUERY OPTIMISATION\\n\")\n",
    "    f.write(\"Example: Only select 2 columns instead of all 4\\n\\n\")\n",
    "    \n",
    "    start_opt = time.perf_counter()\n",
    "    tokens_optimised = pl.scan_parquet(test_file).select(['orth_index', 'token2doc_index']).collect()\n",
    "    end_opt = time.perf_counter()\n",
    "    opt_time = end_opt - start_opt\n",
    "    \n",
    "    f.write(f\"Time to load (2 columns only): {opt_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Data loaded: {tokens_optimised.shape[0]:,} rows × {tokens_optimised.shape[1]} columns\\n\\n\")\n",
    "    \n",
    "    # Summary\n",
    "    f.write(\"SUMMARY\\n\")\n",
    "    f.write(f\"Eager loading (all columns):     {eager_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Lazy loading (all columns):      {total_lazy_time:.4f} seconds\\n\")\n",
    "    f.write(f\"Lazy loading (2 columns only):   {opt_time:.4f} seconds\\n\\n\")\n",
    "    f.write(f\"Optimisation benefit: {(eager_time - opt_time) / eager_time * 100:.1f}% faster\\n\\n\")\n",
    "    f.write(\"KEY INSIGHT:\\n\")\n",
    "    f.write(\"- Lazy loading allows query optimisation before execution\\n\")\n",
    "    f.write(\"- For large files, selecting only needed columns is much faster\\n\")\n",
    "    f.write(\"- Eager loading is simpler for small files or when all data is needed\\n\")\n",
    "\n",
    "print(f\"Loading strategies saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c408ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLARS LOADING STRATEGY COMPARISON\n",
      "\n",
      "Testing with: tokens.parquet\n",
      "File size: 21.04 MB\n",
      "\n",
      "EAGER LOADING: pl.read_parquet()\n",
      "\n",
      "Time to load: 0.0331 seconds\n",
      "Data loaded:  6,387,921 rows × 4 columns\n",
      "Memory:       Data is in RAM immediately\n",
      "\n",
      "LAZY LOADING: pl.scan_parquet() + .collect()\n",
      "\n",
      "Step 1 - Create lazy frame: 0.000256 seconds (near instant)\n",
      "Memory: No data loaded yet - just query plan\n",
      "\n",
      "Step 2 - Execute .collect(): 0.0424 seconds\n",
      "Data loaded: 6,387,921 rows × 4 columns\n",
      "\n",
      "Total lazy time: 0.0426 seconds\n",
      "\n",
      "LAZY LOADING WITH QUERY OPTIMISATION\n",
      "Example: Only select 2 columns instead of all 4\n",
      "\n",
      "Time to load (2 columns only): 0.0193 seconds\n",
      "Data loaded: 6,387,921 rows × 2 columns\n",
      "\n",
      "SUMMARY\n",
      "Eager loading (all columns):     0.0331 seconds\n",
      "Lazy loading (all columns):      0.0426 seconds\n",
      "Lazy loading (2 columns only):   0.0193 seconds\n",
      "\n",
      "Optimisation benefit: 41.6% faster\n",
      "\n",
      "KEY INSIGHT:\n",
      "- Lazy loading allows query optimisation before execution\n",
      "- For large files, selecting only needed columns is much faster\n",
      "- Eager loading is simpler for small files or when all data is needed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back and display\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b9bd4",
   "metadata": {},
   "source": [
    "## File Sizes\n",
    "\n",
    "Display all corpus files with sizes and descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba03c368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File sizes saved to: ..\\results\\file_sizes.txt\n"
     ]
    }
   ],
   "source": [
    "file_info = {\n",
    "    'corpus.json':      'Corpus metadata',\n",
    "    'metadata.parquet': 'Document metadata (url, title, date, year, category)',\n",
    "    'tokens.parquet':   'Token data (6.4M tokens)',\n",
    "    'vocab.parquet':    'Vocabulary (105K unique tokens)',\n",
    "    'spaces.parquet':   'Space positions',\n",
    "    'puncts.parquet':   'Punctuation positions',\n",
    "    'README.md':        'Documentation'\n",
    "}\n",
    "\n",
    "output_file = RESULTS_DIR / 'file_sizes.txt'\n",
    "\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(\"RNZ CLIMATE CORPUS - FILE SIZES\\n\\n\")\n",
    "    \n",
    "    f.write(f\"{'File':<25} {'Size':>12}    {'Description'}\\n\")\n",
    "    f.write(\"-\" * 80 + \"\\n\")\n",
    "    \n",
    "    total_size = 0\n",
    "    \n",
    "    for filename in sorted(file_info.keys()):\n",
    "        filepath = BASE_PATH / filename\n",
    "        \n",
    "        if filepath.exists():\n",
    "            size_bytes = filepath.stat().st_size\n",
    "            total_size += size_bytes\n",
    "            \n",
    "            if size_bytes < 1024:\n",
    "                size_str = f\"{size_bytes} B\"\n",
    "            elif size_bytes < 1024 * 1024:\n",
    "                size_str = f\"{size_bytes / 1024:.2f} KB\"\n",
    "            else:\n",
    "                size_str = f\"{size_bytes / (1024 * 1024):.2f} MB\"\n",
    "            \n",
    "            desc = file_info[filename]\n",
    "            f.write(f\"{filename:<25} {size_str:>12}    {desc}\\n\")\n",
    "        else:\n",
    "            f.write(f\"{filename:<25} {'NOT FOUND':>12}    {file_info[filename]}\\n\")\n",
    "    \n",
    "    total_mb = total_size / (1024 * 1024)\n",
    "    f.write(f\"\\n{'Total':<25} {total_mb:>9.2f} MB\\n\\n\")\n",
    "    \n",
    "    f.write(\"CORPUS STATISTICS\\n\")\n",
    "    f.write(f\"Total files:  {len(file_info)}\\n\")\n",
    "    f.write(f\"Total size:   {total_mb:.2f} MB ({total_size:,} bytes)\\n\")\n",
    "    f.write(f\"Largest file: tokens.parquet (~77% of total)\\n\")\n",
    "\n",
    "print(f\"File sizes saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24b27907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNZ CLIMATE CORPUS - FILE SIZES\n",
      "\n",
      "File                              Size    Description\n",
      "--------------------------------------------------------------------------------\n",
      "README.md                        791 B    Documentation\n",
      "corpus.json                      890 B    Corpus metadata\n",
      "metadata.parquet             367.83 KB    Document metadata (url, title, date, year, category)\n",
      "puncts.parquet                 1.98 MB    Punctuation positions\n",
      "spaces.parquet                 2.63 KB    Space positions\n",
      "tokens.parquet                21.04 MB    Token data (6.4M tokens)\n",
      "vocab.parquet                  1.36 MB    Vocabulary (105K unique tokens)\n",
      "\n",
      "Total                         24.74 MB\n",
      "\n",
      "CORPUS STATISTICS\n",
      "Total files:  7\n",
      "Total size:   24.74 MB (25,938,833 bytes)\n",
      "Largest file: tokens.parquet (~77% of total)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read back and display\n",
    "with open(output_file, 'r', encoding='utf-8') as f:\n",
    "    content = f.read()\n",
    "    print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
