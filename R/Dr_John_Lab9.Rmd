---
title: 'Lab 9 solutions MAST90125: Bayesian Statistical learning'
header-includes:
   - \usepackage{amsmath}
   - \usepackage{unicode-math}
   - \let\bm\symbf
   - \usepackage{tikz}
   - \usetikzlibrary{calc}
   - \usepackage{eso-pic}
   - \usepackage[useregional]{datetime2}
   - \newcommand{\mymarginlabel}{ From Dr John Holmes Curriculum   | Compiled:\space\DTMnow  }
   - \AddToShipoutPictureBG{\begin{tikzpicture}[remember picture,overlay]\node[rotate=90, anchor=north] at ($(current page.north west)+(1.4cm,-13cm)$) {\scriptsize\ttfamily \mymarginlabel};\end{tikzpicture}}
output: 
  pdf_document:
    latex_engine: xelatex    # preference 
    number_sections: false
knit: (function(inputFile, encoding) {
  out_dir <- "../report"
  if (!dir.exists(out_dir)) dir.create(out_dir, recursive = TRUE)
  rmarkdown::render(inputFile, encoding = encoding, output_dir = out_dir) })
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(gt)
set.seed(2334576)
```

## Variational Bayes.

In lecture 18, we looked at (mean-field) Variational Bayes as a method to find approximate posterior distributions for sub-blocks of the parameter vector $\bm \theta$, for the model 

\begin{eqnarray}
p({\bf y}|\bm \beta, {\bf u}, \tau_e) &=& \mathcal{N}({\bf X}\bm \beta + {\bf Z}{\bf u}, \frac{1}{\tau_e}{\bf I}_n) \nonumber \\
p(\bm \beta) &\propto& 1   \nonumber \\
p({\bf u})   &=&\mathcal{N}({\bf 0}_q, \frac{1}{\tau_u}{\bf K}) \nonumber\\
p(\tau_e)    &=& \text{Ga}(\alpha_e,\gamma_e) \nonumber\\
p(\tau_u)    &=& \text{Ga}(\alpha_u,\gamma_u) \nonumber
\end{eqnarray}


## Instructions for lab

Download \texttt{farmdata.csv} and \texttt{Kmat.csv} from LMS.

Re-format the code given in lecture 18 for performing Variational Bayes so that rather than constructing approximate posteriors,

\begin{itemize}
\item $Q(\bm \beta)$
\item $Q({\bf u})$
\item $Q(\tau_e)$
\item $Q(\tau_u)$
\end{itemize}

you determine the approximate posteriors 

\begin{itemize}
\item $Q(\bm \beta, {\bf u})$
\item $Q(\tau_e)$
\item $Q(\tau_u)$.
\end{itemize}

Compare the performance of the new blocking structure to that used in class.


\subsection{Solution}

The difference between the code given in class and this lab is that we approximate the posterior of $\bm \beta, \bm u$ jointly. To do this, note the trick that the joint kernel of $\bm \beta, {\bf u}$ can be wirtten as 

\begin{eqnarray}
		p(\bm \beta,{\bf u}) \propto e^{-\frac{\tau_u\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2}}, \nonumber
\end{eqnarray}

Hence in terms of coding, updating $\bm \beta, {\bf u}$ jointly looks just like updating $\bf u$ in the code given in lecture 18, except that $\bm \beta = {\bf 0}$.

As a reminder, the joint distribution, $p(y, {\bm \beta}, {\bf u}, \tau_e, \tau_u|{\bf X}, {\bf Z})$ is 
		\begin{eqnarray}
		\bigg( \frac{\tau_e}{2\pi}\bigg)^\frac{n}{2}e^{-\frac{\tau_e({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})}{2}} \hspace{-1mm}\times  \hspace{-1mm} 1  \hspace{-1mm} \times  \hspace{-1mm} \bigg(\frac{\tau_u}{2\pi}\bigg)^\frac{q}{2}\det({\bf K})^{-1/2} e^{-\frac{\tau_u{\bf u}'{\bf K}^{-1}{\bf u}}{2}}  \hspace{-1mm}\times  \hspace{-1mm} \frac{\gamma_u^{\alpha_u}\tau_u^{\alpha_u-1}e^{-\gamma_u\tau_u}}{\Gamma(\alpha_u)}  \hspace{-1mm}\times  \hspace{-1mm} \frac{\gamma_e^{\alpha_e}\tau_e^{\alpha_e-1}e^{-\gamma_e\tau_e}}{\Gamma(\alpha_e)}, \nonumber
		\end{eqnarray}
		
and that we need to determine the expectation of log-kernels.		

\begin{itemize}		
		\item For $\tau_e$, the kernel and log-kernel are respectively 

			\begin{eqnarray}
			\text{Kernel:}&&{\tau_e}^\frac{n}{2}e^{-\frac{\tau_e({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})}{2}}{\tau_e^{\alpha_e-1}e^{-\gamma_e\tau_e}}   \nonumber \\
			\text{Log-kernel:}&& (n/2+\alpha_e-1)\log(\tau_e) - \tau_e(\gamma_e +({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u}) /2) \nonumber
			\end{eqnarray}

	\item The expected log-kernel $E_{-\tau_e}(\text{Log-kernel})$ is 

		\begin{eqnarray}
		\hspace*{-7 mm}(\frac{n}{2}+\alpha_e-1)\log(\tau_e) - \tau_e(\gamma_e +\frac{{\bf y}'{\bf y}+E_{\bm \beta, {\bf u} }\bigg (\begin{pmatrix}\bm \beta' & {\bf u}' \end{pmatrix}\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z}\\  {\bf Z}'{\bf X} & {\bf Z}'{\bf Z}\end{pmatrix}\begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix} \bigg)}{2} - {\bf y}'{\bf X}E_{\bm\beta. {\bf u}}(\bm \beta) - {\bf y}'{\bf Z}E_{\bm \beta, \bf u}({\bf u}) ) \nonumber
		\end{eqnarray}
		
		
		\item Using the tricks that $E({\bf x}{\bf x}')= \text{Var}({\bf x}) + E({\bf x})E(\bf x)'$ and that ${\bf a}'{\bf D}{\bf a} = \text{Tr}({\bf a}'{\bf D}{\bf a})$ $= \text{Tr}({\bf D}{\bf a}{\bf a}')$, the expected log-kernel can be written as,		

			\begin{eqnarray}
			\hspace*{-30 mm}\bigg(\hspace{-1 mm}\frac{n}{2}+\alpha_e-1\hspace{-1 mm}\bigg)\hspace{-1 mm}\log(\tau_e)-\tau_e\bigg(\hspace{-1 mm}\gamma_e +\frac{({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta) - {\bf Z}E_{\bm \beta, \bf u}({\bf u}))'({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta)- {\bf Z}E_{\bm \beta, \bf u}({\bf u}))  +\text{Tr}\bigg(\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z}\\  {\bf Z}'{\bf X} & {\bf Z}'{\bf Z}\end{pmatrix}\begin{pmatrix}\text{Var}(\bm \beta) & \text{Var}(\bm \beta, {\bf u})\\  \text{Var}({\bf u},\bm \beta) & \text{Var}({\bf u})\end{pmatrix}\bigg)}{2} \hspace{-1 mm}\bigg),\nonumber
			\end{eqnarray}

	\item which indicates that the approximate posterior for $\tau_e$ is 

	\begin{eqnarray}
		\hspace*{-17 mm}\text{Ga}\bigg(\frac{n}{2}+\alpha_e,\gamma_e +\frac{({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta) - {\bf Z}E_{\bm \beta, \bf u}({\bf u}))'({\bf y} - {\bf X}E_{\bm\beta, \bf u}(\bm \beta)- {\bf Z}E_{\bm \beta, \bf u}({\bf u}))  +\text{Tr}\bigg(\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z}\\  {\bf Z}'{\bf X} & {\bf Z}'{\bf Z}\end{pmatrix}\begin{pmatrix}\text{Var}(\bm \beta) & \text{Var}(\bm \beta, {\bf u})\\  \text{Var}({\bf u},\bm \beta) & \text{Var}({\bf u})\end{pmatrix}\bigg)}{2}\bigg). \nonumber 
	\end{eqnarray}

	\item For $\tau_u$, the kernel and log-kernel are respectively 

		\begin{eqnarray}
		\text{Kernel:}&&\bigg(\frac{\tau_u}{2\pi}\bigg)^\frac{q}{2} e^{-\frac{\tau_u{\bf u}'{\bf K}^{-1}{\bf u}}{2}} {\tau_u^{\alpha_u-1}e^{-\gamma_u\tau_u}}  \nonumber \\
		\text{Log-kernel:}&& (q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u +{\bf u}'{\bf K}^{-1}{\bf u} /2) \nonumber
		\end{eqnarray}

	\item The expected log-kernel $E_{-\tau_u}(\text{Log-kernel})$ is 

		\begin{eqnarray}
		&=&(q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u +E_{\bm \beta, {\bf u}}({\bf u}'{\bf K}^{-1}{\bf u}) /2) \nonumber \\ &=&(q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u \text{Tr}({\bf K}^{-1}E_{\bm \beta, {\bf u}}({\bf u}{\bf u}')) /2) \nonumber \\
		&=& (q/2+\alpha_u-1)\log(\tau_u) - \tau_u(\gamma_u +E_{\bm \beta, {\bf u}}({\bf u})'{\bf K}^{-1}E_{\bm \beta, {\bf u}}({\bf u})/2 +\text{Tr}({\bf K}^{-1}Var({\bf u})) /2) \nonumber 
		\end{eqnarray}

	\item which indicates that the approximate posterior for $\tau_u$ is 

	\begin{eqnarray}
	\text{Ga}\bigg(\frac{q}{2}+\alpha_u,\gamma_u +\frac{E_{\bm \beta, {\bf u}}({\bf u})'{\bf K}^{-1}E_{\bm \beta, {\bf u}}({\bf u}) +\text{Tr}({\bf K}^{-1}Var({\bf u}))}{2}\bigg). \nonumber 
	\end{eqnarray}


		\item For $\bm \beta, \bf u$, the kernel and log-kernel are respectively 

			\begin{eqnarray}
			\text{Kernel:}&=& e^{-\frac{\tau_e({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})'({\bf y} - {\bf X}\bm \beta - {\bf Z}{\bf u})}{2}-\frac{\tau_u{\bf u}'{\bf K}^{-1}{\bf u}}{2}}\nonumber \\
			 &=& e^{-\frac{\tau_e({\bf y}  -  \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})'({\bf y} -   \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})}{2}}e^{-\frac{\tau_u\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2}} \nonumber \\
			\text{Log-kernel:}&&-\frac{\tau_e({\bf y}  -  \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})'({\bf y} -   \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})}{2}-\frac{\tau_u\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} \nonumber
			\end{eqnarray}

		\item The expected log-kernel $E_{-\bm \beta, \bf u}(\text{Log-kernel})$ is 
	
			\begin{eqnarray}
			&=&-\frac{E_{\tau_e}(\tau_e)({\bf y}  -  \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})'({\bf y} -   \begin{tiny}\begin{pmatrix} {\bf X} & {\bf Z} \end{pmatrix} \begin{pmatrix}\bm \beta \\ {\bf u} \end{pmatrix}\end{tiny})}{2}-\frac{E_{\tau_u}(\tau_u)\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} \nonumber \\
			&\propto&-\frac{E_{\tau_e}(\tau_e)\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf X}'{\bf X} & {\bf X}'{\bf Z} \\{\bf Z}'{\bf X} & {\bf Z}'{\bf Z} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} +E_{\tau_e}(\tau_e)\begin{footnotesize}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf X}'{\bf y}  \\{\bf Z}'{\bf y} \end{pmatrix}\end{footnotesize}-\frac{E_{\tau_u}(\tau_u)\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2}  \nonumber \\
			&=&-\frac{\begin{tiny}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}E_{\tau_e}(\tau_e){\bf X}'{\bf X} & E_{\tau_e}(\tau_e){\bf X}'{\bf Z} \\E_{\tau_e}(\tau_e){\bf Z}'{\bf X} & E_{\tau_e}(\tau_e){\bf Z}'{\bf Z} + E_{\tau_u}(\tau_u){\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}\end{tiny}}{2} +E_{\tau_e}(\tau_e)\begin{footnotesize}\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf X}'{\bf y}  \\{\bf Z}'{\bf y} \end{pmatrix}\end{footnotesize} \nonumber 
			\end{eqnarray}

		
		\item which indicates that the approximate posterior for $\bm \beta, \bf u$ is 
			\begin{eqnarray}
			\mathcal{N}\bigg(E_{\tau_e}(\tau_e)\begin{pmatrix}E_{\tau_e}(\tau_e){\bf X}'{\bf X} & E_{\tau_e}(\tau_e){\bf X}'{\bf Z} \\E_{\tau_e}(\tau_e){\bf Z}'{\bf X} & E_{\tau_e}(\tau_e){\bf Z}'{\bf Z} + E_{\tau_u}(\tau_u){\bf K}^{-1} \end{pmatrix}^{-1}\begin{pmatrix}{\bf X}'{\bf y}  \\{\bf Z}'{\bf y} \end{pmatrix},\begin{pmatrix}E_{\tau_e}(\tau_e){\bf X}'{\bf X} & E_{\tau_e}(\tau_e){\bf X}'{\bf Z} \\E_{\tau_e}(\tau_e){\bf Z}'{\bf X} & E_{\tau_e}(\tau_e){\bf Z}'{\bf Z} + E_{\tau_u}(\tau_u){\bf K}^{-1} \end{pmatrix}^{-1}\bigg). \nonumber 
			\end{eqnarray}

	
    		
	\end{itemize}



```{r}
#Arguments are
#epsilon: accuracy cut-off.
#iter: no of iterations
#Kinv: inverse of K, where p(u) = N(0,\sigma^2_u K)
#Z: Predictor matrix for random effects
#X: Predictor matrix for fixed effects
#y: response vector
#taue_0: initial guess for residual precision.
#tauu_0: initial guess for random effect precision
#a.u, g.u: hyper-parameters of gamma prior for tauu
#a.e, g.e: hyper-parameters of gamma prior for taue

#Output are final estimates, plus iteration number when convergence was reached.
VB.mm<-function(epsilon,iter,Kinv,Z,X,y,taue_0,tauu_0,u0,beta0,a.e,g.e,a.u,g.u){
  n<-dim(X)[1]
  p<-dim(X)[2]
  q<-dim(Z)[2]
  W <-cbind(X,Z)
  WTW<-crossprod(W)
  WTY<-crossprod(W,y)
  Kinvall<-matrix(0,p+q,p+q)
  Kinvall[-c(1:p),-c(1:p)]<-Kinv
    
  for(i in 1:iter){
  Vub <-solve(taue_0*WTW+tauu_0*Kinvall) #update Var(b,u)
  ub  <-taue_0*Vub%*%WTY                 #update E(b,u)
  TrKinvub <- sum(diag(Kinvall%*%Vub))
  uKinvub  <- t(ub)%*%Kinvall%*%ub
  tauu    <- (a.u+0.5*q)/(g.u+0.5*as.numeric(uKinvub)+0.5*TrKinvub)
  tauu    <- as.numeric(tauu)
  err     <- y - W%*%ub
  TrWTWub  <- sum(diag(WTW%*%Vub))
  taue    <- (a.e+0.5*n)/(g.e+0.5*sum(err^2)+0.5*TrWTWub)
  taue    <- as.numeric(taue)
  
  if(i > 1){
  diffub  <- sqrt((ub-ub0)^2)/(abs(ub)+0.01)
  diffte <- abs(taue_0-taue)/(taue+0.01)
  difftu <- abs(tauu_0-tauu)/(tauu+0.01)
  diffvub <- sqrt((diag(Vub0) - diag(Vub))^2)/(diag(Vub))
  diff.all<-c(diffub,diffte,difftu,diffvub)
  if(max(diff.all) < epsilon) break
  }
  Vub0 <- Vub;ub0<-ub;taue_0<-taue;tauu_0<-tauu
  #Calculate relative change.
  }
  
  taue.param<-c((a.e+0.5*n),(g.e+0.5*sum(err^2)+0.5*TrWTWub))
  tauu.param<-c((a.u+0.5*q),(g.u+0.5*uKinvub+0.5*TrKinvub))  
  param<-list(ub,Vub,taue.param,tauu.param,i)
  names(param)<-c('betau_mean','betau_var','tau_e','tau_u','iter')
  return(param)
}
```

The R code used to implement Gibbs sampling for this mixed model in lecture 18 is given below:

```{r}
#Arguments are
#iter: no of iterations
#Z: Predictor matrix for random effects
#X: Predictor matrix for fixed effects
#y: response vector
#burnin: number of initial iterations to discard.
#taue_0: initial guess for residual precision.
#tauu_0: initial guess for random effect precision
#Kinv: inverse of K, where p(u) = N(0,\sigma^2_u K)
#a.u, b.u: hyper-parameters of gamma prior for tauu
#a.e, b.e: hyper-parameters of gamma prior for taue

normalmm.Gibbs<-function(iter,Z,X,y,burnin,taue_0,tauu_0,Kinv,a.u,b.u,a.e,b.e){
n   <-length(y) #no. observations
p   <-dim(X)[2] #no of fixed effect predictors.
q   <-dim(Z)[2] #no of random effect levels
tauu<-tauu_0
taue<-taue_0
beta0<-rnorm(p)
u0   <-rnorm(q,0,sd=1/sqrt(tauu))

#Building combined predictor matrix.
W<-cbind(X,Z)
WTW <-crossprod(W)
WTy <-crossprod(W,y)
library(mvtnorm)

#storing results.
par <-matrix(0,iter,p+q+2)
#Calculating log predictive densities
lppd<-matrix(0,iter,n)

#Create modified identity matrix for joint posterior.
I0  <-diag(p+q)
diag(I0)[1:p]<-0
I0[-c(1:p),-c(1:p)]  <-Kinv

for(i in 1:iter){
#Conditional posteriors.
  uKinvu <- t(u0)%*%Kinv%*%u0
  uKinvu <-as.numeric(uKinvu)
tauu <-rgamma(1,a.u+0.5*q,b.u+0.5*uKinvu)
#Updating component of normal posterior for beta,u
Prec <-WTW + tauu*I0/taue
P.mean<- solve(Prec)%*%WTy
P.var <-solve(Prec)/taue
betau <-rmvnorm(1,mean=P.mean,sigma=P.var)
betau <-as.numeric(betau)
err   <- y-W%*%betau
taue  <-rgamma(1,a.e+0.5*n,b.e+0.5*sum(err^2))
#storing iterations.
par[i,]<-c(betau,1/sqrt(tauu),1/sqrt(taue))
beta0  <-betau[1:p]
u0     <-betau[p+1:q]
lppd[i,]= dnorm(y,mean=as.numeric(W%*%betau),sd=1/sqrt(taue))
}

lppd      = lppd[-c(1:burnin),]
lppdest   = sum(log(colMeans(lppd)))        #Estimating lppd for whole dataset.
pwaic2    = sum(apply(log(lppd),2,FUN=var)) #Estimating effective number of parameters.
par <-par[-c(1:burnin),]
colnames(par)<-c(paste('beta',1:p,sep=''),paste('u',1:q,sep=''),'sigma_b','sigma_e')
mresult<-list(par,lppdest,pwaic2)
names(mresult)<-c('par','lppd','pwaic')
return(mresult)
}
```

Any other code you may use can be modified from the code given in Lecture 18. Possible things you may want to check include:

\begin{itemize}
\item convergence of Gibbs sampler
\item comparing the empirical and approximate distributions using density plots.
\end{itemize}


\paragraph{Importing and formatting the data}

The dataset consists of two files:

\begin{itemize}
\item \texttt{farmdata.csv}: 24 observations of weaning weights with variables:
  \begin{itemize}
  \item $y$: response (weaning weight)
  \item \texttt{flock}: fixed effect (levels 1 or 2)
  \item \texttt{sire}: random effect - father ID (1, 2, or 3)
  \item \texttt{dam}: random effect - mother ID (4--13, 10 unique dams)
  \item \texttt{id}: animal identifier (14--37)
  \end{itemize}
\item \texttt{Kmat.csv}: $13 \times 13$ kinship matrix (3 sires + 10 dams) with genetic relationship structure:
  \begin{itemize}
  \item Sire 1 \& Sire 2: Related (0.125 kinship - cousins)
  \item Dams 4 \& 5: Full siblings (0.500 - offspring of Sire 1)
  \item Dams 6 \& 7: Mixed ancestry from Sire 1 and Sire 2
  \item Dams 8 \& 9: Nearly full siblings (0.484 kinship)
  \item Dams 10--13: Offspring of Sire 3, varying relatedness (0.250--0.500)
  \end{itemize}
\end{itemize}

```{r}
data <- read.csv('../data_raw/farmdata.csv')
K    <- read.csv('../data_raw/Kmat.csv', header = FALSE)
K    <- as.matrix(K)
Kinv <- solve(K)

n<-dim(data)[1]
q<-dim(Kinv)[1]
X<-table(1:n,data$flock) #flock is fixed effect
#Indicator matrix for parents.
Z2<-table(1:n,data$sire)
Z3<-cbind(Z2,table(1:n,data$dam))
```

\paragraph{Understanding the Kinship Matrix K}

The kinship matrix ${\bf K}$ is a $13 \times 13$ matrix representing genetic relationships between the 13 ancestors (3 sires + 10 dams). Values represent the proportion of shared genetic material:

- Diagonal = 1.000 (animal related to itself)
- 0.500 = parent-offspring or full siblings
- 0.250 = grandparent-grandchild or half-siblings
- 0.125 = first cousins

```{r display_K}
# Create formatted kinship matrix table
K_df <- as.data.frame(round(K, 3))
colnames(K_df) <- paste0("V", 1:13)
rownames(K_df) <- paste0("V", 1:13)

knitr::kable(K_df, caption = "Kinship Matrix K (13 x 13): Sires 1-3 (V1-V3), then Dams 4-13 (V4-V13)")

cat("\n\nKey relationships in K:\n")
cat("K[1,2] = ", K[1,2], " (Sire 1 & Sire 2 - cousins)\n")
cat("K[4,5] = ", K[4,5], " (Dams 4 & 5 - full siblings)\n")
cat("K[8,9] = ", K[8,9], " (Dams 8 & 9 - nearly full siblings)\n")
```

\paragraph{The Inverse Kinship Matrix K⁻¹}

The inverse kinship matrix ${\bf K}^{-1}$ appears in the prior for random effects:

$$p({\bf u}) = \mathcal{N}({\bf 0}_q, \frac{1}{\tau_u}{\bf K})$$

Equivalently, the precision matrix for ${\bf u}$ is $\tau_u {\bf K}^{-1}$. This matrix determines how strongly correlated random effects are penalised.

```{r display_Kinv}
# Create formatted inverse kinship matrix table
Kinv_df <- as.data.frame(round(Kinv, 3))
colnames(Kinv_df) <- paste0("V", 1:13)
rownames(Kinv_df) <- paste0("V", 1:13)

knitr::kable(Kinv_df, caption = "Inverse Kinship Matrix K^-1 (13 x 13): Precision matrix for random effects prior")

cat("\n\nNote: K⁻¹ has larger diagonal values reflecting genetic constraints\n")
```

\paragraph{The Block Penalty Matrix for Joint (β, u)}

For the joint prior on $(\bm \beta, {\bf u})$, we construct a $(p+q) \times (p+q)$ block matrix where $p = 2$ fixed effects and $q = 13$ random effects, giving a $15 \times 15$ matrix:

$$\begin{pmatrix}
{\bf 0}_{2 \times 2} & {\bf 0}_{2 \times 13} \\ 
{\bf 0}_{13 \times 2} & {\bf K}^{-1}_{13 \times 13}
\end{pmatrix}$$

```{r display_block_penalty}
p <- dim(X)[2]  # number of fixed effects

# Construct block penalty matrix
penalty_matrix <- rbind(
  cbind(matrix(0, p, p), matrix(0, p, q)),   # Top: zeros for β
  cbind(matrix(0, q, p), Kinv)               # Bottom: K⁻¹ for u
)

# Show first 8×8 block to illustrate structure
penalty_sub <- round(penalty_matrix[1:8, 1:8], 3)
colnames(penalty_sub) <- c("β₁", "β₂", "u₁", "u₂", "u₃", "u₄", "u₅", "u₆")
rownames(penalty_sub) <- c("β₁", "β₂", "u₁", "u₂", "u₃", "u₄", "u₅", "u₆")

knitr::kable(penalty_sub, caption = "Block Penalty Matrix (first 8 x 8 of full 15 x 15): Top-left 2x2 zeros for beta, Bottom-right 13x13 K^-1 for u")

cat("\n\nFull matrix dimensions: 15 × 15 (2 fixed effects β + 13 random effects u)\n")
cat("This appears in p(β, u) ∝ exp(-τᵤ/2 × [β' u'] × PenaltyMatrix × [β u]')\n")
```


\paragraph{running the Gibbs sampler, checking convergence, and calculating effective sample size}

```{r}
system.time(chain1<-normalmm.Gibbs(iter=10000,Z=Z3,X=X,y=data$y,burnin=2000,taue_0=5,tauu_0=0.2,Kinv=Kinv,a.u=0.001,b.u=0.001,a.e=0.001,b.e=0.001))
system.time(chain2<-normalmm.Gibbs(iter=10000,Z=Z3,X=X,y=data$y,burnin=2000,taue_0=1,tauu_0=1,Kinv=Kinv,a.u=0.001,b.u=0.001,a.e=0.001,b.e=0.001))
system.time(chain3<-normalmm.Gibbs(iter=10000,Z=Z3,X=X,y=data$y,burnin=2000,taue_0=0.2,tauu_0=3,Kinv=Kinv,a.u=0.001,b.u=0.001,a.e=0.001,b.e=0.001))

library(coda)
rml1<-as.mcmc.list(as.mcmc((chain1$par[1:4000,])))
rml2<-as.mcmc.list(as.mcmc((chain2$par[1:4000,])))
rml3<-as.mcmc.list(as.mcmc((chain3$par[1:4000,])))
rml4<-as.mcmc.list(as.mcmc((chain1$par[4000+1:4000,])))
rml5<-as.mcmc.list(as.mcmc((chain2$par[4000+1:4000,])))
rml6<-as.mcmc.list(as.mcmc((chain3$par[4000+1:4000,])))
rml<-c(rml1,rml2,rml3,rml4,rml5,rml6)

#Gelman-Rubin diagnostic.
gelman.diag(rml)[[1]]
#effective sample size.
effectiveSize(rml)
```

\paragraph{Estimating parameters of variational Bayes approximations}
```{r}
system.time(test1<-VB.mm(epsilon=1e-5,iter=2000,Kinv=Kinv,Z=Z3,X=X,y=data$y,taue_0=0.2,tauu_0=0.2,u0=rnorm(13),beta0=rnorm(2),a.e=0.001,g.e=0.001,a.u=0.001,g.u=0.001))
system.time(test2<-VB.mm(epsilon=1e-5,iter=2000,Kinv=Kinv,Z=Z3,X=X,y=data$y,taue_0=5,tauu_0=1,u0=rnorm(13),beta0=rnorm(2),a.e=0.001,g.e=0.001,a.u=0.001,g.u=0.001))
system.time(test3<-VB.mm(epsilon=1e-5,iter=2000,Kinv=Kinv,Z=Z3,X=X,y=data$y,taue_0=1,tauu_0=5,u0=rnorm(13),beta0=rnorm(2),a.e=0.001,g.e=0.001,a.u=0.001,g.u=0.001))

test1$iter
test2$iter
test3$iter
```

\paragraph{Comparing estimates obtained from Gibbs sampling and Variational Bayes}

```{r, fig1, fig.height = 25, fig.width = 20}
library(gt)

#Comparing point estimates/posterior means

chain.all <- rbind(chain1$par, chain2$par, chain3$par)

# beta, u comparison table with padded numbers
betau_comparison <- data.frame(
  Parameter = c(paste0("β[", 1:2, "]"), paste0("u[", sprintf("%02d", 1:13), "]")),
  VB_Test1  = test1$betau_mean,
  VB_Test2  = test2$betau_mean,
  VB_Test3  = test3$betau_mean,
  Gibbs     = colMeans(chain.all[, 1:15])
)

gt(betau_comparison) |>
  fmt_number(columns = -Parameter, decimals = 4) |>
  tab_header(
    title = "Comparing estimates: Gibbs vs Variational Bayes",
    subtitle = "Fixed effects (β) and random effects (u) posterior means"
  ) |>
  tab_footnote(
    footnote = "VB_Test1-3: Three different blocking structures for Variational Bayes",
    locations = cells_column_labels(columns = starts_with("VB"))
  )

#Variances.
sigmae2 <- sigmau2 <- rep(0, 3)
sigmae2[1] <- test1$tau_e[2] / (test1$tau_e[1] - 1)
sigmau2[1] <- test1$tau_u[2] / (test1$tau_u[1] - 1)
sigmae2[2] <- test2$tau_e[2] / (test2$tau_e[1] - 1)
sigmau2[2] <- test2$tau_u[2] / (test2$tau_u[1] - 1)
sigmae2[3] <- test3$tau_e[2] / (test3$tau_e[1] - 1)
sigmau2[3] <- test3$tau_u[2] / (test3$tau_u[1] - 1)

# Variance component comparison tables
sigmau2_comparison <- data.frame(
  VB_Test1 = sigmau2[1],
  VB_Test2 = sigmau2[2],
  VB_Test3 = sigmau2[3],
  Gibbs    = mean(chain.all[, 16]^2)
)

sigmae2_comparison <- data.frame(
  VB_Test1 = sigmae2[1],
  VB_Test2 = sigmae2[2],
  VB_Test3 = sigmae2[3],
  Gibbs    = mean(chain.all[, 17]^2)
)

gt(sigmau2_comparison) |>
  fmt_number(decimals = 6) |>
  tab_header(
    title = "σ²ᵤ Comparison",
    subtitle = "Random effects variance component: E[τᵤ⁻¹] = b/(a-1)"
  ) |>
  tab_footnote(
    footnote = "Estimated from Inverse-Gamma(a, b) posterior for τᵤ",
    locations = cells_title(groups = "subtitle")
  )

gt(sigmae2_comparison) |>
  fmt_number(decimals = 6) |>
  tab_header(
    title = "σ²ₑ Comparison",
    subtitle = "Residual variance component: E[τₑ⁻¹] = b/(a-1)"
  ) |>
  tab_footnote(
    footnote = "Estimated from Inverse-Gamma(a, b) posterior for τₑ",
    locations = cells_title(groups = "subtitle")
  )

#Comparing posterior distributions.
par(mfrow=c(5,4))
mlim<-quantile(chain.all[,1],c(0.005,0.995))
curve(dnorm(x,mean=test1$betau_mean[1],sd=sqrt(test1$betau_var[1,1])),ylab='Density',main='',xlim=mlim,col=2,lty=1,xlab=expression(beta[1]),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,1]))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)

mlim<-quantile(chain.all[,2],c(0.005,0.995))
curve(dnorm(x,mean=test1$betau_mean[2],sd=sqrt(test1$betau_var[2,2])),ylab='Density',main='',xlim=mlim,col=2,lty=1,xlab=expression(beta[2]),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,2]))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)


#Repeat for random effects.
for(i in 1:13){ 
mlim<-quantile(chain.all[,i+2],c(0.005,0.995))
curve(dnorm(x,mean=test1$betau_mean[i+2],sd=sqrt(test1$betau_var[i+2,i+2])),ylab='Density',main='',xlim=mlim,col=2,lty=1,xlab=paste('u',i,sep=''),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,i+2]))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)
}

mlim<-quantile(chain.all[,16]^2,c(0.005,0.995))
curve(dgamma(1/x,shape=test1$tau_u[1],rate=test1$tau_u[2])*x^(-2),ylab='Density',main='',xlim=c(0,mlim[2]),col=2,lty=1,xlab=expression(sigma[u]^2),cex.lab=2.5,cex.axis=1.5)
lines(density(chain.all[,16]^2)) 
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)
  
mlim<-quantile(chain.all[,17]^2,c(0.005,0.995))
curve(dgamma(1/x,shape=test1$tau_e[1],rate=test1$tau_e[2])*x^(-2),ylab='Density',main='',xlim=c(0,mlim[2]),col=2,lty=1,xlab=expression(sigma[e]^2),cex.lab=2.5,cex.axis=1.5,cex.axis=1.5)
lines(density(chain.all[,17]^2))
legend('topright',legend=c('Gibbs','V-B'),col=1:2,lty=1,bty='n',cex=2.5)
```

By determining the joint approximate posterior for $\bm \beta, {\bf u}$ rather than separate independent approximate posteriors for $\bm \beta$, $\bf u$ as in lecture 18, we improve the accuracy of approximate inference. This is because $\bf X$ and $\bf Z$ are not independent, and so contructing independent approximate posteriors for $\bm \beta$ and $\bf u$, we ignore the information contained in ${\bf X}'{\bf Z}$.