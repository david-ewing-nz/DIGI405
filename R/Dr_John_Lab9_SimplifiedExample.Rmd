---
title: ' VB Matrix Math Step-by-Step a Pedagogical Example\footnote{converted from Dr John Holmes lecture "MAST290125: Bayesian Statistical Learning Semester 2 2021 Lecture 18: Variational Bayes}'
header-includes:
   - \usepackage{amsmath}
   - \usepackage{unicode-math}
   - \let\bm\symbf
   - \usepackage{tikz}
   - \usetikzlibrary{calc}
   - \usepackage{eso-pic}
   - \usepackage[useregional]{datetime2}
   - \newcommand{\mymarginlabel}{DRAFT Pedagogical Example | Compiled:\space\DTMnow   }
   - \AddToShipoutPictureBG{\begin{tikzpicture}[remember picture,overlay]\node[rotate=90, anchor=north] at ($(current page.north west)+(1.4cm,-13cm)$) {\scriptsize\ttfamily \mymarginlabel};\end{tikzpicture}}
output: 
  pdf_document:
    latex_engine: xelatex
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mvtnorm)
library(gt)
set.seed(82171165)
```

# Introduction

 Variational Bayes algorithm with a **tiny example**:

- **n = 20** observations
- **p = 2** fixed effects (β₀, β₁)
- **q = 2** random effects (u₁, u₂)

We'll walk through the matrix math **line by line** to understand exactly what's happening.

# Dr. John's Setup: Constructing the Linear Model

## Likelihood for a Simple Linear Model

Starting with the basic linear model:

\begin{equation}
{\bf y} = {\bf X}\bm \beta + \bm \varepsilon, \quad \bm \varepsilon \sim \mathcal{N}({\bf 0}, \sigma^2{\bf I})
\end{equation}

The likelihood can be written step-by-step:

\begin{eqnarray}
p({\bf y} | \bm \beta, \sigma^2, {\bf X}) &=& \prod_{i=1}^{n} p(y_i | \bm \beta, \sigma^2, {\bf X}) \nonumber \\
&=& \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp\left(-\frac{(y_i - {\bf x}_i\bm \beta)^2}{2\sigma^2}\right) \nonumber \\
&=& \left(\frac{1}{2\pi\sigma^2}\right)^{n/2} \exp\left(-\frac{\sum_i(y_i - {\bf x}_i\bm \beta)^2}{2\sigma^2}\right) \nonumber \\
&=& \frac{1}{(2\pi\sigma^2)^{n/2}} \exp\left(-\frac{({\bf y} - {\bf X}\bm \beta)^T({\bf y} - {\bf X}\bm \beta)}{2\sigma^2}\right) \nonumber
\end{eqnarray}

## Priors

**Flat prior on β** (improper):
\begin{equation}
p(\bm \beta) \propto 1
\end{equation}

**Work with precision** $\tau = (\sigma^2)^{-1}$ instead of variance. This is a **Gamma(α, 0) prior** which is also improper:
\begin{equation}
p(\tau) \propto \tau^{-1}
\end{equation}

Note: This is a Gamma(0, 0) point (improper prior) which is common for uninformative priors.

## Joint Distribution

The joint distribution becomes:
\begin{eqnarray}
p({\bf y}, \bm \beta, \tau | {\bf X}) &=& p({\bf y} | \bm \beta, \tau, {\bf X}) \times p(\bm \beta) \times p(\tau) \nonumber \\
&=& \left(\frac{\tau}{2\pi}\right)^{n/2} \exp\left(-\frac{\tau({\bf y} - {\bf X}\bm \beta)^T({\bf y} - {\bf X}\bm \beta)}{2}\right) \times 1 \times \tau^{-1} \nonumber \\
&=& (2\pi)^{-n/2} \tau^{-1 + n/2} \exp\left(-\frac{\tau({\bf y} - {\bf X}\bm \beta)^T({\bf y} - {\bf X}\bm \beta)}{2}\right) \nonumber
\end{eqnarray}

This setup shows **how the precision τ appears in the joint distribution**, which is key for VB updates.

# Model Setup with Random Effects

For the hierarchical model with random effects, we extend this to:

The hierarchical model is:

\begin{eqnarray}
p({\bf y}|\bm \beta, {\bf u}, \tau_e) &=& \mathcal{N}({\bf X}\bm \beta + {\bf Z}{\bf u}, \frac{1}{\tau_e}{\bf I}_n) \nonumber \\
p(\bm \beta) &\propto& 1   \nonumber \\
p({\bf u})   &=&\mathcal{N}({\bf 0}_q, \frac{1}{\tau_u}{\bf K}) \nonumber\\
p(\tau_e)    &=& \text{Ga}(\alpha_e,\gamma_e) \nonumber\\
p(\tau_u)    &=& \text{Ga}(\alpha_u,\gamma_u) \nonumber
\end{eqnarray}

**For our simplified example, the specific values are:**

- **Dimensions**: $n = 20$ observations, $p = 2$ fixed effects, $q = 2$ random effects
- **Prior for β**: Flat (improper) prior, $p(\bm \beta) \propto 1$
- **Prior for u**: $\mathcal{N}({\bf 0}_2, \frac{1}{\tau_u}{\bf K})$ where ${\bf K} = {\bf I}_2$ (2×2 identity matrix)
- **Prior for τₑ**: $\text{Ga}(0.01, 0.01)$, so $\alpha_e = 0.01$ and $\gamma_e = 0.01$
- **Prior for τᵤ**: $\text{Ga}(0.01, 0.01)$, so $\alpha_u = 0.01$ and $\gamma_u = 0.01$

These are **weakly informative priors** - the Gamma(0.01, 0.01) has very little influence on the posterior.

## Finding Conditional Posteriors (Page 2 of Dr. John's Notes)

Now we need to find **conditional posteriors** for Variational Bayes. We'll start with $p(\bm \beta | \tau, {\bf X}, {\bf y})$.

### Conditional Posterior for β

**Extract the component** of the joint distribution involving β:

The part proportional to β in the joint is:
\begin{equation}
\exp\left(-\frac{\tau({\bf y} - {\bf X}\bm \beta)^T({\bf y} - {\bf X}\bm \beta)}{2}\right)
\end{equation}

**Expanding out** the quadratic form:
\begin{eqnarray}
&& -\tau({\bf y}^T{\bf y} - \bm \beta^T{\bf X}^T{\bf y} - {\bf y}^T{\bf X}\bm \beta + \bm \beta^T{\bf X}^T{\bf X}\bm \beta)/2 \nonumber \\
&\propto& -\tau(\bm \beta^T{\bf X}^T{\bf X}\bm \beta - \bm \beta^T{\bf X}^T{\bf y} - {\bf y}^T{\bf X}\bm \beta)/2 \nonumber \\
&=& -(\bm \beta^T(\tau{\bf X}^T{\bf X})\bm \beta - \bm \beta^T(\tau{\bf X}^T){\bf y} - {\bf y}^T({\bf X}^T)^T(\tau{\bf X}^T)\bm \beta)/2 \nonumber
\end{eqnarray}

**Completing the square** to recognize this as a multivariate normal:

Recall the MV Normal form:
\begin{equation}
\exp\left(-\frac{({\bf x} - \bm \mu)^T\bm \Sigma^{-1}({\bf x} - \bm \mu)}{2}\right) = \exp\left(-\frac{{\bf x}^T\bm \Sigma^{-1}{\bf x} - {\bf x}^T\bm \Sigma^{-1}\bm \mu - \bm \mu^T\bm \Sigma^{-1}{\bf x} + \bm \mu^T\bm \Sigma^{-1}\bm \mu}{2}\right)
\end{equation}

Dropping terms that don't involve β (the $\bm \mu^T\bm \Sigma^{-1}\bm \mu$ term):
\begin{equation}
\propto \exp\left(-\frac{{\bf x}^T\bm \Sigma^{-1}{\bf x} - {\bf x}^T\bm \Sigma^{-1}\bm \mu - \bm \mu^T\bm \Sigma^{-1}{\bf x}}{2}\right)
\end{equation}

**Matching the pattern:**

Comparing our expanded form with the MV Normal pattern:
- Precision matrix: $\bm \Sigma^{-1} = \tau{\bf X}^T{\bf X}$
- Covariance matrix: $\bm \Sigma = (\tau{\bf X}^T{\bf X})^{-1} = \frac{1}{\tau}({\bf X}^T{\bf X})^{-1}$
- Mean vector: $\bm \mu = \bm \Sigma(\tau{\bf X}^T{\bf y}) = ({\bf X}^T{\bf X})^{-1}{\bf X}^T{\bf y}$

**Therefore, the conditional posterior is:**
\begin{equation}
p(\bm \beta | \tau, {\bf X}, {\bf y}) = \mathcal{N}\left(\bm \beta \mid ({\bf X}^T{\bf X})^{-1}{\bf X}^T{\bf y}, \frac{1}{\tau}({\bf X}^T{\bf X})^{-1}\right)
\end{equation}

This is the **standard Bayesian linear regression posterior** for β given τ!

# Generate Small Toy Data

```{r generate_data}
# Dimensions
n <- 20   # observations
p <- 2    # fixed effects
q <- 2    # random effects

# True parameter values (for checking)
beta_true  <- c(5, 2)      # intercept and slope
u_true     <- c(0.5, -0.3) # random effects
tau_e_true <- 4            # residual precision (σ²ₑ = 0.25)
tau_u_true <- 2            # random effects precision (σ²ᵤ = 0.5)

# Design matrices
X <- cbind(1, rnorm(n))  # intercept + one continuous predictor
Z <- cbind(rep(c(1, 0), each = n/2),  # group 1
           rep(c(0, 1), each = n/2))  # group 2

# Kinship matrix (simple identity for this example)
K <- diag(q)

# Generate response
y <- as.vector(X %*% beta_true + Z %*% u_true + rnorm(n, 0, 1/sqrt(tau_e_true)))

# Display data dimensions
cat("Data dimensions:\n")
cat("  X:", nrow(X), "x", ncol(X), "\n")
cat("  Z:", nrow(Z), "x", ncol(Z), "\n")
cat("  y:", length(y), "\n")
cat("  K:", nrow(K), "x", ncol(K), "\n")
```

# VB Algorithm: Joint Update of (β, u)

The key insight is to update $\bm \beta$ and $\bf u$ **jointly** rather than separately.

## Step 1: Joint Kernel of (β, u)

The prior for $(\bm \beta, {\bf u})$ can be written as:

\begin{equation}
p(\bm \beta,{\bf u}) \propto e^{-\frac{\tau_u\begin{pmatrix} \bm \beta' & {\bf u}'\end{pmatrix}\begin{pmatrix}{\bf 0}_{p \times p} & {\bf 0}_{p \times q} \\ {\bf 0}_{q \times p} & {\bf K}^{-1} \end{pmatrix}\begin{pmatrix} \bm \beta \\ {\bf u}\end{pmatrix}}{2}}
\end{equation}

Let's build this block matrix step-by-step:

```{r block_matrix}
# Block penalty matrix: penalises u but not β
# Top-left: p×p zeros (no penalty on β)
# Top-right: p×q zeros
# Bottom-left: q×p zeros  
# Bottom-right: q×q = K⁻¹ (penalty on u)

K_inv <- solve(K)

penalty_matrix <- rbind(
  cbind(matrix(0, p, p), matrix(0, p, q)),  # top row: zeros for β
  cbind(matrix(0, q, p), K_inv)             # bottom row: K⁻¹ for u
)

cat("Penalty matrix (dimension", nrow(penalty_matrix), "x", ncol(penalty_matrix), "):\n")
print(round(penalty_matrix, 4))
```

## Step 2: Joint Design Matrix

Combine X and Z into a single design matrix for the joint parameter $\begin{pmatrix} \bm \beta \\ {\bf u} \end{pmatrix}$:

```{r joint_design}
# Joint design matrix: [X | Z]
# Each row: first p columns are X, next q columns are Z
XZ <- cbind(X, Z)

cat("Joint design matrix XZ (dimension", nrow(XZ), "x", ncol(XZ), "):\n")
cat("First 5 rows:\n")
print(round(head(XZ, 5), 3))
```

## Step 3: Update Q(β, u) - Variational Distribution

Given the current estimates of $E[\tau_e]$ and $E[\tau_u]$, the optimal $Q(\bm \beta, {\bf u})$ is multivariate normal:

\begin{equation}
Q(\bm \beta, {\bf u}) = \mathcal{N}\left(\bm \mu_{\beta,u}, \bm \Sigma_{\beta,u}\right)
\end{equation}

where:

\begin{eqnarray}
\bm \Sigma_{\beta,u} &=& \left(E[\tau_e]{\bf XZ}^T{\bf XZ} + E[\tau_u]\text{PenaltyMatrix}\right)^{-1} \\
\bm \mu_{\beta,u} &=& E[\tau_e]\bm \Sigma_{\beta,u} {\bf XZ}^T{\bf y}
\end{eqnarray}

Let's compute this step-by-step:

```{r update_betau}
# Initialize hyperparameters (priors for precision parameters)
alpha_e <- 0.01
gamma_e <- 0.01
alpha_u <- 0.01
gamma_u <- 0.01

# Initialize expectations
E_tau_e <- alpha_e / gamma_e  # E[τₑ] = a/b for Gamma(a,b)
E_tau_u <- alpha_u / gamma_u  # E[τᵤ] = a/b

cat("Initial expectations:\n")
cat("  E[τₑ] =", round(E_tau_e, 4), "\n")
cat("  E[τᵤ] =", round(E_tau_u, 4), "\n\n")

# Step-by-step covariance matrix computation

# Term 1: E[τₑ] × (XZ)ᵀ(XZ)
XZ_t_XZ <- t(XZ) %*% XZ
cat("(XZ)ᵀ(XZ) (dimension", nrow(XZ_t_XZ), "x", ncol(XZ_t_XZ), "):\n")
print(round(XZ_t_XZ, 3))
cat("\n")

term1 <- E_tau_e * XZ_t_XZ
cat("E[τₑ] × (XZ)ᵀ(XZ):\n")
print(round(term1, 3))
cat("\n")

# Term 2: E[τᵤ] × PenaltyMatrix
term2 <- E_tau_u * penalty_matrix
cat("E[τᵤ] × PenaltyMatrix:\n")
print(round(term2, 3))
cat("\n")

# Precision matrix (inverse of covariance)
precision_betau <- term1 + term2
cat("Precision matrix = Term1 + Term2:\n")
print(round(precision_betau, 3))
cat("\n")

# Covariance matrix
Sigma_betau <- solve(precision_betau)
cat("Covariance Σ_{β,u} = (Precision)⁻¹:\n")
print(round(Sigma_betau, 4))
cat("\n")

# Mean vector computation

# Term: E[τₑ] × (XZ)ᵀy
XZ_t_y <- t(XZ) %*% y
cat("(XZ)ᵀy (dimension", length(XZ_t_y), "):\n")
print(round(XZ_t_y, 3))
cat("\n")

mu_betau <- E_tau_e * Sigma_betau %*% XZ_t_y
cat("Mean μ_{β,u} = E[τₑ] × Σ_{β,u} × (XZ)ᵀy:\n")
print(round(mu_betau, 4))
cat("\n")

# Extract β and u separately
mu_beta <- mu_betau[1:p]
mu_u    <- mu_betau[(p+1):(p+q)]

cat("β estimates:", round(mu_beta, 4), "(true:", beta_true, ")\n")
cat("u estimates:", round(mu_u, 4), "(true:", u_true, ")\n")
```

## Step 4: Update Q(τₑ) - Residual Precision

The optimal $Q(\tau_e)$ is Gamma distributed:

\begin{equation}
Q(\tau_e) = \text{Ga}\left(a_e^{new}, b_e^{new}\right)
\end{equation}

where:

\begin{eqnarray}
a_e^{new} &=& \alpha_e + \frac{n}{2} \\
b_e^{new} &=& \gamma_e + \frac{1}{2}\left[({\bf y} - {\bf XZ}\bm \mu_{\beta,u})^T({\bf y} - {\bf XZ}\bm \mu_{\beta,u}) + \text{Tr}({\bf XZ}^T{\bf XZ}\bm \Sigma_{\beta,u})\right]
\end{eqnarray}

```{r update_tau_e}
cat("Step 4: Update Q(τₑ)\n\n")

# New shape parameter
a_e_new <- alpha_e + n/2
cat("a_e^new = α_e + n/2 =", alpha_e, "+", n/2, "=", a_e_new, "\n\n")

# Residuals
residuals <- y - XZ %*% mu_betau
cat("Residuals (y - XZ μ_{β,u}), first 5:\n")
print(round(head(residuals, 5), 4))
cat("\n")

# Sum of squared residuals
SSR <- sum(residuals^2)
cat("Sum of squared residuals =", round(SSR, 4), "\n\n")

# Trace term: accounts for uncertainty in (β, u)
trace_term <- sum(diag(XZ_t_XZ %*% Sigma_betau))
cat("Trace term Tr((XZ)ᵀ(XZ) Σ_{β,u}) =", round(trace_term, 4), "\n")
cat("  (This accounts for uncertainty in parameter estimates)\n\n")

# New rate parameter
b_e_new <- gamma_e + 0.5 * (SSR + trace_term)
cat("b_e^new = γ_e + 0.5(SSR + Trace) =", gamma_e, "+ 0.5(", 
    round(SSR, 2), "+", round(trace_term, 2), ") =", round(b_e_new, 4), "\n\n")

# New expectation
E_tau_e_new <- a_e_new / b_e_new
cat("E[τₑ]^new = a_e^new / b_e^new =", round(E_tau_e_new, 4), "\n")
cat("Implied σ²ₑ = 1/E[τₑ] =", round(1/E_tau_e_new, 4), 
    "(true:", round(1/tau_e_true, 4), ")\n")
```

## Step 5: Update Q(τᵤ) - Random Effects Precision

The optimal $Q(\tau_u)$ is Gamma distributed:

\begin{equation}
Q(\tau_u) = \text{Ga}\left(a_u^{new}, b_u^{new}\right)
\end{equation}

where:

\begin{eqnarray}
a_u^{new} &=& \alpha_u + \frac{q}{2} \\
b_u^{new} &=& \gamma_u + \frac{1}{2}\left[\bm \mu_u^T {\bf K}^{-1} \bm \mu_u + \text{Tr}({\bf K}^{-1}\bm \Sigma_{uu})\right]
\end{eqnarray}

```{r update_tau_u}
cat("Step 5: Update Q(τᵤ)\n\n")

# New shape parameter
a_u_new <- alpha_u + q/2
cat("a_u^new = α_u + q/2 =", alpha_u, "+", q/2, "=", a_u_new, "\n\n")

# Extract covariance of u only (bottom-right block)
Sigma_uu <- Sigma_betau[(p+1):(p+q), (p+1):(p+q)]
cat("Covariance of u only Σ_uu:\n")
print(round(Sigma_uu, 4))
cat("\n")

# Quadratic form: μᵤᵀ K⁻¹ μᵤ
quad_form <- t(mu_u) %*% K_inv %*% mu_u
cat("Quadratic form μᵤᵀ K⁻¹ μᵤ =", round(quad_form[1,1], 4), "\n\n")

# Trace term
trace_u <- sum(diag(K_inv %*% Sigma_uu))
cat("Trace term Tr(K⁻¹ Σ_uu) =", round(trace_u, 4), "\n")
cat("  (This accounts for uncertainty in u estimates)\n\n")

# New rate parameter
b_u_new <- gamma_u + 0.5 * (quad_form + trace_u)
cat("b_u^new = γ_u + 0.5(QuadForm + Trace) =", gamma_u, "+ 0.5(", 
    round(quad_form[1,1], 2), "+", round(trace_u, 2), ") =", round(b_u_new, 4), "\n\n")

# New expectation
E_tau_u_new <- a_u_new / b_u_new
cat("E[τᵤ]^new = a_u^new / b_u^new =", round(E_tau_u_new, 4), "\n")
cat("Implied σ²ᵤ = 1/E[τᵤ] =", round(1/E_tau_u_new, 4), 
    "(true:", round(1/tau_u_true, 4), ")\n")
```

# Full VB Algorithm (Multiple Iterations)

Now let's run the full algorithm for multiple iterations:

```{r vb_full}
max_iter <- 100
tol      <- 1e-6

# Storage
E_tau_e_history <- numeric(max_iter)
E_tau_u_history <- numeric(max_iter)

# Initialize
E_tau_e <- alpha_e / gamma_e
E_tau_u <- alpha_u / gamma_u

for (iter in 1:max_iter) {
  
  # Update Q(β, u)
  precision_betau <- E_tau_e * XZ_t_XZ + E_tau_u * penalty_matrix
  Sigma_betau     <- solve(precision_betau)
  mu_betau        <- E_tau_e * Sigma_betau %*% XZ_t_y
  
  mu_beta <- mu_betau[1:p]
  mu_u    <- mu_betau[(p+1):(p+q)]
  Sigma_uu <- Sigma_betau[(p+1):(p+q), (p+1):(p+q)]
  
  # Update Q(τₑ)
  residuals <- y - XZ %*% mu_betau
  SSR       <- sum(residuals^2)
  trace_e   <- sum(diag(XZ_t_XZ %*% Sigma_betau))
  
  a_e_new <- alpha_e + n/2
  b_e_new <- gamma_e + 0.5 * (SSR + trace_e)
  
  E_tau_e_old <- E_tau_e
  E_tau_e     <- a_e_new / b_e_new
  
  # Update Q(τᵤ)
  quad_form <- as.numeric(t(mu_u) %*% K_inv %*% mu_u)
  trace_u   <- sum(diag(K_inv %*% Sigma_uu))
  
  a_u_new <- alpha_u + q/2
  b_u_new <- gamma_u + 0.5 * (quad_form + trace_u)
  
  E_tau_u_old <- E_tau_u
  E_tau_u     <- a_u_new / b_u_new
  
  # Store
  E_tau_e_history[iter] <- E_tau_e
  E_tau_u_history[iter] <- E_tau_u
  
  # Check convergence
  delta_e <- abs(E_tau_e - E_tau_e_old)
  delta_u <- abs(E_tau_u - E_tau_u_old)
  
  if (max(delta_e, delta_u) < tol) {
    cat("Converged at iteration", iter, "\n")
    E_tau_e_history <- E_tau_e_history[1:iter]
    E_tau_u_history <- E_tau_u_history[1:iter]
    break
  }
}

# Final estimates
cat("\nFinal VB estimates:\n")
cat("β: ", round(mu_beta, 4), " (true:", beta_true, ")\n")
cat("u: ", round(mu_u, 4), " (true:", u_true, ")\n")
cat("τₑ:", round(E_tau_e, 4), " → σ²ₑ =", round(1/E_tau_e, 4), 
    " (true:", round(1/tau_e_true, 4), ")\n")
cat("τᵤ:", round(E_tau_u, 4), " → σ²ᵤ =", round(1/E_tau_u, 4), 
    " (true:", round(1/tau_u_true, 4), ")\n")
```

# Convergence Plot

```{r convergence_plot, fig.height=6, fig.width=8}
par(mfrow = c(1, 2))

plot(1:length(E_tau_e_history), E_tau_e_history, 
     type = 'l', lwd = 2, col = 'blue',
     xlab = 'Iteration', ylab = expression(E[tau[e]]),
     main = 'Convergence of E[τₑ]')
abline(h = tau_e_true, col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB estimate', 'True value'), 
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)

plot(1:length(E_tau_u_history), E_tau_u_history, 
     type = 'l', lwd = 2, col = 'blue',
     xlab = 'Iteration', ylab = expression(E[tau[u]]),
     main = 'Convergence of E[τᵤ]')
abline(h = tau_u_true, col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB estimate', 'True value'), 
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)
```

# Posterior Distribution Comparisons

Now let's compare the **VB approximate posteriors** with the **true parameter values** visually.

## Parameter Estimates vs Truth

```{r param_comparison_table}
# Create comprehensive comparison table
param_table <- data.frame(
  Parameter = c("β[0]", "β[1]", "u[01]", "u[02]", "τₑ", "τᵤ", "σ²ₑ", "σ²ᵤ"),
  True_Value = c(beta_true, u_true, tau_e_true, tau_u_true, 
                 1/tau_e_true, 1/tau_u_true),
  VB_Estimate = c(mu_beta, mu_u, E_tau_e, E_tau_u, 
                  1/E_tau_e, 1/E_tau_u),
  Absolute_Error = c(abs(mu_beta - beta_true), 
                     abs(mu_u - u_true),
                     abs(E_tau_e - tau_e_true),
                     abs(E_tau_u - tau_u_true),
                     abs(1/E_tau_e - 1/tau_e_true),
                     abs(1/E_tau_u - 1/tau_u_true))
)

gt(param_table) |>
  fmt_number(columns = c(True_Value, VB_Estimate, Absolute_Error), decimals = 4) |>
  tab_header(
    title = "VB Estimates vs True Values",
    subtitle = "Final iteration estimates from simplified example"
  ) |>
  tab_style(
    style = cell_fill(color = "lightgreen"),
    locations = cells_body(columns = Absolute_Error, rows = Absolute_Error < 0.1)
  ) |>
  tab_style(
    style = cell_fill(color = "lightyellow"),
    locations = cells_body(columns = Absolute_Error, rows = Absolute_Error >= 0.1 & Absolute_Error < 0.5)
  )
```

## Visualising VB Approximate Posteriors

```{r vb_posteriors, fig.height=10, fig.width=10}
par(mfrow = c(3, 2), mar = c(4, 4, 3, 1))

# β₀ posterior
x_seq <- seq(mu_beta[1] - 3*sqrt(Sigma_betau[1,1]), 
             mu_beta[1] + 3*sqrt(Sigma_betau[1,1]), length = 200)
plot(x_seq, dnorm(x_seq, mu_beta[1], sqrt(Sigma_betau[1,1])),
     type = 'l', lwd = 2, col = 'blue',
     xlab = expression(beta[0]), ylab = 'Density',
     main = 'VB Posterior for β₀')
abline(v = beta_true[1], col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB posterior', 'True value'),
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)

# β₁ posterior
x_seq <- seq(mu_beta[2] - 3*sqrt(Sigma_betau[2,2]), 
             mu_beta[2] + 3*sqrt(Sigma_betau[2,2]), length = 200)
plot(x_seq, dnorm(x_seq, mu_beta[2], sqrt(Sigma_betau[2,2])),
     type = 'l', lwd = 2, col = 'blue',
     xlab = expression(beta[1]), ylab = 'Density',
     main = 'VB Posterior for β₁')
abline(v = beta_true[2], col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB posterior', 'True value'),
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)

# u₁ posterior
x_seq <- seq(mu_u[1] - 3*sqrt(Sigma_uu[1,1]), 
             mu_u[1] + 3*sqrt(Sigma_uu[1,1]), length = 200)
plot(x_seq, dnorm(x_seq, mu_u[1], sqrt(Sigma_uu[1,1])),
     type = 'l', lwd = 2, col = 'blue',
     xlab = expression(u[1]), ylab = 'Density',
     main = 'VB Posterior for u₁')
abline(v = u_true[1], col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB posterior', 'True value'),
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)

# u₂ posterior
x_seq <- seq(mu_u[2] - 3*sqrt(Sigma_uu[2,2]), 
             mu_u[2] + 3*sqrt(Sigma_uu[2,2]), length = 200)
plot(x_seq, dnorm(x_seq, mu_u[2], sqrt(Sigma_uu[2,2])),
     type = 'l', lwd = 2, col = 'blue',
     xlab = expression(u[2]), ylab = 'Density',
     main = 'VB Posterior for u₂')
abline(v = u_true[2], col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB posterior', 'True value'),
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)

# τₑ posterior (Gamma distribution)
x_seq <- seq(0, a_e_new/b_e_new + 4*sqrt(a_e_new)/b_e_new, length = 200)
plot(x_seq, dgamma(x_seq, a_e_new, b_e_new),
     type = 'l', lwd = 2, col = 'blue',
     xlab = expression(tau[e]), ylab = 'Density',
     main = 'VB Posterior for τₑ')
abline(v = tau_e_true, col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB posterior', 'True value'),
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)

# τᵤ posterior (Gamma distribution)
x_seq <- seq(0, a_u_new/b_u_new + 4*sqrt(a_u_new)/b_u_new, length = 200)
plot(x_seq, dgamma(x_seq, a_u_new, b_u_new),
     type = 'l', lwd = 2, col = 'blue',
     xlab = expression(tau[u]), ylab = 'Density',
     main = 'VB Posterior for τᵤ')
abline(v = tau_u_true, col = 'red', lty = 2, lwd = 2)
legend('topright', legend = c('VB posterior', 'True value'),
       col = c('blue', 'red'), lty = c(1, 2), lwd = 2)
```

# Summary

This simplified example demonstrates:

1. **Joint blocking**: Updating (β, u) together using block matrix structure
2. **Matrix operations**: Step-by-step precision/covariance calculations
3. **Trace terms**: How uncertainty propagates through matrix operations
4. **Gamma updates**: How residuals and uncertainty update precision parameters
5. **Convergence**: How expectations stabilize over iterations

The key insight is that VB treats everything as **multivariate Gaussians** (for β, u) and **Gammas** (for τ), updating their parameters iteratively until convergence.
